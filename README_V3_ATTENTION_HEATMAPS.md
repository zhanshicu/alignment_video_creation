# Video Advertisement Alignment Manipulation Framework v3

## Using Actual Attention Heatmaps

This version of the framework uses **actual attention heatmaps** instead of scalar alignment scores, providing more precise spatial control over attention-keyword alignment manipulation.

---

## ğŸ“ Directory Structure

```
alignment_video_creation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ video_scene_cuts/              # Scene images (RGB)
â”‚   â”‚   â””â”€â”€ {video_id}/
â”‚   â”‚       â””â”€â”€ {video_id}-Scene-0xx-01.jpg
â”‚   â”‚
â”‚   â”œâ”€â”€ keywords.csv                   # Video keywords
â”‚   â”‚   Columns: _id, keyword_list[0]
â”‚   â”‚
â”‚   â””â”€â”€ valid_scenes.csv               # Generated by data_validation.ipynb
â”‚       Columns: video_id, scene_number, keyword,
â”‚                scene_image_path, keyword_mask_path,
â”‚                attention_heatmap_path, filename
â”‚
â”œâ”€â”€ keyword_masks/                     # Keyword masks from CLIPSeg
â”‚   â””â”€â”€ {video_id}/
â”‚       â””â”€â”€ {video_id}-Scene-0xx-01.png
â”‚
â”œâ”€â”€ attention_heatmap/                 # Attention heatmaps
â”‚   â””â”€â”€ {video_id}/
â”‚       â””â”€â”€ {video_id}-Scene-0xx-01.jpg
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ training_v3/                   # Model checkpoints
â”‚   â””â”€â”€ variants_v3/                   # Generated variants
â”‚       â”œâ”€â”€ {video_id}/
â”‚       â”‚   â”œâ”€â”€ {variant_name}/
â”‚       â”‚   â”‚   â””â”€â”€ attention_heatmap/
â”‚       â”‚   â”‚       â””â”€â”€ {video_id}-Scene-0xx-01.jpg
â”‚       â”‚   â”œâ”€â”€ {variant_name}_scenes.csv
â”‚       â”‚   â””â”€â”€ statistics.json
â”‚       â””â”€â”€ manifest.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ dataset_v3.py              # NEW: Dataset with attention heatmaps
â”‚   â”œâ”€â”€ video_editing/
â”‚   â”‚   â””â”€â”€ experimental_variants_v3.py # NEW: Variant generator for heatmaps
â”‚   â””â”€â”€ data_preparation/
â”‚       â”œâ”€â”€ control_tensor.py          # Control tensor builder
â”‚       â”œâ”€â”€ keyword_heatmap.py         # CLIPSeg keyword segmentation
â”‚       â””â”€â”€ attention_heatmap.py       # Attention heatmap processing
â”‚
â”œâ”€â”€ data_validation.ipynb              # NEW: Validate data and create valid_scenes.csv
â”œâ”€â”€ workflow_v3_with_attention_heatmaps.ipynb  # NEW: Complete workflow
â””â”€â”€ README_V3_ATTENTION_HEATMAPS.md    # This file
```

---

## ğŸ”„ Key Changes from V2

### Previous Approach (V2)
- Used scalar `attention_proportion` from `alignment_score.csv`
- Control tensor: `[M_t, S_t]` where `S_t = M_t Ã— attention_proportion`
- Limited spatial information about attention

### New Approach (V3)
- Uses actual attention heatmaps (spatial maps)
- Control tensor: `[M_t, A_t]` where:
  - `M_t` = Keyword mask (binary, where product is located)
  - `A_t` = Attention heatmap (continuous, where viewers look)
- Full spatial information about attention distribution
- More precise manipulation of attention patterns

---

## ğŸš€ Complete Workflow

### Step 1: Data Validation

**Run first**: `data_validation.ipynb`

This notebook:
1. Scans `data/video_scene_cuts/`, `keyword_masks/`, and `attention_heatmap/`
2. Validates that all required files exist for each scene
3. Parses scene filenames: `{video_id}-Scene-0xx-01.jpg`
4. Checks keywords from `data/keywords.csv`
5. Creates `data/valid_scenes.csv` with only valid scenes

**Why?** Avoids repeated file existence checks during training/inference.

```python
# Expected filename format
scene_image:       data/video_scene_cuts/{video_id}/{video_id}-Scene-001-01.jpg
keyword_mask:      data/keyword_masks/{video_id}/scene_1.png
attention_heatmap: data/attention_heatmap/{video_id}/{video_id}-Scene-001.jpg
```

### Step 2: Data Loading

**Dataset**: `src/training/dataset_v3.py`

```python
from src.training.dataset_v3 import VideoSceneDataModuleV3

# Create data module
data_module = VideoSceneDataModuleV3(
    valid_scenes_file='data/valid_scenes.csv',
    train_val_split=0.8,
    batch_size=4,
    image_size=(512, 512),
)

train_loader = data_module.train_dataloader()
val_loader = data_module.val_dataloader()
```

**Each batch contains**:
- `image`: Scene image tensor `[B, 3, H, W]` in `[-1, 1]`
- `control`: Control tensor `[B, 2, H, W]`
  - Channel 0: Keyword mask `M_t` (binary)
  - Channel 1: Attention heatmap `A_t` (continuous `[0, 1]`)
- `keyword_mask`: Binary mask `[B, 1, H, W]`
- `attention_heatmap`: Attention heatmap `[B, 1, H, W]`
- `alignment_score`: Scalar overlap between keyword and attention
- `keyword`: Text prompt (e.g., "product", "sneakers")
- `video_id`, `scene_number`: Metadata

### Step 3: Model Training

**Objective**: Fine-tune ControlNet to manipulate scenes based on attention heatmaps

```python
from src.models import StableDiffusionControlNetWrapper
from src.training import ControlNetTrainer

# Initialize model
model = StableDiffusionControlNetWrapper(
    sd_model_name='runwayml/stable-diffusion-v1-5',
    controlnet_config={'control_channels': 2},
    device='cuda',
)

# Train
trainer = ControlNetTrainer(
    model=model,
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    num_epochs=10,
    output_dir='outputs/training_v3',
)

trainer.train()
```

**Loss function**:
- Reconstruction loss: Preserve scene content
- LPIPS loss: Preserve perceptual quality
- Background preservation loss: Keep non-keyword regions unchanged

### Step 4: Generate Experimental Variants

**Variant Generator**: `src/video_editing/experimental_variants_v3.py`

```python
from src.video_editing.experimental_variants_v3 import VideoVariantGeneratorV3

# Initialize generator
generator = VideoVariantGeneratorV3(
    valid_scenes_file='data/valid_scenes.csv',
    boost_alpha=1.5,      # Boost factor
    reduction_alpha=0.5,  # Reduction factor
)

# Generate variants for a video
variants = generator.create_all_variants_for_video(
    video_id='123456',
    output_dir='outputs/variants_v3'
)

# Generate for all videos
all_variants = generator.generate_variants_for_all_videos(
    output_dir='outputs/variants_v3'
)
```

**7 Experimental Variants**:

1. **baseline**: Original attention heatmaps (control condition)
2. **early_boost**: Boost attention on keyword in first 33% of scenes (Ã—1.5)
3. **middle_boost**: Boost attention in middle 33% (Ã—1.5)
4. **late_boost**: Boost attention in last 33% (Ã—1.5)
5. **full_boost**: Boost attention in all scenes (Ã—1.5)
6. **reduction**: Reduce attention in middle 33% (Ã—0.5)
7. **placebo**: Modify background regions only (control for editing artifacts)

**How variants are created**:
```python
def modify_attention_heatmap(attention_heatmap, keyword_mask, alpha):
    """
    Modify attention in keyword region by factor alpha.

    Args:
        attention_heatmap: Original attention (H, W)
        keyword_mask: Binary keyword mask (H, W)
        alpha: Scaling factor (>1 boost, <1 reduce)

    Returns:
        Modified attention heatmap (H, W)
    """
    # Multiply attention in keyword region by alpha
    modified = attention_heatmap * (1 - keyword_mask) + \
               (attention_heatmap * keyword_mask * alpha)

    # Renormalize to preserve total attention
    modified = modified / modified.sum() * attention_heatmap.sum()

    return np.clip(modified, 0, 1)
```

### Step 5: Inference - Generate Edited Scenes

```python
# Load trained model
model.load_checkpoint('outputs/training_v3/best_model.pt')

# For each variant scene
for variant_type in ['baseline', 'early_boost', ...]:
    for scene in variant_scenes:
        # Load scene and modified attention heatmap
        scene_image = load_image(scene['scene_image_path'])
        keyword_mask = load_mask(scene['keyword_mask_path'])
        attention_heatmap = load_heatmap(
            scene['modified_attention_heatmap_path']
        )

        # Build control tensor
        control = np.stack([keyword_mask, attention_heatmap], axis=0)

        # Generate edited scene
        edited_scene = model.generate(
            prompt=scene['keyword'],
            control_tensor=control,
            num_inference_steps=50,
            guidance_scale=7.5,
        )

        # Save edited scene
        save_image(edited_scene, f'outputs/edited_scenes/{variant_type}/{scene_id}.jpg')
```

### Step 6: Video Assembly

```python
from src.video_editing import VideoEditor

# Reassemble edited scenes into videos
editor = VideoEditor()

for variant_type in ['baseline', 'early_boost', ...]:
    edited_video = editor.assemble_video(
        scenes_dir=f'outputs/edited_scenes/{variant_type}',
        output_path=f'outputs/videos/{video_id}_{variant_type}.mp4',
        apply_temporal_smoothing=True,
    )
```

---

## ğŸ“Š Data Format Specifications

### 1. Scene Images
- **Path**: `data/video_scene_cuts/{video_id}/{video_id}-Scene-0xx-01.jpg`
- **Format**: RGB images (JPEG or PNG)
- **Size**: Any size (will be resized to 512Ã—512)
- **Naming**: `{video_id}-Scene-{scene_number:03d}-01.jpg`

### 2. Keyword Masks
- **Path**: `data/keyword_masks/{video_id}/scene_{x}.png` (where x is scene number)
- **Format**: Grayscale images (PNG)
- **Values**: 0-255 (will be binarized with threshold 0.5)
- **Content**: White (255) where product/keyword is located, black (0) elsewhere
- **Generated by**: CLIPSeg (`scripts/generate_keyword_masks.py`)
- **Example**: `data/keyword_masks/123456/scene_1.png`, `data/keyword_masks/123456/scene_2.png`

### 3. Attention Heatmaps
- **Path**: `data/attention_heatmap/{video_id}/{video_id}-Scene-{number:03d}.jpg` (NO `-01` suffix!)
- **Format**: Grayscale images (JPEG or PNG)
- **Values**: 0-255 (will be normalized to [0, 1])
- **Content**: Higher values = more attention, lower values = less attention
- **Source**: Your attention tracking system

### 4. Keywords File
- **Path**: `data/keywords.csv`
- **Columns**:
  - `_id`: Video identifier (string)
  - `keyword_list[0]`: Product/keyword text (string)
- **Example**:
  ```csv
  _id,keyword_list[0]
  123456,sneakers
  789012,smartphone
  ```

### 5. Valid Scenes File (Generated)
- **Path**: `data/valid_scenes.csv`
- **Columns**:
  - `video_id`: Video identifier
  - `scene_number`: Scene number (int)
  - `keyword`: Product keyword
  - `scene_image_path`: Full path to scene image
  - `keyword_mask_path`: Full path to keyword mask
  - `attention_heatmap_path`: Full path to attention heatmap
  - `filename`: Original filename
- **Generated by**: `data_validation.ipynb`

---

## ğŸ”¬ Control Tensor Design

### V3 Control Tensor: `C_t = [M_t, A_t]`

**Channel 0: Keyword Mask (`M_t`)**
- Binary mask indicating product location
- Values: 0 (background) or 1 (keyword region)
- Derived from CLIPSeg segmentation
- Purpose: Tell model WHERE the product is

**Channel 1: Attention Heatmap (`A_t`)**
- Continuous heatmap indicating viewer attention
- Values: 0 (no attention) to 1 (high attention)
- From actual attention tracking
- Purpose: Tell model WHERE viewers are looking

**Alignment Score**: `alignment_score = mean(M_t âŠ™ A_t)`
- Scalar metric: proportion of attention on product
- Used for statistics and evaluation
- Not used in control tensor (we use full spatial `A_t`)

---

## ğŸ§ª Experimental Design

### Research Question
**Does the timing and intensity of attention-keyword alignment affect video advertisement effectiveness?**

### Independent Variables
1. **Variant type** (7 levels):
   - baseline, early_boost, middle_boost, late_boost, full_boost, reduction, placebo
2. **Temporal window** (3 levels):
   - early (first 33%), middle (middle 33%), late (last 33%)
3. **Manipulation type** (3 levels):
   - boost (Ã—1.5), reduction (Ã—0.5), none (control)

### Dependent Variables
1. **Engagement metrics**:
   - Click-through rate (CTR)
   - Conversion rate (CVR)
   - Watch time
2. **Attention metrics**:
   - Mean attention on product
   - Temporal attention profile
   - Alignment score

### Experimental Conditions

| Variant       | Temporal Window | Manipulation | Purpose                          |
|---------------|-----------------|--------------|----------------------------------|
| baseline      | -               | None         | Control condition                |
| early_boost   | First 33%       | Boost Ã—1.5   | Test early attention importance  |
| middle_boost  | Middle 33%      | Boost Ã—1.5   | Test middle attention importance |
| late_boost    | Last 33%        | Boost Ã—1.5   | Test late attention importance   |
| full_boost    | All scenes      | Boost Ã—1.5   | Test overall boost effect        |
| reduction     | Middle 33%      | Reduce Ã—0.5  | Test reduction effect            |
| placebo       | Background only | None         | Control for editing artifacts    |

---

## ğŸ“ˆ Evaluation and Analysis

### 1. Variant Statistics

```python
from src.video_editing.experimental_variants_v3 import VideoVariantGeneratorV3

generator = VideoVariantGeneratorV3(valid_scenes_file='data/valid_scenes.csv')
variants = generator.create_all_variants_for_video(video_id)
stats = generator.compute_variant_statistics(variants)

# Stats include:
# - mean_alignment: Average alignment across scenes
# - std_alignment: Standard deviation
# - min_alignment, max_alignment
# - temporal_profile: List of alignment scores per scene
```

### 2. Visualization

```python
from src.video_editing.experimental_variants_v3 import visualize_variant_comparison

visualize_variant_comparison(variants, video_id, keyword, generator)
# Creates 2 plots:
# - Alignment scores over scene numbers (line plot)
# - Mean alignment comparison (bar chart with error bars)
```

### 3. A/B Testing Analysis

After deploying variants:
```python
import pandas as pd

# Load engagement data
engagement_df = pd.read_csv('engagement_data.csv')
# Columns: video_id, variant, CTR, CVR, watch_time

# Compare variants
baseline_ctr = engagement_df[engagement_df['variant'] == 'baseline']['CTR'].mean()
early_boost_ctr = engagement_df[engagement_df['variant'] == 'early_boost']['CTR'].mean()

# Statistical significance
from scipy import stats
t_stat, p_value = stats.ttest_ind(
    engagement_df[engagement_df['variant'] == 'baseline']['CTR'],
    engagement_df[engagement_df['variant'] == 'early_boost']['CTR']
)
```

---

## ğŸ› ï¸ Troubleshooting

### Issue 1: Missing Files

**Error**: "No valid scenes found"

**Solution**:
1. Check directory structure:
   ```bash
   ls data/video_scene_cuts/{video_id}/
   ls keyword_masks/{video_id}/
   ls attention_heatmap/{video_id}/
   ```
2. Verify filename format: `{video_id}-Scene-001-01.jpg`
3. Run `data_validation.ipynb` to see detailed statistics

### Issue 2: Filename Parsing Errors

**Error**: "Could not parse scene filename"

**Solution**:
- Ensure filenames follow pattern: `{video_id}-Scene-{number:03d}-01.jpg`
- Examples:
  - âœ… `123456-Scene-001-01.jpg`
  - âœ… `abc123-Scene-042-01.jpg`
  - âŒ `123456_Scene_1.jpg`
  - âŒ `scene001.jpg`

### Issue 3: Memory Issues

**Error**: "CUDA out of memory"

**Solution**:
```python
# Reduce batch size
data_module = VideoSceneDataModuleV3(
    batch_size=2,  # Instead of 4
    ...
)

# Or use smaller image size
data_module = VideoSceneDataModuleV3(
    image_size=(256, 256),  # Instead of (512, 512)
    ...
)
```

---

## ğŸ“š API Reference

### VideoSceneDatasetV3

```python
class VideoSceneDatasetV3(Dataset):
    """Dataset with attention heatmaps."""

    def __init__(
        self,
        valid_scenes_file: str = "data/valid_scenes.csv",
        video_ids: Optional[List[str]] = None,
        image_size: Tuple[int, int] = (512, 512),
        transform: Optional[Callable] = None,
        normalize_heatmap: bool = True,
    )
```

### VideoVariantGeneratorV3

```python
class VideoVariantGeneratorV3:
    """Generate experimental variants."""

    def __init__(
        self,
        valid_scenes_file: str = "data/valid_scenes.csv",
        boost_alpha: float = 1.5,
        reduction_alpha: float = 0.5,
        image_size: Tuple[int, int] = (512, 512),
    )

    def create_all_variants_for_video(
        self,
        video_id: str,
        output_dir: str = "outputs/variants"
    ) -> Dict[str, pd.DataFrame]

    def generate_variants_for_all_videos(
        self,
        output_dir: str = "outputs/variants",
        max_videos: Optional[int] = None
    ) -> Dict[str, Dict[str, pd.DataFrame]]
```

---

## ğŸ¯ Next Steps

1. **Prepare Data**:
   - Ensure scene images are in `data/video_scene_cuts/`
   - Generate keyword masks with CLIPSeg
   - Place attention heatmaps in `attention_heatmap/`
   - Prepare `data/keywords.csv`

2. **Validate Data**:
   - Run `data_validation.ipynb`
   - Verify `data/valid_scenes.csv` is created
   - Check statistics (number of valid scenes)

3. **Train Model**:
   - Run `workflow_v3_with_attention_heatmaps.ipynb`
   - Train ControlNet (Step 4)
   - Monitor training loss

4. **Generate Variants**:
   - Generate all 7 variants (Step 5)
   - Visualize variant statistics
   - Verify modified attention heatmaps

5. **Run Inference**:
   - Generate edited scenes for each variant
   - Assemble videos
   - Apply temporal smoothing

6. **Deploy and Test**:
   - Deploy variants for A/B testing
   - Collect engagement metrics
   - Analyze results

---

## ğŸ“ Citation

If you use this framework in your research, please cite:

```bibtex
@software{video_alignment_framework_v3,
  title={Video Advertisement Alignment Manipulation Framework v3},
  author={Your Name},
  year={2024},
  version={3.0},
  description={Framework for manipulating attention-keyword alignment in video advertisements using actual attention heatmaps}
}
```

---

## ğŸ“ Support

For questions or issues:
1. Check this README
2. Review notebook comments in `workflow_v3_with_attention_heatmaps.ipynb`
3. Inspect example outputs in `outputs/variants_v3/`

---

**Version**: 3.0
**Last Updated**: 2024-11-19
**Status**: Production Ready âœ…

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet Fine-Tuning and Inference for Video Ad Manipulation (Updated)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data preprocessing** - Generate keyword masks using CLIPSeg\n",
    "2. **Data format requirements** - Work with alignment_score.csv and keywords.csv\n",
    "3. **Fine-tuning process** - Training the ControlNet adapter\n",
    "4. **Inference** - Generating 7 experimental video variants\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Data Format Specification](#data-format)\n",
    "3. [Preprocessing: Generate Keyword Masks](#preprocessing)\n",
    "4. [Data Preparation](#data-prep)\n",
    "5. [Model Fine-Tuning](#training)\n",
    "6. [Inference: Generate 7 Variants](#inference)\n",
    "7. [Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import framework modules\n",
    "from src.models import StableDiffusionControlNetWrapper\n",
    "from src.training import ControlNetTrainer\n",
    "from src.training.dataset_v2 import VideoSceneDataModule\n",
    "from src.data_preparation import ControlTensorBuilder\n",
    "from src.video_editing.experimental_variants_v2 import VideoVariantGenerator, visualize_variant_comparison\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Data Format Specification <a name=\"data-format\"></a>\n\n### Current Directory Structure\n\n```\ndata/\n├── alignment_score.csv          # Pre-computed alignment scores\n│   Columns:\n│   - video id: Video identifier\n│   - Scene Number: Scene index within video\n│   - attention_proportion: Alignment score [0-1]\n│   - start_time, end_time: Scene timing\n│   - CTR_mean, CVR_mean: Engagement metrics\n│   - contrast, brightness: Visual features\n│   - industry: Product category\n│\n├── keywords.csv                 # Video keywords\n│   Columns:\n│   - _id: Video identifier\n│   - keyword_list[0]: Product/keyword text\n│\n├── video_scene_cuts/            # RGB images per scene (not in repo)\n│   ├── {video_id}/\n│   │   ├── {video_id}-Scene-001-01.jpg\n│   │   ├── {video_id}-Scene-002-01.jpg\n│   │   └── ...\n│\n├── videos_tiktok/               # Original video files (not in repo)\n│   └── {video_id}.mp4\n│\n└── keyword_masks/               # Generated by preprocessing (see below)\n    ├── {video_id}/\n    │   ├── scene_1.png\n    │   ├── scene_2.png\n    │   └── ...\n```\n\n### Key Differences from Original Design\n\n1. **No raw attention heatmaps** - We have pre-computed `attention_proportion` scores\n2. **No raw keyword heatmaps** - We generate keyword masks using CLIPSeg\n3. **Scene-based instead of frame-based** - Data is organized by scenes, not individual frames\n4. **Alignment scores are scalars** - Not spatial maps"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment scores\n",
    "alignment_df = pd.read_csv('data/alignment_score.csv')\n",
    "print(\"Alignment Score Data:\")\n",
    "print(alignment_df.head())\n",
    "print(f\"\\nShape: {alignment_df.shape}\")\n",
    "print(f\"Unique videos: {alignment_df['video id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords\n",
    "keywords_df = pd.read_csv('data/keywords.csv')\n",
    "print(\"Keywords Data:\")\n",
    "print(keywords_df.head(10))\n",
    "print(f\"\\nShape: {keywords_df.shape}\")\n",
    "\n",
    "# Filter out empty keywords\n",
    "keywords_df_clean = keywords_df[keywords_df['keyword_list[0]'].notna() & (keywords_df['keyword_list[0]'] != '')]\n",
    "print(f\"Videos with valid keywords: {len(keywords_df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Preprocessing: Generate Keyword Masks <a name=\"preprocessing\"></a>\n",
    "\n",
    "Since we don't have keyword heatmaps, we use **CLIPSeg** to generate spatial masks indicating where the product appears in each scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Run Keyword Mask Generation Script\n\n**Note:** This step requires the `video_scene_cuts` directory to be present locally.\n\n```bash\npython scripts/generate_keyword_masks.py \\\n    --screenshots_dir data/video_scene_cuts \\\n    --keywords_file data/keywords.csv \\\n    --output_dir data/keyword_masks \\\n    --device cuda \\\n    --threshold 0.4\n```\n\nThis will:\n1. Load CLIPSeg model\n2. For each video with a keyword:\n   - Load all scene screenshots\n   - Run CLIPSeg to segment the product/keyword\n   - Save binary masks to `data/keyword_masks/`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if keyword masks exist\n",
    "keyword_masks_dir = 'data/keyword_masks'\n",
    "\n",
    "if os.path.exists(keyword_masks_dir):\n",
    "    mask_videos = os.listdir(keyword_masks_dir)\n",
    "    print(f\"✓ Keyword masks found for {len(mask_videos)} videos\")\n",
    "    print(f\"  Directory: {keyword_masks_dir}\")\n",
    "else:\n",
    "    print(\"✗ Keyword masks not found\")\n",
    "    print(\"  Please run: python scripts/generate_keyword_masks.py\")\n",
    "    print(\"  OR create dummy masks for testing (shown below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Masks for Testing (Optional)\n",
    "\n",
    "If you don't have the screenshots, you can create dummy masks for testing the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_masks_for_testing():\n",
    "    \"\"\"Create dummy keyword masks for testing when screenshots aren't available.\"\"\"\n",
    "    os.makedirs('data/keyword_masks', exist_ok=True)\n",
    "    \n",
    "    # Get unique video IDs from alignment_score.csv\n",
    "    video_ids = alignment_df['video id'].unique()[:5]  # Just first 5 for testing\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        # Get scenes for this video\n",
    "        scenes = alignment_df[alignment_df['video id'] == video_id]['Scene Number'].values\n",
    "        \n",
    "        # Create directory\n",
    "        video_mask_dir = os.path.join('data/keyword_masks', str(video_id))\n",
    "        os.makedirs(video_mask_dir, exist_ok=True)\n",
    "        \n",
    "        for scene_num in scenes:\n",
    "            # Create a dummy mask (centered rectangle)\n",
    "            mask = np.zeros((512, 512), dtype=np.uint8)\n",
    "            mask[150:350, 150:350] = 255  # Center square\n",
    "            \n",
    "            # Save mask\n",
    "            mask_path = os.path.join(video_mask_dir, f\"scene_{scene_num}.png\")\n",
    "            Image.fromarray(mask).save(mask_path)\n",
    "    \n",
    "    print(f\"✓ Created dummy masks for {len(video_ids)} videos\")\n",
    "\n",
    "# Uncomment to create dummy masks\n",
    "# create_dummy_masks_for_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preparation <a name=\"data-prep\"></a>\n",
    "\n",
    "### Build Control Tensors\n",
    "\n",
    "Control tensors are constructed from:\n",
    "- **keyword_mask** (M_t): From CLIPSeg segmentation\n",
    "- **alignment_score**: From alignment_score.csv\n",
    "\n",
    "**C_t = [M_t, S_t]** where:\n",
    "- **M_t**: Keyword mask (binary)\n",
    "- **S_t**: Alignment map = M_t × alignment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCONFIG = {\n    'data': {\n        'alignment_score_file': 'data/alignment_score.csv',\n        'keywords_file': 'data/keywords.csv',\n        'screenshots_dir': 'data/video_scene_cuts',\n        'keyword_masks_dir': 'data/keyword_masks',\n        'image_size': 512,\n    },\n    'model': {\n        'sd_model_name': 'runwayml/stable-diffusion-v1-5',\n        'controlnet': {\n            'control_channels': 2,  # [M_t, S_t]\n            'base_channels': 64,\n        },\n        'use_lora': False,\n    },\n    'training': {\n        'batch_size': 4,\n        'num_workers': 4,\n        'learning_rate': 1e-4,\n        'num_epochs': 10,\n        'lambda_recon': 1.0,\n        'lambda_lpips': 1.0,\n        'lambda_bg': 0.5,\n        'use_recon_loss': True,\n        'gradient_accumulation_steps': 1,\n        'mixed_precision': True,\n        'log_wandb': False,\n        'project_name': 'video-ad-manipulation',\n        'output_dir': 'outputs/training',\n    },\n}\n\n# Create output directory\nos.makedirs(CONFIG['training']['output_dir'], exist_ok=True)\n\n# Save config\nconfig_save_path = os.path.join(CONFIG['training']['output_dir'], 'config.yaml')\nwith open(config_save_path, 'w') as f:\n    yaml.dump(CONFIG, f, default_flow_style=False)\nprint(f\"Configuration saved to: {config_save_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get valid video IDs (only videos with both keywords and screenshots)\nvalid_videos = VideoSceneDataModule.get_valid_video_ids(\n    alignment_score_file=CONFIG['data']['alignment_score_file'],\n    keywords_file=CONFIG['data']['keywords_file'],\n    screenshots_dir=CONFIG['data']['screenshots_dir'],\n)\n\nprint(f\"\\n✓ Found {len(valid_videos)} valid videos\")\nprint(f\"  (videos with both keywords and screenshots)\")\n\n# Split: 80% train, 20% validation\nsplit_idx = int(0.8 * len(valid_videos))\ntrain_videos = valid_videos[:split_idx]\nval_videos = valid_videos[split_idx:]\n\nprint(f\"\\nTrain/Val Split:\")\nprint(f\"  Training videos: {len(train_videos)}\")\nprint(f\"  Validation videos: {len(val_videos)}\")\nprint(f\"\\nFirst 5 training videos: {train_videos[:5]}\")\nprint(f\"First 5 validation videos: {val_videos[:5]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = VideoSceneDataModule(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    train_videos=train_videos,\n",
    "    val_videos=val_videos,\n",
    "    screenshots_dir=CONFIG['data']['screenshots_dir'],\n",
    "    keyword_masks_dir=CONFIG['data']['keyword_masks_dir'],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    num_workers=CONFIG['training']['num_workers'],\n",
    "    image_size=(CONFIG['data']['image_size'], CONFIG['data']['image_size']),\n",
    ")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(data_module.train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(data_module.val_dataset)}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Training Batch Contents:\")\n",
    "print(f\"  'image' shape: {batch['image'].shape}\")  # [B, 3, 512, 512]\n",
    "print(f\"  'control' shape: {batch['control'].shape}\")  # [B, 2, 512, 512]\n",
    "print(f\"  'keyword_mask' shape: {batch['keyword_mask'].shape}\")  # [B, 1, 512, 512]\n",
    "print(f\"  'alignment_score': {batch['alignment_score'][:3].tolist()}...\")  # Scalars\n",
    "print(f\"  'keyword' (text prompts): {batch['keyword'][:2]}...\")\n",
    "print(f\"  'video_id': {batch['video_id'][:2]}...\")\n",
    "print(f\"  'scene_number': {batch['scene_number'][:3].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first sample in batch\n",
    "sample_idx = 0\n",
    "image = batch['image'][sample_idx].permute(1, 2, 0).numpy()\n",
    "image = (image * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "keyword_mask = batch['control'][sample_idx, 0].numpy()  # M_t\n",
    "alignment_map = batch['control'][sample_idx, 1].numpy()  # S_t\n",
    "alignment_score = batch['alignment_score'][sample_idx].item()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f\"Scene Image\\nVideo: {batch['video_id'][sample_idx]}\\nScene: {batch['scene_number'][sample_idx]}\\nKeyword: '{batch['keyword'][sample_idx]}'\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(keyword_mask, cmap='gray')\n",
    "axes[1].set_title(\"Control Channel 0 (M_t)\\nKeyword Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(alignment_map, cmap='hot')\n",
    "axes[2].set_title(f\"Control Channel 1 (S_t)\\nAlignment Map\\nScore: {alignment_score:.4f}\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Fine-Tuning <a name=\"training\"></a>\n",
    "\n",
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Stable Diffusion + ControlNet model...\")\n",
    "print(\"This may take a few minutes on first run (downloading pretrained weights)\\n\")\n",
    "\n",
    "model = StableDiffusionControlNetWrapper(\n",
    "    sd_model_name=CONFIG['model']['sd_model_name'],\n",
    "    controlnet_config=CONFIG['model']['controlnet'],\n",
    "    device=device,\n",
    "    use_lora=CONFIG['model']['use_lora'],\n",
    ")\n",
    "\n",
    "print(\"✓ Model initialized successfully\\n\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  SD backbone: {CONFIG['model']['sd_model_name']}\")\n",
    "print(f\"  ControlNet input channels: {CONFIG['model']['controlnet']['control_channels']}\")\n",
    "print(f\"  Using LoRA: {CONFIG['model']['use_lora']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ControlNetTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    learning_rate=CONFIG['training']['learning_rate'],\n",
    "    num_epochs=CONFIG['training']['num_epochs'],\n",
    "    device=device,\n",
    "    output_dir=CONFIG['training']['output_dir'],\n",
    "    lambda_recon=CONFIG['training']['lambda_recon'],\n",
    "    lambda_lpips=CONFIG['training']['lambda_lpips'],\n",
    "    lambda_bg=CONFIG['training']['lambda_bg'],\n",
    "    use_recon_loss=CONFIG['training']['use_recon_loss'],\n",
    "    gradient_accumulation_steps=CONFIG['training']['gradient_accumulation_steps'],\n",
    "    mixed_precision=CONFIG['training']['mixed_precision'],\n",
    "    log_wandb=CONFIG['training']['log_wandb'],\n",
    "    project_name=CONFIG['training']['project_name'],\n",
    ")\n",
    "\n",
    "print(f\"Starting training for {CONFIG['training']['num_epochs']} epochs...\\n\")\n",
    "\n",
    "# Train\n",
    "# trainer.train()  # Uncomment to start training\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model saved to: {os.path.join(CONFIG['training']['output_dir'], 'best_model.pt')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Inference: Generate 7 Experimental Variants <a name=\"inference\"></a>\n",
    "\n",
    "### Define Experimental Variants\n",
    "\n",
    "We generate **7 variants** for each video:\n",
    "\n",
    "1. **baseline**: Original alignment scores (control condition)\n",
    "2. **early_boost**: Boost alignment in first 33% of scenes (×1.5)\n",
    "3. **middle_boost**: Boost alignment in middle 33% of scenes (×1.5)\n",
    "4. **late_boost**: Boost alignment in last 33% of scenes (×1.5)\n",
    "5. **full_boost**: Boost alignment in all scenes (×1.5)\n",
    "6. **reduction**: Reduce alignment in middle 33% of scenes (×0.5)\n",
    "7. **placebo**: Modify non-keyword regions only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variant Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variant generator\n",
    "variant_generator = VideoVariantGenerator(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    boost_alpha=1.5,\n",
    "    reduction_alpha=0.5,\n",
    ")\n",
    "\n",
    "print(\"Variant generator initialized\")\n",
    "print(f\"  Boost alpha: 1.5\")\n",
    "print(f\"  Reduction alpha: 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate variants for a valid video (example)\n# Use one of the valid videos from our dataset\nif len(valid_videos) > 0:\n    example_video_id = valid_videos[0]\n    print(f\"Generating variants for video: {example_video_id}\")\n\n    variants = variant_generator.create_all_variants_for_video(example_video_id)\n\n    print(f\"\\n✓ Generated {len(variants)} variants:\")\n    for variant_name in variants.keys():\n        print(f\"  - {variant_name}\")\nelse:\n    print(\"⚠️ No valid videos found. Please check your data.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display statistics\n",
    "stats = variant_generator.compute_variant_statistics(variants)\n",
    "\n",
    "print(\"\\nVariant Statistics:\\n\")\n",
    "for variant_name, stat in stats.items():\n",
    "    print(f\"{variant_name:15s}: mean={stat['mean_alignment']:.4f}, std={stat['std_alignment']:.4f}, scenes={stat['num_scenes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualization <a name=\"visualization\"></a>\n",
    "\n",
    "### Visualize Variant Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyword for this video\n",
    "keyword = variant_generator.keywords.get(str(example_video_id), \"unknown\")\n",
    "\n",
    "# Visualize alignment profiles\n",
    "visualize_variant_comparison(variants, example_video_id, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variants for All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for all videos\n",
    "all_variants = variant_generator.generate_variants_for_all_videos(\n",
    "    output_dir='outputs/variants'\n",
    ")\n",
    "\n",
    "# Save manifest\n",
    "variant_generator.save_variant_manifest(\n",
    "    all_variants,\n",
    "    output_path='outputs/variants/manifest.json'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated variants for {len(all_variants)} videos\")\n",
    "print(f\"  Output directory: outputs/variants/\")\n",
    "print(f\"  Manifest: outputs/variants/manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Variant Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manifest\n",
    "with open('outputs/variants/manifest.json', 'r') as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(\"Variant Generation Manifest:\")\n",
    "print(f\"  Total videos: {manifest['num_videos']}\")\n",
    "print(f\"  Variants per video: {manifest['num_variants_per_video']}\")\n",
    "print(f\"  Variant types: {manifest['variant_types']}\")\n",
    "print(f\"\\nFirst 3 videos:\")\n",
    "for video_id, info in list(manifest['videos'].items())[:3]:\n",
    "    print(f\"  {video_id}: {info['num_scenes']} scenes, keyword='{info['keyword']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Updated Workflow\n",
    "\n",
    "**Data Preparation:**\n",
    "1. Load `alignment_score.csv` with pre-computed alignment scores\n",
    "2. Load `keywords.csv` with product keywords\n",
    "3. Use CLIPSeg to generate keyword masks from screenshots\n",
    "4. Build control tensors: `C_t = [M_t, S_t]`\n",
    "   - `M_t` = keyword mask from CLIPSeg\n",
    "   - `S_t` = M_t × alignment_score\n",
    "\n",
    "**Training:**\n",
    "- Fine-tune ControlNet adapter\n",
    "- Freeze Stable Diffusion backbone\n",
    "- Train to manipulate scenes based on keyword masks and alignment scores\n",
    "\n",
    "**Inference (7 Variants):**\n",
    "For each video, generate 7 variants by modulating alignment scores:\n",
    "1. **baseline**: No change\n",
    "2. **early_boost**: Boost first 33% of scenes\n",
    "3. **middle_boost**: Boost middle 33%\n",
    "4. **late_boost**: Boost last 33%\n",
    "5. **full_boost**: Boost all scenes\n",
    "6. **reduction**: Reduce middle 33%\n",
    "7. **placebo**: Modify non-keyword regions\n",
    "\n",
    "**Output:**\n",
    "- Variant specifications saved to `outputs/variants/{video_id}/{variant_name}.csv`\n",
    "- Statistics saved to `outputs/variants/{video_id}/statistics.json`\n",
    "- Manifest saved to `outputs/variants/manifest.json`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run preprocessing to generate keyword masks\n",
    "2. Train ControlNet model\n",
    "3. Run inference to generate edited scenes\n",
    "4. Reassemble scenes into video files\n",
    "5. Deploy variants for A/B testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
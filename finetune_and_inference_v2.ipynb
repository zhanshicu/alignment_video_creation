{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet Fine-Tuning and Inference for Video Ad Manipulation (Updated)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data validation** - Get valid video IDs from alignment_score.csv\n",
    "2. **Quick training** - Use subset of videos for fast experimentation\n",
    "3. **Fine-tuning process** - Training the ControlNet adapter\n",
    "4. **Inference** - Generating 7 experimental video variants\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Data Validation](#data-validation)\n",
    "3. [Dataset Preparation](#data-prep)\n",
    "4. [Model Fine-Tuning](#training)\n",
    "5. [Inference: Generate 7 Variants](#inference)\n",
    "6. [Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import framework modules\n",
    "from src.models import StableDiffusionControlNetWrapper\n",
    "from src.training import (\n",
    "    ControlNetTrainer,\n",
    "    get_valid_video_ids,\n",
    "    split_train_val_videos,\n",
    "    print_dataset_statistics,\n",
    ")\n",
    "from src.training.dataset_v2 import VideoSceneDataModule\n",
    "from src.data_preparation import ControlTensorBuilder\n",
    "from src.video_editing.experimental_variants_v2 import VideoVariantGenerator, visualize_variant_comparison\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Validation <a name=\"data-validation\"></a>\n",
    "\n",
    "**IMPORTANT:** Use `alignment_score.csv` as source of truth for video IDs.\n",
    "\n",
    "Some videos in `keywords.csv` may not have alignment scores, so we take the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ALIGNMENT_SCORE_FILE = 'data/alignment_score.csv'\n",
    "KEYWORDS_FILE = 'data/keywords.csv'\n",
    "SCREENSHOTS_DIR = 'data/screenshots_tiktok'\n",
    "KEYWORD_MASKS_DIR = 'data/keyword_masks'\n",
    "\n",
    "# Get valid video IDs (intersection of alignment_score.csv and keywords.csv)\n",
    "print(\"Validating video IDs...\\n\")\n",
    "valid_video_ids = get_valid_video_ids(\n",
    "    alignment_score_file=ALIGNMENT_SCORE_FILE,\n",
    "    keywords_file=KEYWORDS_FILE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Found {len(valid_video_ids)} valid videos\")\n",
    "print(f\"First 10 video IDs: {valid_video_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Training Mode\n",
    "\n",
    "**For fast experimentation**, you can limit the number of videos used for training.\n",
    "\n",
    "Set `USE_SUBSET = True` and `NUM_VIDEOS` to a small number (e.g., 10-20) for quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION: Quick Training Mode\n",
    "# ========================================\n",
    "\n",
    "# Set to True for quick training with subset of videos\n",
    "USE_SUBSET = True\n",
    "\n",
    "# Number of videos to use (only if USE_SUBSET=True)\n",
    "NUM_VIDEOS = 10  # Use 10 videos for quick experimentation\n",
    "\n",
    "# ========================================\n",
    "\n",
    "if USE_SUBSET:\n",
    "    # Use only first NUM_VIDEOS for quick training\n",
    "    video_ids_for_training = valid_video_ids[:NUM_VIDEOS]\n",
    "    print(f\"\\n‚ö° QUICK TRAINING MODE\")\n",
    "    print(f\"Using {len(video_ids_for_training)} videos out of {len(valid_video_ids)} available\")\n",
    "    print(f\"Videos: {video_ids_for_training}\")\n",
    "else:\n",
    "    # Use all valid videos\n",
    "    video_ids_for_training = valid_video_ids\n",
    "    print(f\"\\nüìö FULL TRAINING MODE\")\n",
    "    print(f\"Using all {len(video_ids_for_training)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Preparation <a name=\"data-prep\"></a>\n",
    "\n",
    "### Split into Train/Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split videos into train/val (80/20)\n",
    "train_videos, val_videos = split_train_val_videos(\n",
    "    video_ids=video_ids_for_training,\n",
    "    val_ratio=0.2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain videos ({len(train_videos)}): {train_videos}\")\n",
    "print(f\"Val videos ({len(val_videos)}): {val_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed statistics\n",
    "print_dataset_statistics(\n",
    "    alignment_score_file=ALIGNMENT_SCORE_FILE,\n",
    "    train_videos=train_videos,\n",
    "    val_videos=val_videos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'alignment_score_file': ALIGNMENT_SCORE_FILE,\n",
    "        'keywords_file': KEYWORDS_FILE,\n",
    "        'screenshots_dir': SCREENSHOTS_DIR,\n",
    "        'keyword_masks_dir': KEYWORD_MASKS_DIR,\n",
    "        'image_size': 512,\n",
    "    },\n",
    "    'model': {\n",
    "        'sd_model_name': 'runwayml/stable-diffusion-v1-5',\n",
    "        'controlnet': {\n",
    "            'control_channels': 2,  # [M_t, S_t]\n",
    "            'base_channels': 64,\n",
    "        },\n",
    "        'use_lora': False,\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 4,\n",
    "        'num_workers': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'num_epochs': 5 if USE_SUBSET else 10,  # Fewer epochs for subset\n",
    "        'lambda_recon': 1.0,\n",
    "        'lambda_lpips': 1.0,\n",
    "        'lambda_bg': 0.5,\n",
    "        'use_recon_loss': True,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'mixed_precision': True,\n",
    "        'log_wandb': False,\n",
    "        'project_name': 'video-ad-manipulation',\n",
    "        'output_dir': 'outputs/training_subset' if USE_SUBSET else 'outputs/training_full',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['training']['output_dir'], exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "config_save_path = os.path.join(CONFIG['training']['output_dir'], 'config.yaml')\n",
    "with open(config_save_path, 'w') as f:\n",
    "    yaml.dump(CONFIG, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Mode: {'SUBSET' if USE_SUBSET else 'FULL'}\")\n",
    "print(f\"  Videos: {len(train_videos)} train, {len(val_videos)} val\")\n",
    "print(f\"  Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"  Output: {CONFIG['training']['output_dir']}\")\n",
    "print(f\"\\nConfig saved to: {config_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "print(\"Creating data loaders...\\n\")\n",
    "\n",
    "data_module = VideoSceneDataModule(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    train_videos=train_videos,\n",
    "    val_videos=val_videos,\n",
    "    screenshots_dir=CONFIG['data']['screenshots_dir'],\n",
    "    keyword_masks_dir=CONFIG['data']['keyword_masks_dir'],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    num_workers=CONFIG['training']['num_workers'],\n",
    "    image_size=(CONFIG['data']['image_size'], CONFIG['data']['image_size']),\n",
    ")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "print(f\"\\n‚úì Data loaders created:\")\n",
    "print(f\"  Training scenes: {len(data_module.train_dataset)}\")\n",
    "print(f\"  Validation scenes: {len(data_module.val_dataset)}\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Training Batch Contents:\")\n",
    "print(f\"  'image' shape: {batch['image'].shape}\")  # [B, 3, 512, 512]\n",
    "print(f\"  'control' shape: {batch['control'].shape}\")  # [B, 2, 512, 512]\n",
    "print(f\"  'keyword_mask' shape: {batch['keyword_mask'].shape}\")  # [B, 1, 512, 512]\n",
    "print(f\"  'alignment_score': {batch['alignment_score'][:3].tolist()}...\")  # Scalars\n",
    "print(f\"  'keyword' (text prompts): {batch['keyword'][:2]}...\")\n",
    "print(f\"  'video_id': {batch['video_id'][:2]}...\")\n",
    "print(f\"  'scene_number': {batch['scene_number'][:3].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first sample in batch\n",
    "sample_idx = 0\n",
    "image = batch['image'][sample_idx].permute(1, 2, 0).numpy()\n",
    "image = (image * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "keyword_mask = batch['control'][sample_idx, 0].numpy()  # M_t\n",
    "alignment_map = batch['control'][sample_idx, 1].numpy()  # S_t\n",
    "alignment_score = batch['alignment_score'][sample_idx].item()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f\"Scene Image\\nVideo: {batch['video_id'][sample_idx]}\\nScene: {batch['scene_number'][sample_idx]}\\nKeyword: '{batch['keyword'][sample_idx]}'\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(keyword_mask, cmap='gray')\n",
    "axes[1].set_title(\"Control Channel 0 (M_t)\\nKeyword Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(alignment_map, cmap='hot')\n",
    "axes[2].set_title(f\"Control Channel 1 (S_t)\\nAlignment Map\\nScore: {alignment_score:.4f}\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Fine-Tuning <a name=\"training\"></a>\n",
    "\n",
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Stable Diffusion + ControlNet model...\")\n",
    "print(\"This may take a few minutes on first run (downloading pretrained weights)\\n\")\n",
    "\n",
    "model = StableDiffusionControlNetWrapper(\n",
    "    sd_model_name=CONFIG['model']['sd_model_name'],\n",
    "    controlnet_config=CONFIG['model']['controlnet'],\n",
    "    device=device,\n",
    "    use_lora=CONFIG['model']['use_lora'],\n",
    ")\n",
    "\n",
    "print(\"‚úì Model initialized successfully\\n\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  SD backbone: {CONFIG['model']['sd_model_name']}\")\n",
    "print(f\"  ControlNet input channels: {CONFIG['model']['controlnet']['control_channels']}\")\n",
    "print(f\"  Using LoRA: {CONFIG['model']['use_lora']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "**Note:** Uncomment `trainer.train()` to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ControlNetTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    learning_rate=CONFIG['training']['learning_rate'],\n",
    "    num_epochs=CONFIG['training']['num_epochs'],\n",
    "    device=device,\n",
    "    output_dir=CONFIG['training']['output_dir'],\n",
    "    lambda_recon=CONFIG['training']['lambda_recon'],\n",
    "    lambda_lpips=CONFIG['training']['lambda_lpips'],\n",
    "    lambda_bg=CONFIG['training']['lambda_bg'],\n",
    "    use_recon_loss=CONFIG['training']['use_recon_loss'],\n",
    "    gradient_accumulation_steps=CONFIG['training']['gradient_accumulation_steps'],\n",
    "    mixed_precision=CONFIG['training']['mixed_precision'],\n",
    "    log_wandb=CONFIG['training']['log_wandb'],\n",
    "    project_name=CONFIG['training']['project_name'],\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'SUBSET (' + str(NUM_VIDEOS) + ' videos)' if USE_SUBSET else 'FULL (' + str(len(valid_video_ids)) + ' videos)'}\")\n",
    "print(f\"Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"Training scenes: {len(data_module.train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Output directory: {CONFIG['training']['output_dir']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train (UNCOMMENT TO START TRAINING)\n",
    "# trainer.train()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Training not started (trainer.train() is commented out)\")\n",
    "print(\"   Uncomment the line above to start training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Inference: Generate 7 Experimental Variants <a name=\"inference\"></a>\n",
    "\n",
    "### Variant Definitions\n",
    "\n",
    "We generate **7 variants** for each video:\n",
    "\n",
    "1. **baseline**: Original alignment scores (control condition)\n",
    "2. **early_boost**: Boost alignment in first 33% of scenes (√ó1.5)\n",
    "3. **middle_boost**: Boost alignment in middle 33% of scenes (√ó1.5)\n",
    "4. **late_boost**: Boost alignment in last 33% of scenes (√ó1.5)\n",
    "5. **full_boost**: Boost alignment in all scenes (√ó1.5)\n",
    "6. **reduction**: Reduce alignment in middle 33% of scenes (√ó0.5)\n",
    "7. **placebo**: Modify non-keyword regions only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variant Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variant generator\n",
    "variant_generator = VideoVariantGenerator(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    boost_alpha=1.5,\n",
    "    reduction_alpha=0.5,\n",
    ")\n",
    "\n",
    "print(\"‚úì Variant generator initialized\")\n",
    "print(f\"  Boost alpha: 1.5\")\n",
    "print(f\"  Reduction alpha: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Generate Variants for Single Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for a single video (example)\n",
    "example_video_id = valid_video_ids[0]\n",
    "print(f\"Generating variants for video: {example_video_id}\\n\")\n",
    "\n",
    "variants = variant_generator.create_all_variants_for_video(example_video_id)\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(variants)} variants:\")\n",
    "for variant_name in variants.keys():\n",
    "    print(f\"  - {variant_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display statistics\n",
    "stats = variant_generator.compute_variant_statistics(variants)\n",
    "\n",
    "print(\"\\nVariant Statistics:\\n\")\n",
    "print(f\"{'Variant':<15} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10} {'Scenes'}\")\n",
    "print(\"-\" * 65)\n",
    "for variant_name, stat in stats.items():\n",
    "    print(f\"{variant_name:<15} {stat['mean_alignment']:<10.4f} {stat['std_alignment']:<10.4f} {stat['min_alignment']:<10.4f} {stat['max_alignment']:<10.4f} {stat['num_scenes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization <a name=\"visualization\"></a>\n",
    "\n",
    "### Visualize Variant Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyword for this video\n",
    "keyword = variant_generator.keywords.get(str(example_video_id), \"unknown\")\n",
    "\n",
    "# Visualize alignment profiles\n",
    "visualize_variant_comparison(variants, example_video_id, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variants for All Videos\n",
    "\n",
    "This generates variant specifications (CSV files) for all valid videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for all videos\n",
    "output_dir = 'outputs/variants_subset' if USE_SUBSET else 'outputs/variants_full'\n",
    "\n",
    "print(f\"Generating variants for all videos...\")\n",
    "print(f\"Output directory: {output_dir}\\n\")\n",
    "\n",
    "all_variants = variant_generator.generate_variants_for_all_videos(\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Save manifest\n",
    "manifest_path = os.path.join(output_dir, 'manifest.json')\n",
    "variant_generator.save_variant_manifest(\n",
    "    all_variants,\n",
    "    output_path=manifest_path\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Variant generation complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Generated variants for {len(all_variants)} videos\")\n",
    "print(f\"  Output directory: {output_dir}/\")\n",
    "print(f\"  Manifest: {manifest_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Variant Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display manifest\n",
    "with open(manifest_path, 'r') as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(\"Variant Generation Manifest:\")\n",
    "print(f\"  Total videos: {manifest['num_videos']}\")\n",
    "print(f\"  Variants per video: {manifest['num_variants_per_video']}\")\n",
    "print(f\"  Boost alpha: {manifest['boost_alpha']}\")\n",
    "print(f\"  Reduction alpha: {manifest['reduction_alpha']}\")\n",
    "print(f\"\\n  Variant types:\")\n",
    "for vtype in manifest['variant_types']:\n",
    "    print(f\"    - {vtype}\")\n",
    "\n",
    "print(f\"\\nFirst 5 videos:\")\n",
    "for video_id, info in list(manifest['videos'].items())[:5]:\n",
    "    print(f\"  {video_id}: {info['num_scenes']} scenes, keyword='{info['keyword']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "\n",
    "1. **Validated video IDs** using alignment_score.csv as source of truth\n",
    "2. **Created train/val split** at video level (not scene level)\n",
    "3. **Configured quick training mode** to use subset of videos\n",
    "4. **Prepared data loaders** with proper filtering\n",
    "5. **Generated 7 experimental variants** for all videos\n",
    "\n",
    "### Output Files\n",
    "\n",
    "```\n",
    "outputs/\n",
    "‚îú‚îÄ‚îÄ training_subset/          # Training outputs (subset mode)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ best_model.pt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ training_history.json\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ variants_subset/          # Variant specifications (subset mode)\n",
    "    ‚îú‚îÄ‚îÄ manifest.json\n",
    "    ‚îî‚îÄ‚îÄ {video_id}/\n",
    "        ‚îú‚îÄ‚îÄ baseline.csv\n",
    "        ‚îú‚îÄ‚îÄ early_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ middle_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ late_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ full_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ reduction.csv\n",
    "        ‚îú‚îÄ‚îÄ placebo.csv\n",
    "        ‚îî‚îÄ‚îÄ statistics.json\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Train model**: Uncomment `trainer.train()` to start training\n",
    "2. **Run inference**: Use trained model to generate edited scenes\n",
    "3. **Reassemble videos**: Combine edited scenes into video files\n",
    "4. **Deploy for A/B testing**: Upload variants for experimental study\n",
    "\n",
    "### Quick vs Full Training\n",
    "\n",
    "- **Quick mode** (`USE_SUBSET=True`, `NUM_VIDEOS=10`):\n",
    "  - Fast experimentation\n",
    "  - Test the pipeline\n",
    "  - 5 epochs\n",
    "  - ~10-20 minutes on GPU\n",
    "\n",
    "- **Full mode** (`USE_SUBSET=False`):\n",
    "  - Production training\n",
    "  - All valid videos\n",
    "  - 10 epochs\n",
    "  - Several hours on GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet Fine-Tuning and Inference for Video Ad Manipulation (Updated)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data validation** - Get valid video IDs from alignment_score.csv\n",
    "2. **Keyword mask generation** - Use CLIPSeg to generate spatial masks\n",
    "3. **Quick training** - Use subset of videos for fast experimentation\n",
    "4. **Fine-tuning process** - Training the ControlNet adapter\n",
    "5. **Inference** - Generating 7 experimental video variants\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Data Validation](#data-validation)\n",
    "3. [Generate Keyword Masks](#generate-masks)\n",
    "4. [Dataset Preparation](#data-prep)\n",
    "5. [Model Fine-Tuning](#training)\n",
    "6. [Inference: Generate 7 Variants](#inference)\n",
    "7. [Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import framework modules\n",
    "from src.models import StableDiffusionControlNetWrapper\n",
    "from src.training import (\n",
    "    ControlNetTrainer,\n",
    "    get_valid_video_ids,\n",
    "    split_train_val_videos,\n",
    "    print_dataset_statistics,\n",
    ")\n",
    "from src.training.dataset_v2 import VideoSceneDataModule\n",
    "from src.data_preparation import ControlTensorBuilder\n",
    "from src.video_editing.experimental_variants_v2 import VideoVariantGenerator, visualize_variant_comparison\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Validation <a name=\"data-validation\"></a>\n",
    "\n",
    "**IMPORTANT:** Use `alignment_score.csv` as source of truth for video IDs.\n",
    "\n",
    "Some videos in `keywords.csv` may not have alignment scores, so we take the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ALIGNMENT_SCORE_FILE = 'data/alignment_score.csv'\n",
    "KEYWORDS_FILE = 'data/keywords.csv'\n",
    "SCREENSHOTS_DIR = 'data/screenshots_tiktok'\n",
    "KEYWORD_MASKS_DIR = 'data/keyword_masks'\n",
    "\n",
    "# Get valid video IDs (intersection of alignment_score.csv and keywords.csv)\n",
    "print(\"Validating video IDs...\\n\")\n",
    "valid_video_ids = get_valid_video_ids(\n",
    "    alignment_score_file=ALIGNMENT_SCORE_FILE,\n",
    "    keywords_file=KEYWORDS_FILE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Found {len(valid_video_ids)} valid videos\")\n",
    "print(f\"First 10 video IDs: {valid_video_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Training Mode\n",
    "\n",
    "**For fast experimentation**, you can limit the number of videos used for training.\n",
    "\n",
    "Set `USE_SUBSET = True` and `NUM_VIDEOS` to a small number (e.g., 10-20) for quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION: Quick Training Mode\n",
    "# ========================================\n",
    "\n",
    "# Set to True for quick training with subset of videos\n",
    "USE_SUBSET = True\n",
    "\n",
    "# Number of videos to use (only if USE_SUBSET=True)\n",
    "NUM_VIDEOS = 10  # Use 10 videos for quick experimentation\n",
    "\n",
    "# ========================================\n",
    "\n",
    "if USE_SUBSET:\n",
    "    # Use only first NUM_VIDEOS for quick training\n",
    "    video_ids_for_training = valid_video_ids[:NUM_VIDEOS]\n",
    "    print(f\"\\n‚ö° QUICK TRAINING MODE\")\n",
    "    print(f\"Using {len(video_ids_for_training)} videos out of {len(valid_video_ids)} available\")\n",
    "    print(f\"Videos: {video_ids_for_training}\")\n",
    "else:\n",
    "    # Use all valid videos\n",
    "    video_ids_for_training = valid_video_ids\n",
    "    print(f\"\\nüìö FULL TRAINING MODE\")\n",
    "    print(f\"Using all {len(video_ids_for_training)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Generate Keyword Masks <a name=\"generate-masks\"></a>\n",
    "\n",
    "Since we don't have pre-generated keyword masks, we'll use **CLIPSeg** to segment products in the screenshots.\n",
    "\n",
    "This creates spatial masks showing where the product appears in each scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if keyword masks already exist\n",
    "def check_keyword_masks_exist(video_ids, masks_dir):\n",
    "    \"\"\"Check if keyword masks exist for all videos.\"\"\"\n",
    "    if not os.path.exists(masks_dir):\n",
    "        return False\n",
    "    \n",
    "    # Check if at least some masks exist\n",
    "    for video_id in video_ids[:3]:  # Check first 3 videos\n",
    "        video_mask_dir = os.path.join(masks_dir, str(video_id))\n",
    "        if not os.path.exists(video_mask_dir):\n",
    "            return False\n",
    "        if len(os.listdir(video_mask_dir)) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "masks_exist = check_keyword_masks_exist(video_ids_for_training, KEYWORD_MASKS_DIR)\n",
    "\n",
    "if masks_exist:\n",
    "    print(\"‚úì Keyword masks already exist\")\n",
    "    print(f\"  Directory: {KEYWORD_MASKS_DIR}\")\n",
    "    GENERATE_MASKS = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Keyword masks not found\")\n",
    "    print(f\"  Will generate masks using CLIPSeg\")\n",
    "    GENERATE_MASKS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CLIPSeg Model\n",
    "\n",
    "CLIPSeg is a zero-shot image segmentation model that can segment objects based on text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_MASKS:\n",
    "    from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    "    \n",
    "    print(\"Loading CLIPSeg model...\")\n",
    "    print(\"(This may take a few minutes on first run)\\n\")\n",
    "    \n",
    "    clipseg_processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "    clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "    clipseg_model.to(device)\n",
    "    clipseg_model.eval()\n",
    "    \n",
    "    print(\"‚úì CLIPSeg model loaded successfully\")\n",
    "else:\n",
    "    print(\"Skipping CLIPSeg model loading (masks already exist)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Keyword Masks for All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_MASKS:\n",
    "    # Load keywords and alignment scores\n",
    "    alignment_df = pd.read_csv(ALIGNMENT_SCORE_FILE)\n",
    "    alignment_df.columns = alignment_df.columns.str.strip()\n",
    "    \n",
    "    keywords_df = pd.read_csv(KEYWORDS_FILE)\n",
    "    keywords_df.columns = keywords_df.columns.str.strip()\n",
    "    \n",
    "    # Create keyword mapping\n",
    "    if '_id' in keywords_df.columns:\n",
    "        video_id_col = '_id'\n",
    "    else:\n",
    "        video_id_col = 'video_id'\n",
    "    \n",
    "    if 'keyword_list[0]' in keywords_df.columns:\n",
    "        keyword_col = 'keyword_list[0]'\n",
    "    else:\n",
    "        keyword_col = 'keyword'\n",
    "    \n",
    "    keywords = dict(zip(\n",
    "        keywords_df[video_id_col].astype(str),\n",
    "        keywords_df[keyword_col]\n",
    "    ))\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(KEYWORD_MASKS_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nGenerating keyword masks for {len(video_ids_for_training)} videos...\")\n",
    "    print(f\"Output directory: {KEYWORD_MASKS_DIR}\\n\")\n",
    "    \n",
    "    total_masks_generated = 0\n",
    "    \n",
    "    for video_id in tqdm(video_ids_for_training, desc=\"Processing videos\"):\n",
    "        # Get keyword for this video\n",
    "        keyword = keywords.get(str(video_id))\n",
    "        if not keyword or pd.isna(keyword):\n",
    "            print(f\"  ‚ö†Ô∏è  Skipping video {video_id}: no keyword\")\n",
    "            continue\n",
    "        \n",
    "        # Get scenes for this video\n",
    "        video_scenes = alignment_df[alignment_df['video id'] == video_id]\n",
    "        scene_numbers = video_scenes['Scene Number'].values\n",
    "        \n",
    "        # Create output directory for this video\n",
    "        video_mask_dir = os.path.join(KEYWORD_MASKS_DIR, str(video_id))\n",
    "        os.makedirs(video_mask_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each scene\n",
    "        for scene_num in scene_numbers:\n",
    "            # Find screenshot\n",
    "            screenshot_path = os.path.join(SCREENSHOTS_DIR, str(video_id), f\"scene_{scene_num}.png\")\n",
    "            if not os.path.exists(screenshot_path):\n",
    "                screenshot_path = os.path.join(SCREENSHOTS_DIR, str(video_id), f\"scene_{scene_num:02d}.png\")\n",
    "            \n",
    "            if not os.path.exists(screenshot_path):\n",
    "                continue\n",
    "            \n",
    "            # Load screenshot\n",
    "            image = Image.open(screenshot_path).convert('RGB')\n",
    "            original_size = image.size\n",
    "            \n",
    "            # Run CLIPSeg\n",
    "            inputs = clipseg_processor(\n",
    "                text=[keyword],\n",
    "                images=[image],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = clipseg_model(**inputs)\n",
    "            \n",
    "            # Get mask\n",
    "            preds = outputs.logits[0]  # [H, W]\n",
    "            mask = torch.sigmoid(preds).cpu().numpy()\n",
    "            \n",
    "            # Resize to original size\n",
    "            mask_pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "            mask_pil = mask_pil.resize(original_size, Image.BILINEAR)\n",
    "            \n",
    "            # Apply threshold\n",
    "            mask_array = np.array(mask_pil).astype(np.float32) / 255.0\n",
    "            mask_binary = (mask_array > 0.4).astype(np.uint8) * 255\n",
    "            \n",
    "            # Save mask\n",
    "            mask_output = Image.fromarray(mask_binary)\n",
    "            mask_path = os.path.join(video_mask_dir, f\"scene_{scene_num}.png\")\n",
    "            mask_output.save(mask_path)\n",
    "            \n",
    "            total_masks_generated += 1\n",
    "    \n",
    "    print(f\"\\n‚úì Keyword mask generation complete!\")\n",
    "    print(f\"  Generated {total_masks_generated} masks for {len(video_ids_for_training)} videos\")\n",
    "    print(f\"  Output directory: {KEYWORD_MASKS_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping mask generation (masks already exist)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Example Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an example screenshot and its keyword mask\n",
    "example_video = video_ids_for_training[0]\n",
    "\n",
    "# Load alignment data to get a scene number\n",
    "alignment_df = pd.read_csv(ALIGNMENT_SCORE_FILE)\n",
    "alignment_df.columns = alignment_df.columns.str.strip()\n",
    "example_scenes = alignment_df[alignment_df['video id'] == example_video]\n",
    "example_scene_num = example_scenes.iloc[0]['Scene Number']\n",
    "\n",
    "# Load screenshot\n",
    "screenshot_path = os.path.join(SCREENSHOTS_DIR, str(example_video), f\"scene_{example_scene_num}.png\")\n",
    "if not os.path.exists(screenshot_path):\n",
    "    screenshot_path = os.path.join(SCREENSHOTS_DIR, str(example_video), f\"scene_{example_scene_num:02d}.png\")\n",
    "\n",
    "# Load mask\n",
    "mask_path = os.path.join(KEYWORD_MASKS_DIR, str(example_video), f\"scene_{example_scene_num}.png\")\n",
    "\n",
    "if os.path.exists(screenshot_path) and os.path.exists(mask_path):\n",
    "    screenshot = Image.open(screenshot_path).convert('RGB')\n",
    "    mask = Image.open(mask_path).convert('L')\n",
    "    \n",
    "    # Load keyword\n",
    "    keywords_df = pd.read_csv(KEYWORDS_FILE)\n",
    "    keywords_df.columns = keywords_df.columns.str.strip()\n",
    "    if '_id' in keywords_df.columns:\n",
    "        video_id_col = '_id'\n",
    "    else:\n",
    "        video_id_col = 'video_id'\n",
    "    if 'keyword_list[0]' in keywords_df.columns:\n",
    "        keyword_col = 'keyword_list[0]'\n",
    "    else:\n",
    "        keyword_col = 'keyword'\n",
    "    keyword_row = keywords_df[keywords_df[video_id_col].astype(str) == str(example_video)]\n",
    "    keyword = keyword_row.iloc[0][keyword_col] if len(keyword_row) > 0 else \"unknown\"\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(screenshot)\n",
    "    axes[0].set_title(f\"Screenshot\\nVideo: {example_video}\\nScene: {example_scene_num}\\nKeyword: '{keyword}'\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title(\"Generated Keyword Mask\\n(White = Product Region)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    screenshot_array = np.array(screenshot)\n",
    "    mask_array = np.array(mask)\n",
    "    overlay = screenshot_array.copy()\n",
    "    overlay[mask_array > 128] = (overlay[mask_array > 128] * 0.5 + np.array([255, 0, 0]) * 0.5).astype(np.uint8)\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(\"Overlay\\n(Red = Detected Product)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Could not find screenshot or mask for video {example_video}, scene {example_scene_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dataset Preparation <a name=\"data-prep\"></a>\n",
    "\n",
    "### Split into Train/Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split videos into train/val (80/20)\n",
    "train_videos, val_videos = split_train_val_videos(\n",
    "    video_ids=video_ids_for_training,\n",
    "    val_ratio=0.2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain videos ({len(train_videos)}): {train_videos}\")\n",
    "print(f\"Val videos ({len(val_videos)}): {val_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed statistics\n",
    "print_dataset_statistics(\n",
    "    alignment_score_file=ALIGNMENT_SCORE_FILE,\n",
    "    train_videos=train_videos,\n",
    "    val_videos=val_videos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'alignment_score_file': ALIGNMENT_SCORE_FILE,\n",
    "        'keywords_file': KEYWORDS_FILE,\n",
    "        'screenshots_dir': SCREENSHOTS_DIR,\n",
    "        'keyword_masks_dir': KEYWORD_MASKS_DIR,\n",
    "        'image_size': 512,\n",
    "    },\n",
    "    'model': {\n",
    "        'sd_model_name': 'runwayml/stable-diffusion-v1-5',\n",
    "        'controlnet': {\n",
    "            'control_channels': 2,  # [M_t, S_t]\n",
    "            'base_channels': 64,\n",
    "        },\n",
    "        'use_lora': False,\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 4,\n",
    "        'num_workers': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'num_epochs': 5 if USE_SUBSET else 10,  # Fewer epochs for subset\n",
    "        'lambda_recon': 1.0,\n",
    "        'lambda_lpips': 1.0,\n",
    "        'lambda_bg': 0.5,\n",
    "        'use_recon_loss': True,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'mixed_precision': True,\n",
    "        'log_wandb': False,\n",
    "        'project_name': 'video-ad-manipulation',\n",
    "        'output_dir': 'outputs/training_subset' if USE_SUBSET else 'outputs/training_full',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['training']['output_dir'], exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "config_save_path = os.path.join(CONFIG['training']['output_dir'], 'config.yaml')\n",
    "with open(config_save_path, 'w') as f:\n",
    "    yaml.dump(CONFIG, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Mode: {'SUBSET' if USE_SUBSET else 'FULL'}\")\n",
    "print(f\"  Videos: {len(train_videos)} train, {len(val_videos)} val\")\n",
    "print(f\"  Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"  Output: {CONFIG['training']['output_dir']}\")\n",
    "print(f\"\\nConfig saved to: {config_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "print(\"Creating data loaders...\\n\")\n",
    "\n",
    "data_module = VideoSceneDataModule(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    train_videos=train_videos,\n",
    "    val_videos=val_videos,\n",
    "    screenshots_dir=CONFIG['data']['screenshots_dir'],\n",
    "    keyword_masks_dir=CONFIG['data']['keyword_masks_dir'],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    num_workers=CONFIG['training']['num_workers'],\n",
    "    image_size=(CONFIG['data']['image_size'], CONFIG['data']['image_size']),\n",
    ")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "print(f\"\\n‚úì Data loaders created:\")\n",
    "print(f\"  Training scenes: {len(data_module.train_dataset)}\")\n",
    "print(f\"  Validation scenes: {len(data_module.val_dataset)}\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Training Batch Contents:\")\n",
    "print(f\"  'image' shape: {batch['image'].shape}\")  # [B, 3, 512, 512]\n",
    "print(f\"  'control' shape: {batch['control'].shape}\")  # [B, 2, 512, 512]\n",
    "print(f\"  'keyword_mask' shape: {batch['keyword_mask'].shape}\")  # [B, 1, 512, 512]\n",
    "print(f\"  'alignment_score': {batch['alignment_score'][:3].tolist()}...\")  # Scalars\n",
    "print(f\"  'keyword' (text prompts): {batch['keyword'][:2]}...\")\n",
    "print(f\"  'video_id': {batch['video_id'][:2]}...\")\n",
    "print(f\"  'scene_number': {batch['scene_number'][:3].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first sample in batch\n",
    "sample_idx = 0\n",
    "image = batch['image'][sample_idx].permute(1, 2, 0).numpy()\n",
    "image = (image * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "keyword_mask = batch['control'][sample_idx, 0].numpy()  # M_t\n",
    "alignment_map = batch['control'][sample_idx, 1].numpy()  # S_t\n",
    "alignment_score = batch['alignment_score'][sample_idx].item()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f\"Scene Image\\nVideo: {batch['video_id'][sample_idx]}\\nScene: {batch['scene_number'][sample_idx]}\\nKeyword: '{batch['keyword'][sample_idx]}'\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(keyword_mask, cmap='gray')\n",
    "axes[1].set_title(\"Control Channel 0 (M_t)\\nKeyword Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(alignment_map, cmap='hot')\n",
    "axes[2].set_title(f\"Control Channel 1 (S_t)\\nAlignment Map\\nScore: {alignment_score:.4f}\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Fine-Tuning <a name=\"training\"></a>\n",
    "\n",
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Stable Diffusion + ControlNet model...\")\n",
    "print(\"This may take a few minutes on first run (downloading pretrained weights)\\n\")\n",
    "\n",
    "model = StableDiffusionControlNetWrapper(\n",
    "    sd_model_name=CONFIG['model']['sd_model_name'],\n",
    "    controlnet_config=CONFIG['model']['controlnet'],\n",
    "    device=device,\n",
    "    use_lora=CONFIG['model']['use_lora'],\n",
    ")\n",
    "\n",
    "print(\"‚úì Model initialized successfully\\n\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  SD backbone: {CONFIG['model']['sd_model_name']}\")\n",
    "print(f\"  ControlNet input channels: {CONFIG['model']['controlnet']['control_channels']}\")\n",
    "print(f\"  Using LoRA: {CONFIG['model']['use_lora']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "**Note:** Uncomment `trainer.train()` to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ControlNetTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    learning_rate=CONFIG['training']['learning_rate'],\n",
    "    num_epochs=CONFIG['training']['num_epochs'],\n",
    "    device=device,\n",
    "    output_dir=CONFIG['training']['output_dir'],\n",
    "    lambda_recon=CONFIG['training']['lambda_recon'],\n",
    "    lambda_lpips=CONFIG['training']['lambda_lpips'],\n",
    "    lambda_bg=CONFIG['training']['lambda_bg'],\n",
    "    use_recon_loss=CONFIG['training']['use_recon_loss'],\n",
    "    gradient_accumulation_steps=CONFIG['training']['gradient_accumulation_steps'],\n",
    "    mixed_precision=CONFIG['training']['mixed_precision'],\n",
    "    log_wandb=CONFIG['training']['log_wandb'],\n",
    "    project_name=CONFIG['training']['project_name'],\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'SUBSET (' + str(NUM_VIDEOS) + ' videos)' if USE_SUBSET else 'FULL (' + str(len(valid_video_ids)) + ' videos)'}\")\n",
    "print(f\"Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"Training scenes: {len(data_module.train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Output directory: {CONFIG['training']['output_dir']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train (UNCOMMENT TO START TRAINING)\n",
    "# trainer.train()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Training not started (trainer.train() is commented out)\")\n",
    "print(\"   Uncomment the line above to start training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Inference: Generate 7 Experimental Variants <a name=\"inference\"></a>\n",
    "\n",
    "### Variant Definitions\n",
    "\n",
    "We generate **7 variants** for each video:\n",
    "\n",
    "1. **baseline**: Original alignment scores (control condition)\n",
    "2. **early_boost**: Boost alignment in first 33% of scenes (√ó1.5)\n",
    "3. **middle_boost**: Boost alignment in middle 33% of scenes (√ó1.5)\n",
    "4. **late_boost**: Boost alignment in last 33% of scenes (√ó1.5)\n",
    "5. **full_boost**: Boost alignment in all scenes (√ó1.5)\n",
    "6. **reduction**: Reduce alignment in middle 33% of scenes (√ó0.5)\n",
    "7. **placebo**: Modify non-keyword regions only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variant Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variant generator\n",
    "variant_generator = VideoVariantGenerator(\n",
    "    alignment_score_file=CONFIG['data']['alignment_score_file'],\n",
    "    keywords_file=CONFIG['data']['keywords_file'],\n",
    "    boost_alpha=1.5,\n",
    "    reduction_alpha=0.5,\n",
    ")\n",
    "\n",
    "print(\"‚úì Variant generator initialized\")\n",
    "print(f\"  Boost alpha: 1.5\")\n",
    "print(f\"  Reduction alpha: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Generate Variants for Single Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for a single video (example)\n",
    "example_video_id = valid_video_ids[0]\n",
    "print(f\"Generating variants for video: {example_video_id}\\n\")\n",
    "\n",
    "variants = variant_generator.create_all_variants_for_video(example_video_id)\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(variants)} variants:\")\n",
    "for variant_name in variants.keys():\n",
    "    print(f\"  - {variant_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display statistics\n",
    "stats = variant_generator.compute_variant_statistics(variants)\n",
    "\n",
    "print(\"\\nVariant Statistics:\\n\")\n",
    "print(f\"{'Variant':<15} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10} {'Scenes'}\")\n",
    "print(\"-\" * 65)\n",
    "for variant_name, stat in stats.items():\n",
    "    print(f\"{variant_name:<15} {stat['mean_alignment']:<10.4f} {stat['std_alignment']:<10.4f} {stat['min_alignment']:<10.4f} {stat['max_alignment']:<10.4f} {stat['num_scenes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualization <a name=\"visualization\"></a>\n",
    "\n",
    "### Visualize Variant Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyword for this video\n",
    "keyword = variant_generator.keywords.get(str(example_video_id), \"unknown\")\n",
    "\n",
    "# Visualize alignment profiles\n",
    "visualize_variant_comparison(variants, example_video_id, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variants for All Videos\n",
    "\n",
    "This generates variant specifications (CSV files) for all valid videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for all videos\n",
    "output_dir = 'outputs/variants_subset' if USE_SUBSET else 'outputs/variants_full'\n",
    "\n",
    "print(f\"Generating variants for all videos...\")\n",
    "print(f\"Output directory: {output_dir}\\n\")\n",
    "\n",
    "all_variants = variant_generator.generate_variants_for_all_videos(\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Save manifest\n",
    "manifest_path = os.path.join(output_dir, 'manifest.json')\n",
    "variant_generator.save_variant_manifest(\n",
    "    all_variants,\n",
    "    output_path=manifest_path\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Variant generation complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Generated variants for {len(all_variants)} videos\")\n",
    "print(f\"  Output directory: {output_dir}/\")\n",
    "print(f\"  Manifest: {manifest_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Variant Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display manifest\n",
    "with open(manifest_path, 'r') as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(\"Variant Generation Manifest:\")\n",
    "print(f\"  Total videos: {manifest['num_videos']}\")\n",
    "print(f\"  Variants per video: {manifest['num_variants_per_video']}\")\n",
    "print(f\"  Boost alpha: {manifest['boost_alpha']}\")\n",
    "print(f\"  Reduction alpha: {manifest['reduction_alpha']}\")\n",
    "print(f\"\\n  Variant types:\")\n",
    "for vtype in manifest['variant_types']:\n",
    "    print(f\"    - {vtype}\")\n",
    "\n",
    "print(f\"\\nFirst 5 videos:\")\n",
    "for video_id, info in list(manifest['videos'].items())[:5]:\n",
    "    print(f\"  {video_id}: {info['num_scenes']} scenes, keyword='{info['keyword']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "\n",
    "1. **Validated video IDs** using alignment_score.csv as source of truth\n",
    "2. **Generated keyword masks** using CLIPSeg from screenshots\n",
    "3. **Created train/val split** at video level (not scene level)\n",
    "4. **Configured quick training mode** to use subset of videos\n",
    "5. **Prepared data loaders** with proper filtering\n",
    "6. **Generated 7 experimental variants** for all videos\n",
    "\n",
    "### Output Files\n",
    "\n",
    "```\n",
    "data/\n",
    "‚îî‚îÄ‚îÄ keyword_masks/           # Generated keyword masks\n",
    "    ‚îî‚îÄ‚îÄ {video_id}/\n",
    "        ‚îî‚îÄ‚îÄ scene_{N}.png\n",
    "\n",
    "outputs/\n",
    "‚îú‚îÄ‚îÄ training_subset/         # Training outputs (subset mode)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ best_model.pt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ training_history.json\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ variants_subset/         # Variant specifications (subset mode)\n",
    "    ‚îú‚îÄ‚îÄ manifest.json\n",
    "    ‚îî‚îÄ‚îÄ {video_id}/\n",
    "        ‚îú‚îÄ‚îÄ baseline.csv\n",
    "        ‚îú‚îÄ‚îÄ early_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ middle_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ late_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ full_boost.csv\n",
    "        ‚îú‚îÄ‚îÄ reduction.csv\n",
    "        ‚îú‚îÄ‚îÄ placebo.csv\n",
    "        ‚îî‚îÄ‚îÄ statistics.json\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Train model**: Uncomment `trainer.train()` to start training\n",
    "2. **Run inference**: Use trained model to generate edited scenes\n",
    "3. **Reassemble videos**: Combine edited scenes into video files\n",
    "4. **Deploy for A/B testing**: Upload variants for experimental study\n",
    "\n",
    "### Quick vs Full Training\n",
    "\n",
    "- **Quick mode** (`USE_SUBSET=True`, `NUM_VIDEOS=10`):\n",
    "  - Fast experimentation\n",
    "  - Test the pipeline\n",
    "  - 5 epochs\n",
    "  - ~10-20 minutes on GPU\n",
    "\n",
    "- **Full mode** (`USE_SUBSET=False`):\n",
    "  - Production training\n",
    "  - All valid videos\n",
    "  - 10 epochs\n",
    "  - Several hours on GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

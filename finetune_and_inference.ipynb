{"cells":[{"cell_type":"markdown","metadata":{"id":"nbyxDMkSqoq4"},"source":["# ControlNet Fine-Tuning and Inference for Video Ad Manipulation\n","\n","This notebook demonstrates:\n","1. **Data format requirements** - Exact structure needed for training\n","2. **Fine-tuning process** - Training the ControlNet adapter\n","3. **Inference** - Generating manipulated video variants\n","\n","---\n","\n","## Table of Contents\n","1. [Setup](#setup)\n","2. [Data Format Specification](#data-format)\n","3. [Data Preparation](#data-prep)\n","4. [Model Fine-Tuning](#training)\n","5. [Inference](#inference)\n","6. [Visualization](#visualization)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsnEPBrYqtOC","executionInfo":{"status":"ok","timestamp":1763251357615,"user_tz":360,"elapsed":16726,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}},"outputId":"b390b18c-6cd1-46fb-8d25-b836a29e83e8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/meaning_alignment_tiktok/"],"metadata":{"id":"kiwNm4vnq1t6","executionInfo":{"status":"ok","timestamp":1763251366528,"user_tz":360,"elapsed":192,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}},"outputId":"b04e780e-0f8c-449b-8e91-21dbd89b15b9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/meaning_alignment_tiktok\n"]}]},{"cell_type":"markdown","metadata":{"id":"1VdelmqOqoq5"},"source":["---\n","## 1. Setup <a name=\"setup\"></a>"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"F9hgZmRcqoq5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763251471918,"user_tz":360,"elapsed":103175,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}},"outputId":"5f412399-58e8-4952-ab09-0e7fae76cb95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["import os\n","import sys\n","import json\n","import yaml\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.auto import tqdm\n","from torch.utils.data import DataLoader\n","\n","# Add src to path\n","sys.path.insert(0, str(Path.cwd()))\n","\n","# Import framework modules\n","from src.models import StableDiffusionControlNetWrapper\n","from src.training import ControlNetTrainer, VideoAdDataModule\n","from src.data_preparation import ControlTensorBuilder\n","from src.video_editing import VideoEditor\n","\n","# Check GPU\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","if device == \"cuda\":\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"]},{"cell_type":"markdown","metadata":{"id":"JciitlL4qoq6"},"source":["---\n","## 2. Data Format Specification <a name=\"data-format\"></a>\n","\n","### Required Directory Structure\n","\n","```\n","data/\n","├── frames/                          # Original video frames (RGB)\n","│   ├── video_001/\n","│   │   ├── frame_00000.png         # PNG format, any resolution\n","│   │   ├── frame_00001.png\n","│   │   └── ...\n","│   ├── video_002/\n","│   └── ...\n","│\n","├── attention_heatmaps/              # Attention heatmaps from LLaVA\n","│   ├── video_001/\n","│   │   ├── frame_00000.png         # Grayscale, [0-255], same size as frames\n","│   │   ├── frame_00001.png\n","│   │   └── ...\n","│   ├── video_002/\n","│   └── ...\n","│\n","├── keyword_heatmaps/                # Keyword heatmaps from CLIPSeg\n","│   ├── video_001/\n","│   │   ├── frame_00000.png         # Grayscale, [0-255], same size as frames\n","│   │   ├── frame_00001.png\n","│   │   └── ...\n","│   ├── video_002/\n","│   └── ...\n","│\n","└── keywords.json                    # Mapping video_id -> keyword text\n","```\n","\n","### File Format Details\n","\n","#### 1. **frames/** (Original RGB frames)\n","- **Format**: PNG, JPEG, or any PIL-readable format\n","- **Resolution**: Any (will be resized to 512×512 during training)\n","- **Channels**: 3 (RGB)\n","- **Naming**: `frame_{index:05d}.png` (e.g., frame_00000.png, frame_00001.png)\n","- **Values**: [0, 255] uint8\n","\n","#### 2. **attention_heatmaps/** (Attention from LLaVA)\n","- **Format**: PNG (grayscale)\n","- **Resolution**: MUST match corresponding frame\n","- **Channels**: 1 (grayscale)\n","- **Naming**: Same as frames (frame_00000.png, etc.)\n","- **Values**: [0, 255] uint8, where:\n","  - 0 = minimum attention (black)\n","  - 255 = maximum attention (white)\n","- **Semantics**: Higher values = higher viewer attention/importance\n","\n","#### 3. **keyword_heatmaps/** (Keywords from CLIPSeg)\n","- **Format**: PNG (grayscale)\n","- **Resolution**: MUST match corresponding frame\n","- **Channels**: 1 (grayscale)\n","- **Naming**: Same as frames (frame_00000.png, etc.)\n","- **Values**: [0, 255] uint8, where:\n","  - 0 = not keyword region (black)\n","  - 255 = keyword region (white)\n","- **Semantics**: Probability/confidence that pixel contains the product/keyword\n","\n","#### 4. **keywords.json**\n","```json\n","{\n","  \"video_001\": \"jewelry\",\n","  \"video_002\": \"running shoes\",\n","  \"video_003\": \"lipstick\",\n","  \"video_004\": \"smartphone\"\n","}\n","```\n","- **Format**: JSON object\n","- **Keys**: Video ID (must match folder names)\n","- **Values**: Keyword text (product name/description)"]},{"cell_type":"code","source":["import pandas as pd\n","\n","alignment_score = pd.read_csv('data/alignment_score.csv')\n","alignment_score.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"JkgRi_qrDs3C","executionInfo":{"status":"ok","timestamp":1763252834750,"user_tz":360,"elapsed":1575,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}},"outputId":"76a00204-eebc-4376-ac38-d2087f892bf6"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              video id  Scene Number  attention_proportion  start_time  \\\n","0  7163329870906884097             1              0.057578       0.000   \n","1  7163329870906884097             2              0.085758       0.633   \n","2  7163329870906884097             3              0.022061       2.067   \n","3  7163329870906884097             4              0.038521       2.700   \n","4  7163329870906884097             5              0.048467       4.167   \n","\n","   end_time  CTR_mean  CVR_mean  Clicks_mean  Conversion_mean  Remain_mean  \\\n","0     0.633  0.029925  0.020833     0.215042         0.076923     1.000000   \n","1     2.067  0.201291  0.236111     0.585539         0.239316     0.589117   \n","2     2.700  0.307857  0.395833     0.541576         0.256410     0.244668   \n","3     4.167  0.215623  0.333333     0.292796         0.153846     0.172204   \n","4     4.867  0.149898  0.416667     0.126129         0.128205     0.117016   \n","\n","   contrast  brightness            industry  \n","0  0.278934  153.610451  Children's Apparel  \n","1  0.256491  161.464963  Children's Apparel  \n","2  0.187419  177.340560  Children's Apparel  \n","3  0.278855  155.625972  Children's Apparel  \n","4  0.328010  151.592408  Children's Apparel  "],"text/html":["\n","  <div id=\"df-d710f936-8913-49f1-be64-40d55aad2391\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video id</th>\n","      <th>Scene Number</th>\n","      <th>attention_proportion</th>\n","      <th>start_time</th>\n","      <th>end_time</th>\n","      <th>CTR_mean</th>\n","      <th>CVR_mean</th>\n","      <th>Clicks_mean</th>\n","      <th>Conversion_mean</th>\n","      <th>Remain_mean</th>\n","      <th>contrast</th>\n","      <th>brightness</th>\n","      <th>industry</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7163329870906884097</td>\n","      <td>1</td>\n","      <td>0.057578</td>\n","      <td>0.000</td>\n","      <td>0.633</td>\n","      <td>0.029925</td>\n","      <td>0.020833</td>\n","      <td>0.215042</td>\n","      <td>0.076923</td>\n","      <td>1.000000</td>\n","      <td>0.278934</td>\n","      <td>153.610451</td>\n","      <td>Children's Apparel</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7163329870906884097</td>\n","      <td>2</td>\n","      <td>0.085758</td>\n","      <td>0.633</td>\n","      <td>2.067</td>\n","      <td>0.201291</td>\n","      <td>0.236111</td>\n","      <td>0.585539</td>\n","      <td>0.239316</td>\n","      <td>0.589117</td>\n","      <td>0.256491</td>\n","      <td>161.464963</td>\n","      <td>Children's Apparel</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7163329870906884097</td>\n","      <td>3</td>\n","      <td>0.022061</td>\n","      <td>2.067</td>\n","      <td>2.700</td>\n","      <td>0.307857</td>\n","      <td>0.395833</td>\n","      <td>0.541576</td>\n","      <td>0.256410</td>\n","      <td>0.244668</td>\n","      <td>0.187419</td>\n","      <td>177.340560</td>\n","      <td>Children's Apparel</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7163329870906884097</td>\n","      <td>4</td>\n","      <td>0.038521</td>\n","      <td>2.700</td>\n","      <td>4.167</td>\n","      <td>0.215623</td>\n","      <td>0.333333</td>\n","      <td>0.292796</td>\n","      <td>0.153846</td>\n","      <td>0.172204</td>\n","      <td>0.278855</td>\n","      <td>155.625972</td>\n","      <td>Children's Apparel</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7163329870906884097</td>\n","      <td>5</td>\n","      <td>0.048467</td>\n","      <td>4.167</td>\n","      <td>4.867</td>\n","      <td>0.149898</td>\n","      <td>0.416667</td>\n","      <td>0.126129</td>\n","      <td>0.128205</td>\n","      <td>0.117016</td>\n","      <td>0.328010</td>\n","      <td>151.592408</td>\n","      <td>Children's Apparel</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d710f936-8913-49f1-be64-40d55aad2391')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d710f936-8913-49f1-be64-40d55aad2391 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d710f936-8913-49f1-be64-40d55aad2391');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d60d6798-7bbb-4960-85dc-2b3f6b06bd34\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d60d6798-7bbb-4960-85dc-2b3f6b06bd34')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d60d6798-7bbb-4960-85dc-2b3f6b06bd34 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"alignment_score"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import pandas as pd\n","\n","video_metadata = pd.read_csv('data/video_metadata.csv')\n","video_metadata.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yd0KTiFRGTD4","executionInfo":{"status":"ok","timestamp":1763253671057,"user_tz":360,"elapsed":26139,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}},"outputId":"44a1d3ae-7471-4f2c-8f75-56f28ae7f265"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3673341612.py:3: DtypeWarning: Columns (21,22,23,24,25,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n","  video_metadata = pd.read_csv('data/video_metadata.csv')\n"]},{"output_type":"execute_result","data":{"text/plain":["Index(['_id', 'time', 'industry', 'video_url', 'recommend_video[0]',\n","       'recommend_video[1]', 'recommend_video[2]', 'recommend_video[3]',\n","       'recommend_video[4]', 'metric.comment',\n","       ...\n","       'Remain_keyframe[116].value', 'Remain_keyframe[117].value',\n","       'Remain_keyframe[118].value', 'Remain_keyframe[119].value',\n","       'Remain_keyframe[120].value', 'Remain_keyframe[121].value',\n","       'Remain_keyframe[122].value', 'Remain_keyframe[123].value',\n","       'Remain_percentile', 'keyword_list'],\n","      dtype='object', length=1340)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["keyword_data = video_metadata[['_id', 'keyword_list[0]']]\n","keyword_data.to_csv('data/keywords.csv', index=False)"],"metadata":{"id":"0ugCAsjzHAAg","executionInfo":{"status":"ok","timestamp":1763253786412,"user_tz":360,"elapsed":274,"user":{"displayName":"zhan shi","userId":"11554369190339857099"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6O70aRhqoq6"},"source":["### Verify Data Structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7dyOisBqoq6"},"outputs":[],"source":["# Define paths\n","DATA_ROOT = \"data\"\n","FRAMES_DIR = os.path.join(DATA_ROOT, \"frames\")\n","ATTENTION_DIR = os.path.join(DATA_ROOT, \"attention_heatmaps\")\n","KEYWORD_DIR = os.path.join(DATA_ROOT, \"keyword_heatmaps\")\n","KEYWORDS_FILE = os.path.join(DATA_ROOT, \"keywords.json\")\n","\n","def verify_data_structure():\n","    \"\"\"\n","    Verify that all required data files exist and are properly formatted.\n","    \"\"\"\n","    print(\"Verifying data structure...\\n\")\n","\n","    # Check directories exist\n","    for dir_path, name in [(FRAMES_DIR, \"frames\"),\n","                            (ATTENTION_DIR, \"attention_heatmaps\"),\n","                            (KEYWORD_DIR, \"keyword_heatmaps\")]:\n","        if os.path.exists(dir_path):\n","            print(f\"✓ {name}/ directory found\")\n","        else:\n","            print(f\"✗ {name}/ directory NOT FOUND\")\n","            return False\n","\n","    # Check keywords file\n","    if os.path.exists(KEYWORDS_FILE):\n","        print(f\"✓ keywords.json found\")\n","        with open(KEYWORDS_FILE, 'r') as f:\n","            keywords = json.load(f)\n","        print(f\"  Found {len(keywords)} videos with keywords\")\n","    else:\n","        print(f\"✗ keywords.json NOT FOUND\")\n","        return False\n","\n","    # Check each video has all required files\n","    print(\"\\nChecking video files...\")\n","    for video_id in keywords.keys():\n","        frame_dir = os.path.join(FRAMES_DIR, video_id)\n","        attn_dir = os.path.join(ATTENTION_DIR, video_id)\n","        kw_dir = os.path.join(KEYWORD_DIR, video_id)\n","\n","        if not all([os.path.exists(frame_dir), os.path.exists(attn_dir), os.path.exists(kw_dir)]):\n","            print(f\"✗ {video_id}: Missing directories\")\n","            continue\n","\n","        # Count frames\n","        frames = sorted([f for f in os.listdir(frame_dir) if f.endswith('.png')])\n","        attn_maps = sorted([f for f in os.listdir(attn_dir) if f.endswith('.png')])\n","        kw_maps = sorted([f for f in os.listdir(kw_dir) if f.endswith('.png')])\n","\n","        if len(frames) == len(attn_maps) == len(kw_maps):\n","            print(f\"✓ {video_id}: {len(frames)} frames, keyword='{keywords[video_id]}'\")\n","        else:\n","            print(f\"✗ {video_id}: Mismatched counts - frames:{len(frames)}, attn:{len(attn_maps)}, kw:{len(kw_maps)}\")\n","\n","    return True\n","\n","# Run verification\n","verify_data_structure()"]},{"cell_type":"markdown","metadata":{"id":"L7tLq0WRqoq6"},"source":["### Visualize Example Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiSVhEZVqoq6"},"outputs":[],"source":["def visualize_example_data(video_id, frame_idx=0):\n","    \"\"\"\n","    Visualize a single frame with its attention and keyword heatmaps.\n","\n","    Args:\n","        video_id: Video ID to visualize\n","        frame_idx: Frame index (default: 0)\n","    \"\"\"\n","    # Load data\n","    frame_path = os.path.join(FRAMES_DIR, video_id, f\"frame_{frame_idx:05d}.png\")\n","    attn_path = os.path.join(ATTENTION_DIR, video_id, f\"frame_{frame_idx:05d}.png\")\n","    kw_path = os.path.join(KEYWORD_DIR, video_id, f\"frame_{frame_idx:05d}.png\")\n","\n","    frame = np.array(Image.open(frame_path).convert('RGB'))\n","    attn_map = np.array(Image.open(attn_path).convert('L'))\n","    kw_map = np.array(Image.open(kw_path).convert('L'))\n","\n","    # Load keyword\n","    with open(KEYWORDS_FILE, 'r') as f:\n","        keywords = json.load(f)\n","    keyword = keywords[video_id]\n","\n","    # Compute derived maps\n","    attn_norm = attn_map.astype(float) / 255.0\n","    kw_norm = kw_map.astype(float) / 255.0\n","\n","    # M_t: Keyword mask (binarized at threshold 0.5)\n","    keyword_mask = (kw_norm > 0.5).astype(float)\n","\n","    # S_t: Alignment map (attention × keyword)\n","    alignment_map = attn_norm * kw_norm\n","    if alignment_map.max() > 0:\n","        alignment_map = alignment_map / alignment_map.max()  # Normalize\n","\n","    # Plot\n","    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n","\n","    # Row 1: Input data\n","    axes[0, 0].imshow(frame)\n","    axes[0, 0].set_title(f\"Original Frame\\n{video_id} - frame {frame_idx}\", fontsize=12, fontweight='bold')\n","    axes[0, 0].axis('off')\n","\n","    axes[0, 1].imshow(attn_map, cmap='hot')\n","    axes[0, 1].set_title(f\"Attention Heatmap (A_t)\\nFrom LLaVA\", fontsize=12, fontweight='bold')\n","    axes[0, 1].axis('off')\n","\n","    axes[0, 2].imshow(kw_map, cmap='hot')\n","    axes[0, 2].set_title(f\"Keyword Heatmap (K_t)\\nFrom CLIPSeg\\nKeyword: '{keyword}'\", fontsize=12, fontweight='bold')\n","    axes[0, 2].axis('off')\n","\n","    # Row 2: Derived maps (control tensor components)\n","    axes[1, 0].imshow(keyword_mask, cmap='gray', vmin=0, vmax=1)\n","    axes[1, 0].set_title(\"Keyword Mask (M_t)\\nControl Tensor Channel 0\", fontsize=12, fontweight='bold', color='blue')\n","    axes[1, 0].axis('off')\n","\n","    axes[1, 1].imshow(alignment_map, cmap='hot', vmin=0, vmax=1)\n","    axes[1, 1].set_title(\"Alignment Map (S_t)\\nControl Tensor Channel 1\\nS_t = A_t ⊙ K_t\", fontsize=12, fontweight='bold', color='blue')\n","    axes[1, 1].axis('off')\n","\n","    # Show overlay\n","    overlay = frame.copy()\n","    overlay_mask = np.stack([np.zeros_like(attn_map), np.zeros_like(attn_map), attn_map], axis=-1)\n","    overlay = (0.6 * overlay + 0.4 * overlay_mask).astype(np.uint8)\n","    axes[1, 2].imshow(overlay)\n","    axes[1, 2].set_title(\"Attention Overlay\\n(Visualization only)\", fontsize=12, fontweight='bold')\n","    axes[1, 2].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print stats\n","    print(f\"\\nData Statistics for {video_id}, frame {frame_idx}:\")\n","    print(f\"  Frame shape: {frame.shape}\")\n","    print(f\"  Attention range: [{attn_map.min()}, {attn_map.max()}]\")\n","    print(f\"  Keyword range: [{kw_map.min()}, {kw_map.max()}]\")\n","    print(f\"  Keyword mask coverage: {keyword_mask.mean()*100:.2f}% of frame\")\n","    print(f\"  Alignment score: {alignment_map.mean():.4f}\")\n","\n","# Example: Visualize first frame of first video\n","with open(KEYWORDS_FILE, 'r') as f:\n","    keywords = json.load(f)\n","first_video = list(keywords.keys())[0]\n","\n","visualize_example_data(first_video, frame_idx=0)"]},{"cell_type":"markdown","metadata":{"id":"gSKcB8-Vqoq7"},"source":["---\n","## 3. Data Preparation <a name=\"data-prep\"></a>\n","\n","### Build Control Tensors\n","\n","Control tensors are constructed from attention and keyword heatmaps:\n","\n","**C_t = [M_t, S_t]** where:\n","- **M_t**: Keyword mask (binarized keyword heatmap)\n","- **S_t**: Alignment map (attention × keyword, normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AV5gQmh9qoq7"},"outputs":[],"source":["# Configuration\n","CONFIG = {\n","    'data': {\n","        'data_root': DATA_ROOT,\n","        'keywords_file': KEYWORDS_FILE,\n","        'image_size': 512,  # Resize all inputs to 512×512\n","        'include_raw_maps': False,  # Use 2-channel [M_t, S_t] instead of 4-channel\n","        'keyword_threshold': 0.5,  # Threshold for binarizing keyword heatmap\n","    },\n","    'model': {\n","        'sd_model_name': 'runwayml/stable-diffusion-v1-5',\n","        'controlnet': {\n","            'control_channels': 2,  # [M_t, S_t]\n","            'base_channels': 64,\n","        },\n","        'use_lora': False,  # Set to True for LoRA fine-tuning\n","    },\n","    'training': {\n","        'batch_size': 4,\n","        'num_workers': 4,\n","        'learning_rate': 1e-4,\n","        'num_epochs': 10,\n","        'lambda_recon': 1.0,      # Reconstruction loss weight\n","        'lambda_lpips': 1.0,      # Perceptual loss weight\n","        'lambda_bg': 0.5,         # Background preservation weight\n","        'use_recon_loss': True,\n","        'gradient_accumulation_steps': 1,\n","        'mixed_precision': True,\n","        'log_wandb': False,\n","        'project_name': 'video-ad-manipulation',\n","        'output_dir': 'outputs/training',\n","    },\n","}\n","\n","# Create output directory\n","os.makedirs(CONFIG['training']['output_dir'], exist_ok=True)\n","\n","# Save config\n","config_save_path = os.path.join(CONFIG['training']['output_dir'], 'config.yaml')\n","with open(config_save_path, 'w') as f:\n","    yaml.dump(CONFIG, f, default_flow_style=False)\n","print(f\"Configuration saved to: {config_save_path}\")"]},{"cell_type":"markdown","metadata":{"id":"pyBusUeBqoq7"},"source":["### Setup Data Loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GJBjDaQqoq7"},"outputs":[],"source":["# Define train/validation split\n","with open(KEYWORDS_FILE, 'r') as f:\n","    all_videos = list(json.load(f).keys())\n","\n","# Split: 80% train, 20% validation\n","split_idx = int(0.8 * len(all_videos))\n","train_videos = all_videos[:split_idx]\n","val_videos = all_videos[split_idx:]\n","\n","print(f\"Training videos ({len(train_videos)}): {train_videos}\")\n","print(f\"Validation videos ({len(val_videos)}): {val_videos}\")\n","\n","# Create data module\n","data_module = VideoAdDataModule(\n","    data_root=CONFIG['data']['data_root'],\n","    keywords_file=CONFIG['data']['keywords_file'],\n","    train_videos=train_videos,\n","    val_videos=val_videos,\n","    batch_size=CONFIG['training']['batch_size'],\n","    num_workers=CONFIG['training']['num_workers'],\n","    image_size=CONFIG['data']['image_size'],\n","    include_raw_maps=CONFIG['data']['include_raw_maps'],\n",")\n","\n","train_loader = data_module.train_dataloader()\n","val_loader = data_module.val_dataloader()\n","\n","print(f\"\\nDataset Statistics:\")\n","print(f\"  Training samples: {len(data_module.train_dataset)}\")\n","print(f\"  Validation samples: {len(data_module.val_dataset)}\")\n","print(f\"  Batches per epoch: {len(train_loader)}\")"]},{"cell_type":"markdown","metadata":{"id":"GD5K04NQqoq7"},"source":["### Inspect a Training Batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlJYY-C-qoq7"},"outputs":[],"source":["# Get one batch\n","batch = next(iter(train_loader))\n","\n","print(\"Training Batch Contents:\")\n","print(f\"  'image' shape: {batch['image'].shape}\")  # [B, 3, 512, 512]\n","print(f\"  'control' shape: {batch['control'].shape}\")  # [B, 2, 512, 512]\n","print(f\"  'keyword' (text prompts): {batch['keyword'][:2]}...\")  # List of strings\n","print(f\"  'keyword_mask' shape: {batch['keyword_mask'].shape}\")  # [B, 1, 512, 512]\n","\n","# Visualize first sample in batch\n","sample_idx = 0\n","image = batch['image'][sample_idx].permute(1, 2, 0).numpy()  # [512, 512, 3]\n","image = (image * 0.5 + 0.5).clip(0, 1)  # Denormalize from [-1, 1] to [0, 1]\n","\n","keyword_mask = batch['control'][sample_idx, 0].numpy()  # M_t: [512, 512]\n","alignment_map = batch['control'][sample_idx, 1].numpy()  # S_t: [512, 512]\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","axes[0].imshow(image)\n","axes[0].set_title(f\"Training Image\\nKeyword: '{batch['keyword'][sample_idx]}'\")\n","axes[0].axis('off')\n","\n","axes[1].imshow(keyword_mask, cmap='gray')\n","axes[1].set_title(\"Control Channel 0 (M_t)\\nKeyword Mask\")\n","axes[1].axis('off')\n","\n","axes[2].imshow(alignment_map, cmap='hot')\n","axes[2].set_title(\"Control Channel 1 (S_t)\\nAlignment Map\")\n","axes[2].axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"woNgDb8pqoq7"},"source":["---\n","## 4. Model Fine-Tuning <a name=\"training\"></a>\n","\n","### Initialize Model\n","\n","The model consists of:\n","- **Frozen**: Stable Diffusion (U-Net, VAE, text encoder)\n","- **Trainable**: ControlNet adapter (~50-100M parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rXnCpoJqoq7"},"outputs":[],"source":["print(\"Initializing Stable Diffusion + ControlNet model...\")\n","print(\"This may take a few minutes on first run (downloading pretrained weights)\\n\")\n","\n","model = StableDiffusionControlNetWrapper(\n","    sd_model_name=CONFIG['model']['sd_model_name'],\n","    controlnet_config=CONFIG['model']['controlnet'],\n","    device=device,\n","    use_lora=CONFIG['model']['use_lora'],\n",")\n","\n","print(\"✓ Model initialized successfully\\n\")\n","print(f\"Model configuration:\")\n","print(f\"  SD backbone: {CONFIG['model']['sd_model_name']}\")\n","print(f\"  ControlNet input channels: {CONFIG['model']['controlnet']['control_channels']}\")\n","print(f\"  Using LoRA: {CONFIG['model']['use_lora']}\")\n","\n","# Count trainable parameters\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\nParameter counts:\")\n","print(f\"  Total parameters: {total_params/1e6:.2f}M\")\n","print(f\"  Trainable parameters: {trainable_params/1e6:.2f}M ({100*trainable_params/total_params:.2f}%)\")"]},{"cell_type":"markdown","metadata":{"id":"zF9QZSszqoq7"},"source":["### Training Loop\n","\n","**Loss Functions:**\n","```\n","L_diff = ||ε̂ - ε||²                              (Diffusion prediction)\n","L_recon = ||Î - I||₁ + λ_LPIPS·LPIPS(Î, I)      (Reconstruction quality)\n","L_bg = λ_bg·||(Î - I) ⊙ B||₁                     (Background preservation)\n","\n","L_total = L_diff + λ_recon·L_recon + λ_bg·L_bg\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWMn6xCCqoq7"},"outputs":[],"source":["# Initialize trainer\n","trainer = ControlNetTrainer(\n","    model=model,\n","    train_dataloader=train_loader,\n","    val_dataloader=val_loader,\n","    learning_rate=CONFIG['training']['learning_rate'],\n","    num_epochs=CONFIG['training']['num_epochs'],\n","    device=device,\n","    output_dir=CONFIG['training']['output_dir'],\n","    lambda_recon=CONFIG['training']['lambda_recon'],\n","    lambda_lpips=CONFIG['training']['lambda_lpips'],\n","    lambda_bg=CONFIG['training']['lambda_bg'],\n","    use_recon_loss=CONFIG['training']['use_recon_loss'],\n","    gradient_accumulation_steps=CONFIG['training']['gradient_accumulation_steps'],\n","    mixed_precision=CONFIG['training']['mixed_precision'],\n","    log_wandb=CONFIG['training']['log_wandb'],\n","    project_name=CONFIG['training']['project_name'],\n",")\n","\n","print(f\"Starting training for {CONFIG['training']['num_epochs']} epochs...\\n\")\n","print(\"Loss weights:\")\n","print(f\"  λ_recon = {CONFIG['training']['lambda_recon']}\")\n","print(f\"  λ_LPIPS = {CONFIG['training']['lambda_lpips']}\")\n","print(f\"  λ_bg = {CONFIG['training']['lambda_bg']}\")\n","print()\n","\n","# Train\n","trainer.train()\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"Training completed!\")\n","print(f\"Best model saved to: {os.path.join(CONFIG['training']['output_dir'], 'best_model.pt')}\")\n","print(\"=\"*50)"]},{"cell_type":"markdown","metadata":{"id":"AXxTE2lDqoq7"},"source":["### Plot Training Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YimoHN53qoq7"},"outputs":[],"source":["# Load training history\n","history_path = os.path.join(CONFIG['training']['output_dir'], 'training_history.json')\n","if os.path.exists(history_path):\n","    with open(history_path, 'r') as f:\n","        history = json.load(f)\n","\n","    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","    # Total loss\n","    axes[0].plot(history['train_loss'], label='Train')\n","    axes[0].plot(history['val_loss'], label='Validation')\n","    axes[0].set_xlabel('Epoch')\n","    axes[0].set_ylabel('Total Loss')\n","    axes[0].set_title('Training and Validation Loss')\n","    axes[0].legend()\n","    axes[0].grid(True, alpha=0.3)\n","\n","    # Individual loss components\n","    axes[1].plot(history['diff_loss'], label='L_diff (diffusion)')\n","    axes[1].plot(history['recon_loss'], label='L_recon (reconstruction)')\n","    axes[1].plot(history['bg_loss'], label='L_bg (background)')\n","    axes[1].set_xlabel('Epoch')\n","    axes[1].set_ylabel('Loss')\n","    axes[1].set_title('Loss Components')\n","    axes[1].legend()\n","    axes[1].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Training history not found\")"]},{"cell_type":"markdown","metadata":{"id":"8T08vwFxqoq8"},"source":["---\n","## 5. Inference <a name=\"inference\"></a>\n","\n","### Load Trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzNARAxRqoq8"},"outputs":[],"source":["# Load best trained model\n","model_path = os.path.join(CONFIG['training']['output_dir'], 'best_model.pt')\n","\n","if os.path.exists(model_path):\n","    print(f\"Loading trained model from: {model_path}\")\n","\n","    # Re-initialize model\n","    model = StableDiffusionControlNetWrapper(\n","        sd_model_name=CONFIG['model']['sd_model_name'],\n","        controlnet_config=CONFIG['model']['controlnet'],\n","        device=device,\n","        use_lora=CONFIG['model']['use_lora'],\n","    )\n","\n","    # Load trained weights\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","\n","    print(\"✓ Model loaded successfully\")\n","else:\n","    print(\"Trained model not found. Please complete training first.\")"]},{"cell_type":"markdown","metadata":{"id":"GJN13RUNqoq8"},"source":["### Inference: Generate Experimental Variants\n","\n","#### Required Inputs for Inference:\n","\n","```python\n","inputs = {\n","    'image': torch.Tensor,           # Original frame [1, 3, 512, 512], normalized to [-1, 1]\n","    'control': torch.Tensor,         # Control tensor [1, 2, 512, 512]\n","                                     #   Channel 0: keyword_mask (M_t)\n","                                     #   Channel 1: alignment_map (S_t)\n","    'keyword': str,                  # Keyword text (e.g., \"jewelry\")\n","    'variant': str,                  # One of: 'baseline', 'boost', 'reduction', 'placebo'\n","    'boost_alpha': float,            # Multiplication factor for boost (default: 1.5)\n","    'reduction_alpha': float,        # Multiplication factor for reduction (default: 0.5)\n","}\n","```\n","\n","#### Expected Outputs:\n","\n","```python\n","outputs = {\n","    'edited_image': torch.Tensor,    # Edited frame [1, 3, 512, 512], range [0, 1]\n","    'modified_control': torch.Tensor,# Modified control tensor [1, 2, 512, 512]\n","    'variant_name': str,             # Variant identifier\n","}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tEPWZuWqoq8"},"outputs":[],"source":["def create_variant_control_tensor(original_control, variant_type, boost_alpha=1.5, reduction_alpha=0.5):\n","    \"\"\"\n","    Modify control tensor to create experimental variant.\n","\n","    Args:\n","        original_control: [B, 2, H, W] tensor with [M_t, S_t]\n","        variant_type: 'baseline', 'boost', 'reduction', or 'placebo'\n","        boost_alpha: Multiplication factor for boosting alignment\n","        reduction_alpha: Multiplication factor for reducing alignment\n","\n","    Returns:\n","        modified_control: [B, 2, H, W] tensor\n","    \"\"\"\n","    modified_control = original_control.clone()\n","\n","    keyword_mask = original_control[:, 0:1]  # M_t: [B, 1, H, W]\n","    alignment_map = original_control[:, 1:2]  # S_t: [B, 1, H, W]\n","\n","    if variant_type == 'baseline':\n","        # No change\n","        pass\n","\n","    elif variant_type == 'boost':\n","        # Increase alignment: S_t' = boost_alpha * S_t\n","        modified_alignment = (alignment_map * boost_alpha).clamp(0, 1)\n","        modified_control[:, 1:2] = modified_alignment\n","\n","    elif variant_type == 'reduction':\n","        # Decrease alignment: S_t' = reduction_alpha * S_t\n","        modified_alignment = (alignment_map * reduction_alpha).clamp(0, 1)\n","        modified_control[:, 1:2] = modified_alignment\n","\n","    elif variant_type == 'placebo':\n","        # Manipulate background (outside keyword region)\n","        background_mask = 1 - keyword_mask\n","        # Boost attention in background regions\n","        modified_alignment = alignment_map.clone()\n","        # This creates a control that affects non-keyword areas\n","        modified_control[:, 1:2] = modified_alignment * background_mask\n","\n","    else:\n","        raise ValueError(f\"Unknown variant type: {variant_type}\")\n","\n","    return modified_control\n","\n","\n","@torch.no_grad()\n","def inference_single_frame(model, image, control, keyword, variant_type='baseline',\n","                          boost_alpha=1.5, reduction_alpha=0.5,\n","                          num_inference_steps=50, guidance_scale=7.5, strength=0.8):\n","    \"\"\"\n","    Run inference on a single frame.\n","\n","    Args:\n","        model: Trained StableDiffusionControlNetWrapper\n","        image: [1, 3, H, W] tensor, normalized to [-1, 1]\n","        control: [1, 2, H, W] tensor with [M_t, S_t]\n","        keyword: Text prompt (e.g., \"jewelry\")\n","        variant_type: 'baseline', 'boost', 'reduction', 'placebo'\n","        boost_alpha: Boost factor (default: 1.5)\n","        reduction_alpha: Reduction factor (default: 0.5)\n","        num_inference_steps: Number of diffusion steps\n","        guidance_scale: CFG scale\n","        strength: Edit strength (0.0 = no change, 1.0 = full regeneration)\n","\n","    Returns:\n","        dict with:\n","            'edited_image': [1, 3, H, W] tensor in [0, 1]\n","            'modified_control': [1, 2, H, W] tensor\n","            'variant_name': str\n","    \"\"\"\n","    # Create variant control tensor\n","    modified_control = create_variant_control_tensor(\n","        control, variant_type, boost_alpha, reduction_alpha\n","    )\n","\n","    # Run model\n","    edited_image = model(\n","        image=image,\n","        control=modified_control,\n","        prompt=keyword,\n","        num_inference_steps=num_inference_steps,\n","        guidance_scale=guidance_scale,\n","        strength=strength,\n","    )\n","\n","    return {\n","        'edited_image': edited_image,\n","        'modified_control': modified_control,\n","        'variant_name': variant_type,\n","    }\n","\n","print(\"Inference functions defined.\")\n","print(\"\\nAvailable variant types:\")\n","print(\"  - 'baseline': No manipulation (control)\")\n","print(\"  - 'boost': Increase attention-keyword alignment\")\n","print(\"  - 'reduction': Decrease attention-keyword alignment\")\n","print(\"  - 'placebo': Manipulate non-keyword regions\")"]},{"cell_type":"markdown","metadata":{"id":"GLu6xjx7qoq8"},"source":["### Example: Generate Variants for a Single Frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euNF7vgRqoq8"},"outputs":[],"source":["# Select a test sample\n","test_batch = next(iter(val_loader))\n","test_idx = 0\n","\n","# Prepare inputs\n","original_image = test_batch['image'][test_idx:test_idx+1].to(device)  # [1, 3, 512, 512]\n","original_control = test_batch['control'][test_idx:test_idx+1].to(device)  # [1, 2, 512, 512]\n","keyword = test_batch['keyword'][test_idx]  # str\n","\n","print(f\"Generating variants for keyword: '{keyword}'\\n\")\n","\n","# Generate all variants\n","variants = ['baseline', 'boost', 'reduction', 'placebo']\n","results = {}\n","\n","for variant in variants:\n","    print(f\"Generating {variant} variant...\")\n","    result = inference_single_frame(\n","        model=model,\n","        image=original_image,\n","        control=original_control,\n","        keyword=keyword,\n","        variant_type=variant,\n","        num_inference_steps=50,\n","        guidance_scale=7.5,\n","        strength=0.8,\n","    )\n","    results[variant] = result\n","\n","print(\"\\n✓ All variants generated\")"]},{"cell_type":"markdown","metadata":{"id":"7RTFPYhsqoq8"},"source":["---\n","## 6. Visualization <a name=\"visualization\"></a>\n","\n","### Visualize All Variants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqn107qXqoq8"},"outputs":[],"source":["def tensor_to_image(tensor):\n","    \"\"\"Convert tensor to numpy image for visualization.\"\"\"\n","    img = tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n","    img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize from [-1, 1] to [0, 1]\n","    return img\n","\n","# Prepare original image\n","original_img = tensor_to_image(original_image)\n","\n","# Create comparison plot\n","fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","\n","# Original\n","axes[0, 0].imshow(original_img)\n","axes[0, 0].set_title(f\"Original Frame\\nKeyword: '{keyword}'\", fontsize=14, fontweight='bold')\n","axes[0, 0].axis('off')\n","\n","# Original control maps\n","orig_keyword_mask = original_control[0, 0].cpu().numpy()\n","orig_alignment = original_control[0, 1].cpu().numpy()\n","\n","axes[0, 1].imshow(orig_keyword_mask, cmap='gray')\n","axes[0, 1].set_title(\"Original Keyword Mask (M_t)\", fontsize=14, fontweight='bold')\n","axes[0, 1].axis('off')\n","\n","axes[0, 2].imshow(orig_alignment, cmap='hot')\n","axes[0, 2].set_title(\"Original Alignment (S_t)\", fontsize=14, fontweight='bold')\n","axes[0, 2].axis('off')\n","\n","# Edited variants\n","variant_positions = {\n","    'baseline': (1, 0),\n","    'boost': (1, 1),\n","    'reduction': (1, 2),\n","}\n","\n","for variant, (row, col) in variant_positions.items():\n","    edited_img = tensor_to_image(results[variant]['edited_image'])\n","    axes[row, col].imshow(edited_img)\n","\n","    # Add title with variant details\n","    if variant == 'baseline':\n","        title = \"Baseline (No Change)\\nControl Condition\"\n","    elif variant == 'boost':\n","        title = \"Boosted Alignment\\nS_t' = 1.5 × S_t\"\n","    elif variant == 'reduction':\n","        title = \"Reduced Alignment\\nS_t' = 0.5 × S_t\"\n","\n","    axes[row, col].set_title(title, fontsize=14, fontweight='bold', color='blue')\n","    axes[row, col].axis('off')\n","\n","plt.suptitle(\"Experimental Variants Comparison\", fontsize=16, fontweight='bold', y=0.98)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LUxumc7wqoq8"},"source":["### Compare Alignment Maps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxUFT2iMqoq8"},"outputs":[],"source":["# Plot alignment maps for all variants\n","fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n","\n","for idx, variant in enumerate(['baseline', 'boost', 'reduction', 'placebo']):\n","    modified_alignment = results[variant]['modified_control'][0, 1].cpu().numpy()\n","\n","    im = axes[idx].imshow(modified_alignment, cmap='hot', vmin=0, vmax=1)\n","    axes[idx].set_title(f\"{variant.capitalize()}\\nAlignment Map (S_t')\", fontsize=14, fontweight='bold')\n","    axes[idx].axis('off')\n","\n","    # Add colorbar\n","    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n","\n","plt.suptitle(\"Modified Alignment Maps (Control Signal)\", fontsize=16, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","# Print statistics\n","print(\"\\nAlignment Map Statistics:\")\n","for variant in ['baseline', 'boost', 'reduction', 'placebo']:\n","    alignment = results[variant]['modified_control'][0, 1].cpu().numpy()\n","    print(f\"  {variant:12s}: mean={alignment.mean():.4f}, max={alignment.max():.4f}, std={alignment.std():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"NWIH7qnDqoq8"},"source":["### Temporal Variants (for full videos)\n","\n","For complete video editing with temporal windows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK2tiqbRqoq8"},"outputs":[],"source":["def create_temporal_variants(video_id, model, output_dir='outputs/variants'):\n","    \"\"\"\n","    Create all 7 experimental variants for a full video.\n","\n","    Variants:\n","      1. baseline: No change\n","      2. early_boost: Boost frames 0-33%\n","      3. middle_boost: Boost frames 33-66%\n","      4. late_boost: Boost frames 66-100%\n","      5. full_boost: Boost all frames\n","      6. reduction: Reduce alignment in middle section\n","      7. placebo: Manipulate background only\n","\n","    Args:\n","        video_id: Video identifier\n","        model: Trained model\n","        output_dir: Output directory for variants\n","\n","    Returns:\n","        dict: Paths to generated variant videos\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Load all frames for this video\n","    frame_dir = os.path.join(FRAMES_DIR, video_id)\n","    frames = sorted([f for f in os.listdir(frame_dir) if f.endswith('.png')])\n","    num_frames = len(frames)\n","\n","    # Define temporal windows\n","    early_end = int(num_frames * 0.33)\n","    middle_start = early_end\n","    middle_end = int(num_frames * 0.66)\n","    late_start = middle_end\n","\n","    print(f\"Processing video: {video_id}\")\n","    print(f\"  Total frames: {num_frames}\")\n","    print(f\"  Early window: [0, {early_end})\")\n","    print(f\"  Middle window: [{middle_start}, {middle_end})\")\n","    print(f\"  Late window: [{late_start}, {num_frames})\")\n","\n","    # Variant definitions\n","    variant_specs = {\n","        'baseline': {'type': 'baseline', 'frames': range(num_frames)},\n","        'early_boost': {'type': 'boost', 'frames': range(0, early_end)},\n","        'middle_boost': {'type': 'boost', 'frames': range(middle_start, middle_end)},\n","        'late_boost': {'type': 'boost', 'frames': range(late_start, num_frames)},\n","        'full_boost': {'type': 'boost', 'frames': range(num_frames)},\n","        'reduction': {'type': 'reduction', 'frames': range(middle_start, middle_end)},\n","        'placebo': {'type': 'placebo', 'frames': range(middle_start, middle_end)},\n","    }\n","\n","    print(\"\\nGenerating variants...\")\n","\n","    # Process each variant\n","    # (Full implementation would iterate through frames and generate edited videos)\n","    # This is a template - actual implementation in src/video_editing/\n","\n","    return variant_specs\n","\n","print(\"Temporal variant specification:\")\n","print(\"\\n1. baseline: Original video, no changes\")\n","print(\"2. early_boost: Boost alignment in first third (frames 0-33%)\")\n","print(\"3. middle_boost: Boost alignment in middle third (frames 33-66%)\")\n","print(\"4. late_boost: Boost alignment in last third (frames 66-100%)\")\n","print(\"5. full_boost: Boost alignment throughout entire video\")\n","print(\"6. reduction: Reduce alignment in middle section\")\n","print(\"7. placebo: Manipulate background regions only\")\n","print(\"\\nEach variant is saved as a separate video file for experimental use.\")"]},{"cell_type":"markdown","metadata":{"id":"Iblz0P54qoq8"},"source":["---\n","## Summary\n","\n","### Fine-Tuning Requirements\n","\n","**Data:**\n","- Original frames (RGB)\n","- Attention heatmaps from LLaVA (grayscale)\n","- Keyword heatmaps from CLIPSeg (grayscale)\n","- Keywords JSON file\n","\n","**Training:**\n","- Fine-tune ControlNet adapter only (~50-100M params)\n","- Freeze Stable Diffusion backbone\n","- Loss: L_diff + L_recon + L_bg\n","- Typical: 10-20 epochs on 4-8 videos\n","\n","### Inference Requirements\n","\n","**Inputs:**\n","```python\n","{\n","    'image': torch.Tensor,        # [1, 3, 512, 512], range [-1, 1]\n","    'control': torch.Tensor,      # [1, 2, 512, 512]\n","                                  #   control[0] = keyword_mask (M_t)\n","                                  #   control[1] = alignment_map (S_t)\n","    'keyword': str,               # Product/keyword text\n","    'variant_type': str,          # 'baseline', 'boost', 'reduction', 'placebo'\n","}\n","```\n","\n","**Outputs:**\n","```python\n","{\n","    'edited_image': torch.Tensor,     # [1, 3, 512, 512], range [0, 1]\n","    'modified_control': torch.Tensor, # [1, 2, 512, 512]\n","    'variant_name': str,              # Variant identifier\n","}\n","```\n","\n","### Output Videos\n","\n","For each input video, the framework generates **7 variant videos**:\n","1. baseline.mp4\n","2. early_boost.mp4\n","3. middle_boost.mp4\n","4. late_boost.mp4\n","5. full_boost.mp4\n","6. reduction.mp4\n","7. placebo.mp4\n","\n","These variants are used in experiments to study how attention-keyword alignment affects viewer responses."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Complete Workflow with Attention Heatmaps\n\nThis notebook demonstrates the complete workflow for manipulating video attention-keyword alignment using **actual attention heatmaps**.\n\n## Workflow Overview\n\n1. **Data Validation**: Validate scene images, keyword masks, and attention heatmaps\n2. **Data Loading**: Load validated scenes for training/inference\n3. **Model Training**: Fine-tune ControlNet to manipulate scenes based on attention heatmaps\n4. **Variant Generation**: Create 7 experimental variants with modified attention patterns\n5. **Inference**: Generate edited scenes using the fine-tuned model\n6. **Video Assembly**: Reassemble edited scenes into videos\n\n## Directory Structure\n\n```\ndata/\n├── video_scene_cuts/          # Scene images\n│   └── {video_id}/\n│       └── {video_id}-Scene-0xx-01.jpg\n│\n├── keyword_masks/             # Keyword masks from CLIPSeg\n│   └── {video_id}/\n│       └── scene_1.png\n│       └── scene_2.png\n│\n├── attention_heatmap/         # Attention heatmaps\n│   └── {video_id}/\n│       └── {video_id}-Scene-001.jpg  # NO -01 suffix!\n│       └── {video_id}-Scene-002.jpg\n│\n├── keywords.csv               # Video keywords\n│\n└── valid_scenes.csv           # Generated by data_validation.ipynb\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import framework modules\n",
    "from src.training.dataset_v3 import VideoSceneDatasetV3, VideoSceneDataModuleV3\n",
    "from src.video_editing.experimental_variants_v3 import VideoVariantGeneratorV3, visualize_variant_comparison\n",
    "from src.data_preparation import ControlTensorBuilder\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Validation\n",
    "\n",
    "**IMPORTANT**: Before running this notebook, you must run `data_validation.ipynb` to generate `data/valid_scenes.csv`.\n",
    "\n",
    "The data validation notebook:\n",
    "- Scans scene images, keyword masks, and attention heatmaps\n",
    "- Validates that all required files exist\n",
    "- Creates `data/valid_scenes.csv` with only valid scenes\n",
    "\n",
    "This prevents repeated file existence checks during training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if valid_scenes.csv exists\n",
    "VALID_SCENES_FILE = 'data/valid_scenes.csv'\n",
    "\n",
    "if not os.path.exists(VALID_SCENES_FILE):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⚠ ERROR: valid_scenes.csv not found!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPlease run the data_validation.ipynb notebook first to:\")\n",
    "    print(\"  1. Validate scene images, keyword masks, and attention heatmaps\")\n",
    "    print(\"  2. Generate data/valid_scenes.csv\")\n",
    "    print(\"\\nThen come back to this notebook.\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    # Load and inspect valid scenes\n",
    "    valid_scenes_df = pd.read_csv(VALID_SCENES_FILE)\n",
    "    print(\"\\n✓ Found valid_scenes.csv\")\n",
    "    print(f\"\\nTotal valid scenes: {len(valid_scenes_df)}\")\n",
    "    print(f\"Unique videos: {valid_scenes_df['video_id'].nunique()}\")\n",
    "    print(f\"\\nColumns: {list(valid_scenes_df.columns)}\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(valid_scenes_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Loading\n",
    "\n",
    "Load validated scenes into PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = VideoSceneDataModuleV3(\n",
    "    valid_scenes_file=VALID_SCENES_FILE,\n",
    "    train_val_split=0.8,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    image_size=(512, 512),\n",
    "    normalize_heatmap=True,\n",
    ")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(data_module.train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(data_module.val_dataset)}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Training Batch Contents:\")\n",
    "print(f\"  'image' shape: {batch['image'].shape}\")  # [B, 3, H, W]\n",
    "print(f\"  'control' shape: {batch['control'].shape}\")  # [B, 2, H, W]\n",
    "print(f\"  'keyword_mask' shape: {batch['keyword_mask'].shape}\")  # [B, 1, H, W]\n",
    "print(f\"  'attention_heatmap' shape: {batch['attention_heatmap'].shape}\")  # [B, 1, H, W]\n",
    "print(f\"  'alignment_score': {batch['alignment_score'][:3].tolist()}...\")  # Scalars\n",
    "print(f\"  'keyword' (text prompts): {batch['keyword'][:2]}...\")\n",
    "print(f\"  'video_id': {batch['video_id'][:2]}...\")\n",
    "print(f\"  'scene_number': {batch['scene_number'][:3].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Control Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first sample in batch\n",
    "sample_idx = 0\n",
    "image = batch['image'][sample_idx].permute(1, 2, 0).numpy()\n",
    "image = (image * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "keyword_mask = batch['control'][sample_idx, 0].numpy()  # M_t\n",
    "attention_heatmap = batch['control'][sample_idx, 1].numpy()  # A_t\n",
    "alignment_score = batch['alignment_score'][sample_idx].item()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Scene image\n",
    "axes[0, 0].imshow(image)\n",
    "axes[0, 0].set_title(f\"Scene Image\\nVideo: {batch['video_id'][sample_idx]}\\nScene: {batch['scene_number'][sample_idx]}\\nKeyword: '{batch['keyword'][sample_idx]}'\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Keyword mask\n",
    "axes[0, 1].imshow(keyword_mask, cmap='gray')\n",
    "axes[0, 1].set_title(\"Control Channel 0: M_t\\nKeyword Mask (Binary)\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Attention heatmap\n",
    "axes[1, 0].imshow(attention_heatmap, cmap='hot')\n",
    "axes[1, 0].set_title(\"Control Channel 1: A_t\\nAttention Heatmap\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Alignment visualization\n",
    "alignment_map = keyword_mask * attention_heatmap\n",
    "axes[1, 1].imshow(alignment_map, cmap='hot')\n",
    "axes[1, 1].set_title(f\"Alignment Map (M_t ⊙ A_t)\\nAlignment Score: {alignment_score:.4f}\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAlignment Score: {alignment_score:.4f}\")\n",
    "print(f\"  = Mean of (keyword_mask * attention_heatmap)\")\n",
    "print(f\"  = Proportion of attention on keyword region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "Fine-tune ControlNet to manipulate scenes based on attention heatmaps.\n",
    "\n",
    "**Goal**: Learn to increase/decrease attention-keyword alignment in scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import StableDiffusionControlNetWrapper\n",
    "from src.training import ControlNetTrainer\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'sd_model_name': 'runwayml/stable-diffusion-v1-5',\n",
    "    'controlnet': {\n",
    "        'control_channels': 2,  # [M_t, A_t]\n",
    "        'base_channels': 64,\n",
    "    },\n",
    "    'use_lora': False,\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 10,\n",
    "    'lambda_recon': 1.0,\n",
    "    'lambda_lpips': 1.0,\n",
    "    'lambda_bg': 0.5,\n",
    "    'use_recon_loss': True,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'mixed_precision': True,\n",
    "    'log_wandb': False,\n",
    "    'output_dir': 'outputs/training_v3',\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  SD backbone: {MODEL_CONFIG['sd_model_name']}\")\n",
    "print(f\"  ControlNet input channels: 2 [keyword_mask, attention_heatmap]\")\n",
    "print(f\"  Using LoRA: {MODEL_CONFIG['use_lora']}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing Stable Diffusion + ControlNet model...\")\n",
    "print(\"This may take a few minutes on first run (downloading pretrained weights)\\n\")\n",
    "\n",
    "model = StableDiffusionControlNetWrapper(\n",
    "    sd_model_name=MODEL_CONFIG['sd_model_name'],\n",
    "    controlnet_config=MODEL_CONFIG['controlnet'],\n",
    "    device=device,\n",
    "    use_lora=MODEL_CONFIG['use_lora'],\n",
    ")\n",
    "\n",
    "print(\"✓ Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "os.makedirs(TRAINING_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "trainer = ControlNetTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    num_epochs=TRAINING_CONFIG['num_epochs'],\n",
    "    device=device,\n",
    "    output_dir=TRAINING_CONFIG['output_dir'],\n",
    "    lambda_recon=TRAINING_CONFIG['lambda_recon'],\n",
    "    lambda_lpips=TRAINING_CONFIG['lambda_lpips'],\n",
    "    lambda_bg=TRAINING_CONFIG['lambda_bg'],\n",
    "    use_recon_loss=TRAINING_CONFIG['use_recon_loss'],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "    mixed_precision=TRAINING_CONFIG['mixed_precision'],\n",
    "    log_wandb=TRAINING_CONFIG['log_wandb'],\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting training for {TRAINING_CONFIG['num_epochs']} epochs...\\n\")\n",
    "\n",
    "# Train (uncomment to start training)\n",
    "# trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"To start training, uncomment: trainer.train()\")\n",
    "print(f\"Best model will be saved to: {os.path.join(TRAINING_CONFIG['output_dir'], 'best_model.pt')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Experimental Variants\n",
    "\n",
    "Create 7 experimental variants with modified attention patterns:\n",
    "\n",
    "1. **baseline**: Original attention heatmaps\n",
    "2. **early_boost**: Boost alignment in first 33% of scenes (×1.5)\n",
    "3. **middle_boost**: Boost alignment in middle 33% (×1.5)\n",
    "4. **late_boost**: Boost alignment in last 33% (×1.5)\n",
    "5. **full_boost**: Boost alignment in all scenes (×1.5)\n",
    "6. **reduction**: Reduce alignment in middle 33% (×0.5)\n",
    "7. **placebo**: Modify non-keyword regions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variant generator\n",
    "variant_generator = VideoVariantGeneratorV3(\n",
    "    valid_scenes_file=VALID_SCENES_FILE,\n",
    "    boost_alpha=1.5,\n",
    "    reduction_alpha=0.5,\n",
    "    image_size=(512, 512),\n",
    ")\n",
    "\n",
    "print(\"Variant generator initialized\")\n",
    "print(f\"  Boost alpha: 1.5\")\n",
    "print(f\"  Reduction alpha: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variants for a Single Video (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample video\n",
    "sample_video_id = valid_scenes_df['video_id'].iloc[0]\n",
    "print(f\"Generating variants for video: {sample_video_id}\")\n",
    "\n",
    "# Generate variants\n",
    "variants = variant_generator.create_all_variants_for_video(\n",
    "    sample_video_id,\n",
    "    output_dir='outputs/variants_v3'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(variants)} variants:\")\n",
    "for variant_name in variants.keys():\n",
    "    print(f\"  - {variant_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and Visualize Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "stats = variant_generator.compute_variant_statistics(variants)\n",
    "\n",
    "print(\"\\nVariant Statistics:\\n\")\n",
    "for variant_name, stat in stats.items():\n",
    "    print(f\"{variant_name:15s}: mean={stat['mean_alignment']:.4f}, std={stat['std_alignment']:.4f}, scenes={stat['num_scenes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyword for this video\n",
    "keyword = variants['baseline'].iloc[0]['keyword']\n",
    "\n",
    "# Visualize alignment profiles\n",
    "visualize_variant_comparison(variants, sample_video_id, keyword, variant_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Variants for All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for all videos (or subset for testing)\n",
    "all_variants = variant_generator.generate_variants_for_all_videos(\n",
    "    output_dir='outputs/variants_v3',\n",
    "    max_videos=5  # Set to None to process all videos\n",
    ")\n",
    "\n",
    "# Save manifest\n",
    "variant_generator.save_variant_manifest(\n",
    "    all_variants,\n",
    "    output_path='outputs/variants_v3/manifest.json'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated variants for {len(all_variants)} videos\")\n",
    "print(f\"  Output directory: outputs/variants_v3/\")\n",
    "print(f\"  Manifest: outputs/variants_v3/manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inference - Generate Edited Scenes\n",
    "\n",
    "Use the fine-tuned ControlNet to generate edited scenes based on modified attention heatmaps.\n",
    "\n",
    "**NOTE**: This requires a trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "CHECKPOINT_PATH = 'outputs/training_v3/best_model.pt'\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading trained model from: {CHECKPOINT_PATH}\")\n",
    "    # model.load_checkpoint(CHECKPOINT_PATH)  # Implement this in your wrapper\n",
    "    print(\"✓ Model loaded\")\n",
    "else:\n",
    "    print(f\"⚠ Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "    print(\"  Please train the model first (Step 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate edited scene for a variant\n",
    "def generate_edited_scene(\n",
    "    model,\n",
    "    scene_image,\n",
    "    keyword_mask,\n",
    "    attention_heatmap,\n",
    "    keyword,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate edited scene using ControlNet.\n",
    "    \n",
    "    Args:\n",
    "        model: StableDiffusionControlNetWrapper\n",
    "        scene_image: Original scene (PIL Image)\n",
    "        keyword_mask: Keyword mask (numpy array)\n",
    "        attention_heatmap: Modified attention heatmap (numpy array)\n",
    "        keyword: Text prompt\n",
    "        num_inference_steps: Number of denoising steps\n",
    "        guidance_scale: Classifier-free guidance scale\n",
    "    \n",
    "    Returns:\n",
    "        Edited scene (PIL Image)\n",
    "    \"\"\"\n",
    "    # Build control tensor\n",
    "    control_builder = ControlTensorBuilder()\n",
    "    control_tensor = np.stack([keyword_mask, attention_heatmap], axis=0)\n",
    "    control_tensor = torch.from_numpy(control_tensor).unsqueeze(0).float().to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        edited_image = model.generate(\n",
    "            prompt=keyword,\n",
    "            control_tensor=control_tensor,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "        )\n",
    "    \n",
    "    return edited_image\n",
    "\n",
    "print(\"Inference function defined\")\n",
    "print(\"To generate edited scenes, call: generate_edited_scene(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete workflow for manipulating video attention-keyword alignment using **actual attention heatmaps**.\n",
    "\n",
    "### Key Differences from Previous Approach\n",
    "\n",
    "**Old Approach** (alignment_score.csv):\n",
    "- Used scalar alignment scores\n",
    "- Control tensor: `[keyword_mask, keyword_mask * scalar]`\n",
    "- Limited spatial information\n",
    "\n",
    "**New Approach** (attention heatmaps):\n",
    "- Uses actual attention heatmaps\n",
    "- Control tensor: `[keyword_mask, attention_heatmap]`\n",
    "- Full spatial information about where viewers look\n",
    "- More precise manipulation of attention patterns\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "1. **Data Validation** (`data_validation.ipynb`)\n",
    "   - Validates scene images, keyword masks, attention heatmaps\n",
    "   - Creates `data/valid_scenes.csv`\n",
    "\n",
    "2. **Data Loading** (`dataset_v3.py`)\n",
    "   - Loads validated scenes efficiently\n",
    "   - Constructs control tensors: `[M_t, A_t]`\n",
    "\n",
    "3. **Model Training** (`trainer.py`)\n",
    "   - Fine-tunes ControlNet to manipulate attention-keyword alignment\n",
    "   - Preserves background regions\n",
    "\n",
    "4. **Variant Generation** (`experimental_variants_v3.py`)\n",
    "   - Creates 7 variants with modified attention heatmaps\n",
    "   - Boosts/reduces attention on keyword regions\n",
    "\n",
    "5. **Inference**\n",
    "   - Generates edited scenes using fine-tuned model\n",
    "   - Applies modified attention heatmaps\n",
    "\n",
    "6. **Video Assembly**\n",
    "   - Reassembles edited scenes into videos\n",
    "   - Applies temporal smoothing for consistency\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `data_validation.ipynb` to generate `valid_scenes.csv`\n",
    "2. Train ControlNet model (Step 4)\n",
    "3. Generate experimental variants (Step 5)\n",
    "4. Run inference to create edited scenes (Step 6)\n",
    "5. Assemble videos and deploy for A/B testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
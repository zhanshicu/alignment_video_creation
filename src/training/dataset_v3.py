"""
Updated Dataset for Video Advertisement Training with Actual Attention Heatmaps

Uses actual attention heatmaps instead of alignment_score.csv.
"""

import os
import numpy as np
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Tuple, Optional, Callable


class VideoSceneDatasetV3(Dataset):
    """
    Dataset for video advertisement scenes with actual attention heatmaps.

    Uses pre-validated scenes from valid_scenes.csv for efficient loading.
    """

    def __init__(
        self,
        valid_scenes_file: str = "data/valid_scenes.csv",
        video_ids: Optional[List[str]] = None,
        image_size: Tuple[int, int] = (512, 512),
        transform: Optional[Callable] = None,
        normalize_heatmap: bool = True,
    ):
        """
        Initialize dataset.

        Args:
            valid_scenes_file: Path to valid_scenes.csv (generated by data_validation.ipynb)
            video_ids: Optional list of video IDs to include (None = all videos)
            image_size: Target image size (H, W)
            transform: Optional transform to apply
            normalize_heatmap: If True, normalize attention heatmaps to [0, 1]
        """
        self.image_size = image_size
        self.transform = transform
        self.normalize_heatmap = normalize_heatmap

        # Load valid scenes
        self.scenes_df = pd.read_csv(valid_scenes_file)
        self.scenes_df.columns = self.scenes_df.columns.str.strip()

        # Filter by video_ids if provided
        if video_ids is not None:
            video_ids_str = [str(vid) for vid in video_ids]
            self.scenes_df = self.scenes_df[
                self.scenes_df['video_id'].astype(str).isin(video_ids_str)
            ]

        # Reset index
        self.scenes_df = self.scenes_df.reset_index(drop=True)

        print(f"Dataset initialized with {len(self.scenes_df)} scenes from "
              f"{self.scenes_df['video_id'].nunique()} videos")

    def __len__(self) -> int:
        return len(self.scenes_df)

    def _load_image(self, path: str) -> np.ndarray:
        """Load image and resize."""
        img = Image.open(path).convert('RGB')
        img = img.resize((self.image_size[1], self.image_size[0]), Image.BILINEAR)
        img = np.array(img).astype(np.float32) / 255.0
        return img

    def _load_heatmap(self, path: str) -> np.ndarray:
        """Load grayscale heatmap and resize."""
        heatmap = Image.open(path).convert('L')
        heatmap = heatmap.resize((self.image_size[1], self.image_size[0]), Image.BILINEAR)
        heatmap = np.array(heatmap).astype(np.float32) / 255.0
        return heatmap

    def __getitem__(self, idx: int) -> Dict:
        """
        Get a single sample.

        Returns:
            Dict containing:
                - 'image': Frame tensor (3, H, W) in [-1, 1]
                - 'control': Control tensor (2, H, W)
                  - Channel 0: Keyword mask M_t (binary)
                  - Channel 1: Attention heatmap A_t (continuous [0, 1])
                - 'keyword_mask': Binary keyword mask (1, H, W)
                - 'attention_heatmap': Attention heatmap (1, H, W)
                - 'keyword': Text description
                - 'video_id': Video ID
                - 'scene_number': Scene number
        """
        row = self.scenes_df.iloc[idx]

        video_id = str(row['video_id'])
        scene_number = int(row['scene_number'])
        keyword = str(row['keyword'])

        # Load scene image
        image = self._load_image(row['scene_image_path'])
        # Convert to [-1, 1]
        image = (image * 2.0) - 1.0

        # Load keyword mask
        keyword_mask = self._load_heatmap(row['keyword_mask_path'])
        # Binarize keyword mask
        keyword_mask = (keyword_mask > 0.5).astype(np.float32)

        # Load attention heatmap
        attention_heatmap = self._load_heatmap(row['attention_heatmap_path'])

        # Normalize attention heatmap if requested
        if self.normalize_heatmap:
            heatmap_min = attention_heatmap.min()
            heatmap_max = attention_heatmap.max()
            if heatmap_max > heatmap_min:
                attention_heatmap = (attention_heatmap - heatmap_min) / (heatmap_max - heatmap_min)

        # Create background mask
        background_mask = 1.0 - keyword_mask

        # Build control tensor
        # Control channel 0: keyword mask (M_t) - where the product is
        # Control channel 1: attention heatmap (A_t) - what viewers pay attention to
        control_tensor = np.stack([
            keyword_mask,
            attention_heatmap
        ], axis=0)

        # Convert to torch tensors
        image = torch.from_numpy(image).permute(2, 0, 1).float()
        control_tensor = torch.from_numpy(control_tensor).float()
        keyword_mask = torch.from_numpy(keyword_mask).unsqueeze(0).float()
        background_mask = torch.from_numpy(background_mask).unsqueeze(0).float()
        attention_heatmap = torch.from_numpy(attention_heatmap).unsqueeze(0).float()

        # Apply optional transform
        if self.transform is not None:
            image = self.transform(image)

        # Compute alignment score as overlap between keyword mask and attention heatmap
        alignment_score = float((keyword_mask * attention_heatmap).mean())

        return {
            'image': image,
            'control_tensor': control_tensor,  # Trainer expects 'control_tensor'
            'control': control_tensor,  # Keep for backward compatibility
            'keyword_mask': keyword_mask,
            'background_mask': background_mask,
            'attention_heatmap': attention_heatmap,
            'alignment_score': alignment_score,
            'keyword_text': keyword,  # Trainer expects 'keyword_text'
            'keyword': keyword,  # Keep for backward compatibility
            'video_id': video_id,
            'scene_number': scene_number,
        }


class VideoSceneDataModuleV3:
    """Data module for organizing train/val splits with attention heatmaps."""

    def __init__(
        self,
        valid_scenes_file: str = "data/valid_scenes.csv",
        train_videos: Optional[List[str]] = None,
        val_videos: Optional[List[str]] = None,
        train_val_split: float = 0.8,
        batch_size: int = 4,
        num_workers: int = 4,
        image_size: Tuple[int, int] = (512, 512),
        normalize_heatmap: bool = True,
    ):
        """
        Initialize data module.

        Args:
            valid_scenes_file: Path to valid_scenes.csv
            train_videos: List of video IDs for training (if None, will split automatically)
            val_videos: List of video IDs for validation (if None, will split automatically)
            train_val_split: Train/val split ratio if train_videos/val_videos not provided
            batch_size: Batch size
            num_workers: Number of data loading workers
            image_size: Target image size (H, W)
            normalize_heatmap: Whether to normalize attention heatmaps
        """
        self.batch_size = batch_size
        self.num_workers = num_workers

        # If train/val videos not provided, split automatically
        if train_videos is None or val_videos is None:
            # Load valid scenes to get all video IDs
            scenes_df = pd.read_csv(valid_scenes_file)
            all_video_ids = scenes_df['video_id'].unique().tolist()

            # Split
            split_idx = int(train_val_split * len(all_video_ids))
            train_videos = all_video_ids[:split_idx]
            val_videos = all_video_ids[split_idx:]

            print(f"Auto-split: {len(train_videos)} training videos, {len(val_videos)} validation videos")

        # Create datasets
        print("Creating training dataset...")
        self.train_dataset = VideoSceneDatasetV3(
            valid_scenes_file=valid_scenes_file,
            video_ids=train_videos,
            image_size=image_size,
            normalize_heatmap=normalize_heatmap,
        )

        print("Creating validation dataset...")
        self.val_dataset = VideoSceneDatasetV3(
            valid_scenes_file=valid_scenes_file,
            video_ids=val_videos,
            image_size=image_size,
            normalize_heatmap=normalize_heatmap,
        )

    def train_dataloader(self):
        """Get training dataloader."""
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )

    def val_dataloader(self):
        """Get validation dataloader."""
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
        )

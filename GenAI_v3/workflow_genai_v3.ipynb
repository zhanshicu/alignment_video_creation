{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Attention-Keyword Alignment Manipulation (GenAI v3)\n",
    "\n",
    "**No training required!** This notebook demonstrates how to manipulate attention-keyword alignment using pre-trained generative models.\n",
    "\n",
    "## Key Features\n",
    "- ✅ **Zero training** - uses pre-trained models (InstructPix2Pix, Inpainting)\n",
    "- ✅ **Fast** - 2-3 seconds per frame\n",
    "- ✅ **Simple** - no complex training loops or loss functions\n",
    "- ✅ **Flexible** - easy to adjust via text instructions\n",
    "\n",
    "## Workflow\n",
    "1. Load pre-trained models\n",
    "2. Load scene images and keyword masks\n",
    "3. Generate variants by editing frames\n",
    "4. Apply temporal smoothing\n",
    "5. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import GenAI v3 modules\n",
    "from GenAI_v3.zero_shot_manipulator import (\n",
    "    ZeroShotAlignmentManipulator,\n",
    "    load_scenes_from_paths,\n",
    "    load_masks_from_paths,\n",
    "    save_scenes,\n",
    ")\n",
    "from GenAI_v3.temporal_smoother import (\n",
    "    TemporalSmoother,\n",
    "    apply_temporal_consistency_filter,\n",
    ")\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data\n",
    "\n",
    "We use the same data structure as before:\n",
    "- Scene images: `data/video_scene_cuts/{video_id}/{video_id}-Scene-0xx-01.jpg`\n",
    "- Keyword masks: `data/keyword_masks/{video_id}/scene_{x}.png`\n",
    "- Valid scenes: `data/valid_scenes.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load valid scenes\n",
    "VALID_SCENES_FILE = '../data/valid_scenes.csv'\n",
    "\n",
    "if not os.path.exists(VALID_SCENES_FILE):\n",
    "    print(\"⚠ ERROR: Please run data_validation.ipynb first!\")\n",
    "else:\n",
    "    scenes_df = pd.read_csv(VALID_SCENES_FILE)\n",
    "    print(f\"✓ Loaded {len(scenes_df)} valid scenes from {scenes_df['video_id'].nunique()} videos\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(scenes_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Models\n",
    "\n",
    "Choose your method:\n",
    "- **InstructPix2Pix**: Simpler, text-guided editing\n",
    "- **Inpainting**: More precise, edits only keyword region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose method: \"instruct_pix2pix\" or \"inpainting\"\n",
    "METHOD = \"instruct_pix2pix\"  # Recommended for simplicity\n",
    "\n",
    "# Initialize manipulator\n",
    "manipulator = ZeroShotAlignmentManipulator(\n",
    "    method=METHOD,\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for speed\n",
    ")\n",
    "\n",
    "# Initialize temporal smoother\n",
    "smoother = TemporalSmoother(method=\"simple_blend\")  # Fast and effective\n",
    "\n",
    "print(\"\\n✓ Models initialized!\")\n",
    "print(f\"  Method: {METHOD}\")\n",
    "print(f\"  Temporal smoothing: simple_blend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on Single Video\n",
    "\n",
    "Let's test the pipeline on a single video first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test video\n",
    "test_video_id = scenes_df['video_id'].iloc[0]\n",
    "print(f\"Test video ID: {test_video_id}\")\n",
    "\n",
    "# Get all scenes for this video\n",
    "video_scenes_df = scenes_df[scenes_df['video_id'] == test_video_id].sort_values('scene_number')\n",
    "print(f\"Number of scenes: {len(video_scenes_df)}\")\n",
    "\n",
    "# Get keyword\n",
    "keyword = video_scenes_df.iloc[0]['keyword']\n",
    "print(f\"Keyword: {keyword}\")\n",
    "\n",
    "# Load scenes and masks\n",
    "scene_paths = video_scenes_df['scene_image_path'].tolist()\n",
    "mask_paths = video_scenes_df['keyword_mask_path'].tolist()\n",
    "\n",
    "scenes = load_scenes_from_paths(scene_paths)\n",
    "masks = load_masks_from_paths(mask_paths) if METHOD == \"inpainting\" else None\n",
    "\n",
    "print(f\"✓ Loaded {len(scenes)} scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Original Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 6 scenes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, len(scenes))):\n",
    "    axes[i].imshow(scenes[i])\n",
    "    axes[i].set_title(f\"Scene {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Variants\n",
    "\n",
    "Generate all 7 experimental variants:\n",
    "1. baseline\n",
    "2. early_boost\n",
    "3. middle_boost\n",
    "4. late_boost\n",
    "5. full_boost\n",
    "6. reduction\n",
    "7. placebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(f'../outputs/genai_v3/{test_video_id}')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Variant types\n",
    "VARIANT_TYPES = [\n",
    "    \"baseline\",\n",
    "    \"early_boost\",\n",
    "    \"middle_boost\",\n",
    "    \"late_boost\",\n",
    "    \"full_boost\",\n",
    "    \"reduction\",\n",
    "    \"placebo\",\n",
    "]\n",
    "\n",
    "# Generate each variant\n",
    "variants = {}\n",
    "\n",
    "for variant_type in VARIANT_TYPES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Creating variant: {variant_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create variant (edit frames)\n",
    "    edited_scenes = manipulator.create_variant(\n",
    "        scenes=scenes,\n",
    "        keyword=keyword,\n",
    "        variant_type=variant_type,\n",
    "        keyword_masks=masks,\n",
    "        num_inference_steps=20,  # Adjust for speed/quality trade-off\n",
    "    )\n",
    "    \n",
    "    # Apply temporal smoothing (if not baseline/placebo)\n",
    "    if variant_type not in [\"baseline\", \"placebo\"]:\n",
    "        print(\"Applying temporal smoothing...\")\n",
    "        \n",
    "        # Determine which indices were edited\n",
    "        num_scenes = len(scenes)\n",
    "        third = num_scenes // 3\n",
    "        \n",
    "        if variant_type == \"early_boost\":\n",
    "            edited_indices = set(range(third))\n",
    "        elif variant_type == \"middle_boost\" or variant_type == \"reduction\":\n",
    "            edited_indices = set(range(third, 2*third))\n",
    "        elif variant_type == \"late_boost\":\n",
    "            edited_indices = set(range(2*third, num_scenes))\n",
    "        elif variant_type == \"full_boost\":\n",
    "            edited_indices = set(range(num_scenes))\n",
    "        else:\n",
    "            edited_indices = set()\n",
    "        \n",
    "        smoothed_scenes = smoother.smooth(\n",
    "            original_frames=scenes,\n",
    "            edited_frames=edited_scenes,\n",
    "            edited_indices=edited_indices,\n",
    "            blend_strength=0.7,\n",
    "        )\n",
    "    else:\n",
    "        smoothed_scenes = edited_scenes\n",
    "    \n",
    "    # Save variant\n",
    "    variant_dir = output_dir / variant_type\n",
    "    save_scenes(smoothed_scenes, variant_dir, prefix=\"scene\")\n",
    "    \n",
    "    variants[variant_type] = smoothed_scenes\n",
    "\n",
    "print(f\"\\n✓ All variants created!\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Results\n",
    "\n",
    "Compare variants side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a specific scene across variants\n",
    "scene_idx = len(scenes) // 2  # Middle scene\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, variant_type in enumerate(VARIANT_TYPES):\n",
    "    axes[i].imshow(variants[variant_type][scene_idx])\n",
    "    axes[i].set_title(f\"{variant_type}\\n(Scene {scene_idx+1})\", fontsize=14, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Comparison: {keyword}\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Temporal Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how a variant changes over time\n",
    "variant_to_show = \"middle_boost\"\n",
    "\n",
    "# Select evenly spaced scenes\n",
    "num_to_show = 6\n",
    "indices = np.linspace(0, len(scenes)-1, num_to_show, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i].imshow(variants[variant_to_show][idx])\n",
    "    axes[i].set_title(f\"Scene {idx+1}/{len(scenes)}\", fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Temporal Evolution: {variant_to_show}\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Process Multiple Videos\n",
    "\n",
    "Scale up to process all videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique video IDs\n",
    "video_ids = scenes_df['video_id'].unique()[:5]  # Limit to first 5 for testing\n",
    "\n",
    "print(f\"Processing {len(video_ids)} videos...\")\n",
    "\n",
    "for video_id in tqdm(video_ids, desc=\"Videos\"):\n",
    "    print(f\"\\nProcessing video: {video_id}\")\n",
    "    \n",
    "    # Get scenes for this video\n",
    "    video_scenes_df = scenes_df[scenes_df['video_id'] == video_id].sort_values('scene_number')\n",
    "    \n",
    "    # Get keyword\n",
    "    keyword = video_scenes_df.iloc[0]['keyword']\n",
    "    \n",
    "    # Load scenes and masks\n",
    "    scene_paths = video_scenes_df['scene_image_path'].tolist()\n",
    "    mask_paths = video_scenes_df['keyword_mask_path'].tolist()\n",
    "    \n",
    "    scenes = load_scenes_from_paths(scene_paths)\n",
    "    masks = load_masks_from_paths(mask_paths) if METHOD == \"inpainting\" else None\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(f'../outputs/genai_v3/{video_id}')\n",
    "    \n",
    "    # Generate variants\n",
    "    for variant_type in VARIANT_TYPES:\n",
    "        edited_scenes = manipulator.create_variant(\n",
    "            scenes=scenes,\n",
    "            keyword=keyword,\n",
    "            variant_type=variant_type,\n",
    "            keyword_masks=masks,\n",
    "            num_inference_steps=20,\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        variant_dir = output_dir / variant_type\n",
    "        save_scenes(edited_scenes, variant_dir, prefix=\"scene\")\n",
    "\n",
    "print(\"\\n✓ All videos processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Video Assembly (Optional)\n",
    "\n",
    "Reassemble edited scenes into videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def assemble_video(scenes: list, output_path: str, fps: int = 30):\n",
    "    \"\"\"\n",
    "    Assemble scenes into a video file.\n",
    "    \n",
    "    Args:\n",
    "        scenes: List of PIL Images\n",
    "        output_path: Output video path (.mp4)\n",
    "        fps: Frames per second\n",
    "    \"\"\"\n",
    "    # Get dimensions from first scene\n",
    "    width, height = scenes[0].size\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Write frames\n",
    "    for scene in scenes:\n",
    "        # Convert PIL to CV2 format (RGB -> BGR)\n",
    "        frame = cv2.cvtColor(np.array(scene), cv2.COLOR_RGB2BGR)\n",
    "        writer.write(frame)\n",
    "    \n",
    "    writer.release()\n",
    "    print(f\"✓ Video saved to: {output_path}\")\n",
    "\n",
    "# Example: assemble one variant\n",
    "video_output_dir = Path('../outputs/genai_v3/videos')\n",
    "video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for variant_type in [\"baseline\", \"full_boost\"]:\n",
    "    output_path = str(video_output_dir / f\"{test_video_id}_{variant_type}.mp4\")\n",
    "    assemble_video(variants[variant_type], output_path, fps=30)\n",
    "\n",
    "print(\"\\n✓ Videos assembled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "1. ✅ Loaded pre-trained models (no training!)\n",
    "2. ✅ Loaded scene images and keyword masks\n",
    "3. ✅ Generated 7 experimental variants\n",
    "4. ✅ Applied temporal smoothing\n",
    "5. ✅ Saved results\n",
    "\n",
    "### Performance\n",
    "- **Speed**: ~2-3 seconds per frame\n",
    "- **Quality**: Natural-looking edits\n",
    "- **Training**: Zero!\n",
    "\n",
    "### Next Steps\n",
    "1. Deploy variants for A/B testing\n",
    "2. Collect engagement metrics (CTR, CVR, watch time)\n",
    "3. Analyze which variants perform best\n",
    "4. Iterate on text instructions for better control\n",
    "\n",
    "### Advantages Over ControlNet Training\n",
    "- ✅ No training loop - immediate results\n",
    "- ✅ No GPU memory issues\n",
    "- ✅ No debugging complex architectures\n",
    "- ✅ Easy to adjust behavior via text\n",
    "- ✅ Can switch methods instantly\n",
    "\n",
    "### Trade-offs\n",
    "- ⚠️ Slightly slower per-frame than trained ControlNet\n",
    "- ⚠️ Less precise numerical control\n",
    "- ⚠️ May edit unintended regions slightly (InstructPix2Pix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

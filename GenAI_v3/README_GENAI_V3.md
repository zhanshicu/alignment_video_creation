# Zero-Shot Attention Manipulation (GenAI v3)

**No training required!** This approach uses pre-trained generative models to manipulate attention-keyword alignment in video advertisements.

---

## ğŸ¯ Key Advantages

âœ… **Zero Training** - Use pre-trained models as-is
âœ… **Fast Setup** - Start manipulating videos in minutes
âœ… **Simple** - No complex loss functions or training loops
âœ… **Flexible** - Easy to adjust via text instructions
âœ… **No GPU Memory Issues** - No optimizer states or gradient accumulation
âœ… **Easy Debugging** - If something goes wrong, just adjust the prompt

---

## ğŸ—ï¸ Architecture

### Two-Stage Pipeline

**Stage 1: Frame Manipulation**
- Use pre-trained InstructPix2Pix or Stable Diffusion Inpainting
- Edit individual frames to change keyword prominence
- Control via text instructions (boost/reduce alignment)

**Stage 2: Temporal Smoothing**
- Blend edited frames with original frames
- Apply optical flow warping for consistency
- Prevent flickering and artifacts

---

## ğŸ“‚ Directory Structure

```
GenAI_v3/
â”œâ”€â”€ zero_shot_manipulator.py      # Main manipulation class
â”œâ”€â”€ temporal_smoother.py           # Temporal smoothing utilities
â”œâ”€â”€ workflow_genai_v3.ipynb        # Complete workflow notebook
â””â”€â”€ README_GENAI_V3.md             # This file

outputs/genai_v3/
â””â”€â”€ {video_id}/
    â”œâ”€â”€ baseline/
    â”‚   â””â”€â”€ scene_001.jpg, scene_002.jpg, ...
    â”œâ”€â”€ early_boost/
    â”œâ”€â”€ middle_boost/
    â”œâ”€â”€ late_boost/
    â”œâ”€â”€ full_boost/
    â”œâ”€â”€ reduction/
    â””â”€â”€ placebo/
```

Uses same data structure as v3:
- Scene images: `data/video_scene_cuts/{video_id}/{video_id}-Scene-0xx-01.jpg`
- Keyword masks: `data/keyword_masks/{video_id}/scene_{x}.png`
- Valid scenes: `data/valid_scenes.csv`

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install diffusers transformers accelerate torch pillow opencv-python
```

### 2. Run the Workflow

Open and run `workflow_genai_v3.ipynb`:

```python
from GenAI_v3.zero_shot_manipulator import ZeroShotAlignmentManipulator

# Initialize (downloads model on first run)
manipulator = ZeroShotAlignmentManipulator(
    method="instruct_pix2pix",  # or "inpainting"
    device="cuda",
)

# Create variants
edited_scenes = manipulator.create_variant(
    scenes=scenes,
    keyword="sneakers",
    variant_type="full_boost",
    num_inference_steps=20,
)
```

That's it! No training required.

---

## ğŸ›ï¸ Methods

### Method 1: InstructPix2Pix (Recommended)

**Model**: `timbrooks/instruct-pix2pix`

**How it works**: Give text instructions to edit images

**Pros**:
- âœ… Very simple - just write text instructions
- âœ… Fast - 2-3 seconds per frame
- âœ… Natural-looking edits
- âœ… No masks required

**Cons**:
- âš ï¸ May edit unintended regions slightly
- âš ï¸ Less precise numerical control

**Example**:
```python
manipulator = ZeroShotAlignmentManipulator(method="instruct_pix2pix")

# Boost alignment
edited = manipulator.manipulate_frame(
    frame=scene,
    keyword="smartphone",
    boost_level=1.5,  # Make 1.5Ã— more prominent
)
```

### Method 2: Stable Diffusion Inpainting

**Model**: `runwayml/stable-diffusion-inpainting`

**How it works**: Edit only keyword region using masks

**Pros**:
- âœ… Very precise - only edits keyword region
- âœ… Preserves background perfectly
- âœ… More control over editing strength

**Cons**:
- âš ï¸ Requires good keyword masks
- âš ï¸ Slightly slower than InstructPix2Pix

**Example**:
```python
manipulator = ZeroShotAlignmentManipulator(method="inpainting")

# Boost alignment in keyword region only
edited = manipulator.manipulate_frame(
    frame=scene,
    keyword="product",
    boost_level=1.5,
    keyword_mask=mask,  # Required for inpainting
)
```

---

## ğŸ“Š Variant Types

Create 7 experimental variants:

| Variant | Description | Boost Level | Temporal Window |
|---------|-------------|-------------|-----------------|
| **baseline** | Original scenes (control) | 1.0 | - |
| **early_boost** | Boost first 33% of scenes | 1.5 | First third |
| **middle_boost** | Boost middle 33% | 1.5 | Middle third |
| **late_boost** | Boost last 33% | 1.5 | Last third |
| **full_boost** | Boost all scenes | 1.5 | All scenes |
| **reduction** | Reduce middle 33% | 0.5 | Middle third |
| **placebo** | No editing (artifact control) | 1.0 | - |

---

## âš¡ Performance

### Speed

- **InstructPix2Pix**: ~2-3 seconds per frame
- **Inpainting**: ~3-4 seconds per frame
- **Temporal smoothing**: ~0.1 seconds per frame

**Example**: 30-scene video
- Total time: ~60-120 seconds (1-2 minutes)
- Compare to ControlNet training: Hours of training + inference

### Quality

- Natural-looking edits
- Temporal consistency with smoothing
- Preserves video quality
- No training artifacts

### GPU Memory

- **Model**: ~5-6 GB (InstructPix2Pix or Inpainting)
- **Inference**: ~2-3 GB per frame
- **Total**: ~8-10 GB peak

Much less than training (which needs ~20-22 GB)!

---

## ğŸ¨ Customization

### Adjust Boost Strength

```python
# Subtle boost
edited = manipulator.manipulate_frame(frame, keyword, boost_level=1.2)

# Moderate boost (default)
edited = manipulator.manipulate_frame(frame, keyword, boost_level=1.5)

# Strong boost
edited = manipulator.manipulate_frame(frame, keyword, boost_level=2.0)

# Reduction
edited = manipulator.manipulate_frame(frame, keyword, boost_level=0.5)
```

### Custom Text Instructions (InstructPix2Pix)

Modify `zero_shot_manipulator.py`:

```python
# In _edit_with_instruction method
if boost_level > 1.0:
    instruction = f"Make the {keyword} sparkle and glow, draw attention to it"
elif boost_level < 1.0:
    instruction = f"Make the {keyword} fade into the background, desaturate it"
```

### Adjust Inference Steps

```python
# Fast (lower quality)
manipulator.create_variant(..., num_inference_steps=10)

# Balanced (recommended)
manipulator.create_variant(..., num_inference_steps=20)

# High quality (slower)
manipulator.create_variant(..., num_inference_steps=50)
```

---

## ğŸ”§ Temporal Smoothing

### Simple Blending (Default)

```python
smoother = TemporalSmoother(method="simple_blend")

smoothed = smoother.smooth(
    original_frames=original,
    edited_frames=edited,
    edited_indices={5, 6, 7, 8, 9},
    blend_strength=0.7,  # 0.0 = all original, 1.0 = all edited
)
```

### Optical Flow Warping

```python
smoother = TemporalSmoother(method="optical_flow")

smoothed = smoother.smooth(
    original_frames=original,
    edited_frames=edited,
    edited_indices={5, 6, 7, 8, 9},
    blend_strength=0.7,
)
```

Better quality, slightly slower.

### Frame Interpolation

```python
smoother = TemporalSmoother(method="interpolation")

smoothed = smoother.smooth(
    original_frames=original,
    edited_frames=edited,
    edited_indices={5, 6, 7, 8, 9},
)
```

Highest quality, creates gradual transitions.

---

## ğŸ“ˆ Comparison: GenAI v3 vs ControlNet Training

| Aspect | **GenAI v3 (Zero-Shot)** | **ControlNet Training** |
|--------|--------------------------|-------------------------|
| **Training** | âŒ None | âœ… Required (hours) |
| **Setup Time** | âš¡ Minutes | ğŸŒ Hours/days |
| **GPU Memory** | ğŸ’š 8-10 GB | ğŸ”´ 20-22 GB |
| **Speed (per frame)** | âš¡ 2-3s | âš¡âš¡ 1-2s (after training) |
| **Debugging** | ğŸ’š Easy (adjust text) | ğŸ”´ Hard (loss functions) |
| **Flexibility** | ğŸ’š High (change prompts) | ğŸŸ¡ Medium (retrain) |
| **Precision** | ğŸŸ¡ Medium | ğŸ’š High |
| **Quality** | ğŸ’š Very Good | ğŸ’š Excellent |
| **Reproducibility** | ğŸ’š Perfect | ğŸŸ¡ Good |

**Recommendation**:
- Use **GenAI v3** for quick experiments and prototyping
- Use **ControlNet** if you need maximum precision and have time to train

---

## ğŸ§ª Experimental Design

### Research Question
Does the timing and intensity of attention-keyword alignment affect video advertisement effectiveness?

### Independent Variables
1. Variant type (7 levels)
2. Temporal window (early/middle/late)
3. Manipulation strength (boost/reduce)

### Dependent Variables
1. Click-through rate (CTR)
2. Conversion rate (CVR)
3. Watch time
4. Engagement metrics

### Deployment
1. Generate all 7 variants
2. Deploy for A/B testing
3. Collect engagement data
4. Statistical analysis

---

## ğŸ› ï¸ Troubleshooting

### Issue 1: CUDA Out of Memory

**Error**: `CUDA out of memory`

**Solution**:
```python
# Use CPU (slower but works)
manipulator = ZeroShotAlignmentManipulator(device="cpu")

# Or use FP16 (smaller memory)
manipulator = ZeroShotAlignmentManipulator(torch_dtype=torch.float16)

# Or reduce inference steps
manipulator.create_variant(..., num_inference_steps=10)
```

### Issue 2: Model Download Fails

**Error**: `Connection timeout`

**Solution**:
```bash
# Pre-download models
huggingface-cli login
huggingface-cli download timbrooks/instruct-pix2pix
huggingface-cli download runwayml/stable-diffusion-inpainting
```

### Issue 3: Edits Look Unnatural

**Solution**:
```python
# Reduce boost level
boost_level=1.2  # Instead of 1.5

# Reduce blend strength
blend_strength=0.5  # Instead of 0.7

# Increase inference steps
num_inference_steps=30  # Instead of 20
```

### Issue 4: Flickering in Video

**Solution**:
```python
# Use optical flow smoothing
smoother = TemporalSmoother(method="optical_flow")

# Or apply temporal consistency filter
from GenAI_v3.temporal_smoother import apply_temporal_consistency_filter
filtered = apply_temporal_consistency_filter(frames, window_size=5)
```

---

## ğŸ“š API Reference

### ZeroShotAlignmentManipulator

```python
class ZeroShotAlignmentManipulator:
    def __init__(
        self,
        method: Literal["instruct_pix2pix", "inpainting"] = "instruct_pix2pix",
        device: str = "cuda",
        torch_dtype = torch.float16,
    )

    def manipulate_frame(
        self,
        frame: Image.Image,
        keyword: str,
        boost_level: float = 1.5,
        keyword_mask: Optional[np.ndarray] = None,
        num_inference_steps: int = 20,
    ) -> Image.Image

    def create_variant(
        self,
        scenes: List[Image.Image],
        keyword: str,
        variant_type: str,
        keyword_masks: Optional[List[np.ndarray]] = None,
        num_inference_steps: int = 20,
    ) -> List[Image.Image]
```

### TemporalSmoother

```python
class TemporalSmoother:
    def __init__(self, method: str = "simple_blend")

    def smooth(
        self,
        original_frames: List[Image.Image],
        edited_frames: List[Image.Image],
        edited_indices: Set[int],
        blend_strength: float = 0.7,
    ) -> List[Image.Image]
```

---

## ğŸ¯ Next Steps

1. **Run the workflow** - Open `workflow_genai_v3.ipynb` and execute all cells
2. **Generate variants** - Create all 7 variants for your videos
3. **Deploy for A/B testing** - Upload variants to your platform
4. **Collect metrics** - Track CTR, CVR, watch time
5. **Analyze results** - Determine which variants perform best
6. **Iterate** - Adjust boost levels and text instructions based on results

---

## ğŸ“ Support

For issues or questions:
1. Check this README
2. Review `workflow_genai_v3.ipynb` comments
3. Inspect example outputs in `outputs/genai_v3/`

---

**Version**: 1.0
**Date**: 2024-11-20
**Status**: Production Ready âœ…
**Training Required**: âŒ None!

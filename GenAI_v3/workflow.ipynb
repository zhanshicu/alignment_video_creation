{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GenAI v3: Background Manipulation for Attention Control\n\n**Zero training required!**\n\n## How It Works\n- **Product is AUTO-DETECTED** using SAM + DINO (no keyword mask needed!)\n- **Episodic memory** tracks consistent elements across video frames\n- **Background is modified** to control attention (product stays unchanged)\n- `increase`: Make background less distracting → more attention on product\n- `decrease`: Make background more interesting → less attention on product\n\n## Models Used\n- **SDXL Inpainting** - State-of-the-art background editing\n- **SAM ViT-H** - Segment Anything for object detection\n- **DINOv2** - Feature tracking across frames"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GenAI_v3 import SceneManipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize (loads SDXL + SAM + DINO, ~12GB total)\nmanipulator = SceneManipulator(\n    valid_scenes_file=\"data/valid_scenes.csv\",  # Optional (for scene timing)\n    video_dir=\"data/data_tiktok\",\n    output_dir=\"outputs/genai_v3\",\n    device=\"cuda\",\n    auto_detect_product=True,  # Auto-detect main product (default)\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase Attention on Product\n",
    "\n",
    "Makes background less distracting (muted, simple) → viewer focuses on product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCREASE attention on product in scene 6\n",
    "output = manipulator.manipulate(\n",
    "    video_id=\"YOUR_VIDEO_ID\",  # ← Change this\n",
    "    scene_index=6,              # ← Change this\n",
    "    action=\"increase\",\n",
    ")\n",
    "\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease Attention on Product\n",
    "\n",
    "Makes background more interesting (vibrant, detailed) → viewer distracted from product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECREASE attention on product in scene 3\n",
    "output = manipulator.manipulate(\n",
    "    video_id=\"YOUR_VIDEO_ID\",  # ← Change this\n",
    "    scene_index=3,              # ← Change this\n",
    "    action=\"decrease\",\n",
    ")\n",
    "\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Strength\n",
    "\n",
    "- `strength=0.5`: Subtle change\n",
    "- `strength=0.8`: Default, noticeable change\n",
    "- `strength=1.0`: Maximum change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtle background change\n",
    "output = manipulator.manipulate(\n",
    "    video_id=\"YOUR_VIDEO_ID\",\n",
    "    scene_index=6,\n",
    "    action=\"increase\",\n",
    "    strength=0.5,  # ← Subtle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load valid scenes\n",
    "scenes_df = pd.read_csv(\"data/valid_scenes.csv\")\n",
    "\n",
    "# Process multiple videos\n",
    "for video_id in scenes_df['video_id'].unique()[:3]:  # First 3 videos\n",
    "    try:\n",
    "        output = manipulator.manipulate(\n",
    "            video_id=str(video_id),\n",
    "            scene_index=1,  # First scene\n",
    "            action=\"increase\",\n",
    "        )\n",
    "        print(f\"✓ {video_id} -> {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {video_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### What It Does\n1. Load video frames\n2. **Auto-detect main product** using SAM + DINO (episodic memory)\n3. **Invert mask** → select background (not product)\n4. Use SDXL to modify background only\n5. Replace scene in video with smooth blending\n6. Export edited video\n\n### Why Auto-Detection?\n- **Keywords can be unreliable** - CLIPSeg might miss the actual product\n- **Episodic memory** - Tracks what appears consistently across frames\n- **No manual labeling** - Works out of the box\n- **More robust** - Finds the actual main content\n\n### Why Background?\n- **Product stays authentic** - no fake edits\n- **Attention is contextual** - background affects focus\n- **A/B testing** - clean comparison (same product, different context)\n\n### Performance\n- ~60s per scene (including auto-detection)\n- ~12GB GPU memory peak\n- No training required!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
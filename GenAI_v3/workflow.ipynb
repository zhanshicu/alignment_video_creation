{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GenAI v3: Full Scene Background Manipulation\n\n**Zero training required!**\n\n## Key Features\n- **ENTIRE SCENES manipulated** (not just single frames) via PySceneDetect\n- **DRAMATIC background changes** (not subtle)\n- **Product AUTO-DETECTED** using SAM + DINO episodic memory\n- **Smooth video output** with keyframe interpolation\n- **Outputs BOTH** manipulated video AND frame\n\n## How It Works\n- `increase`: Background becomes **EXTREMELY plain/gray** → all attention on product\n- `decrease`: Background becomes **EXTREMELY vibrant/psychedelic** → attention diverted\n\n## Models Used\n- **SDXL Inpainting** - State-of-the-art background editing\n- **SAM ViT-H** - Segment Anything for product detection\n- **DINOv2** - Feature tracking for episodic memory\n- **PySceneDetect** - Real scene boundary detection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GenAI_v3 import SceneManipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize (loads SDXL + SAM + DINO, ~12GB total)\nmanipulator = SceneManipulator(\n    valid_scenes_file=\"data/valid_scenes.csv\",  # Optional (for scene timing)\n    video_dir=\"data/data_tiktok\",\n    output_dir=\"outputs/genai_v3\",\n    device=\"cuda\",\n    auto_detect_product=True,  # Auto-detect main product (default)\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase Attention on Product\n",
    "\n",
    "Makes background less distracting (muted, simple) → viewer focuses on product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# INCREASE attention on product in scene 6\nresult = manipulator.manipulate(\n    video_id=\"YOUR_VIDEO_ID\",  # ← Change this\n    scene_index=6,              # ← Change this\n    action=\"increase\",\n)\n\n# Both video and frame are output\nprint(f\"Video: {result.video_path}\")\nprint(f\"Frame: {result.frame_path}\")\nprint(f\"Frames manipulated: {result.frames_manipulated}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease Attention on Product\n",
    "\n",
    "Makes background more interesting (vibrant, detailed) → viewer distracted from product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DECREASE attention on product in scene 3\nresult = manipulator.manipulate(\n    video_id=\"YOUR_VIDEO_ID\",  # ← Change this\n    scene_index=3,              # ← Change this\n    action=\"decrease\",\n)\n\n# Both video and frame are output\nprint(f\"Video: {result.video_path}\")\nprint(f\"Frame: {result.frame_path}\")\nprint(f\"Frames manipulated: {result.frames_manipulated}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Adjust Parameters\n\n- `strength=0.95`: Default - DRAMATIC change (recommended)\n- `strength=0.7`: Moderate change\n- `keyframe_interval=10`: Process every 10th frame (interpolate between)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom parameters\nresult = manipulator.manipulate(\n    video_id=\"YOUR_VIDEO_ID\",\n    scene_index=6,\n    action=\"increase\",\n    strength=0.95,           # Higher = more dramatic\n    num_inference_steps=40,  # Higher = better quality\n    keyframe_interval=5,     # Process every 5th frame for smoother result\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load valid scenes\nscenes_df = pd.read_csv(\"data/valid_scenes.csv\")\n\n# Process multiple videos\nresults = []\nfor video_id in scenes_df['video_id'].unique()[:3]:  # First 3 videos\n    try:\n        result = manipulator.manipulate(\n            video_id=str(video_id),\n            scene_index=1,  # First scene\n            action=\"increase\",\n        )\n        results.append(result)\n        print(f\"✓ {video_id}\")\n        print(f\"  Video: {result.video_path}\")\n        print(f\"  Frame: {result.frame_path}\")\n    except Exception as e:\n        print(f\"✗ {video_id}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### What It Does\n1. Load video with all frames\n2. **Detect scene boundaries** using PySceneDetect (real scene cuts)\n3. **Auto-detect main product** using SAM + DINO (episodic memory)\n4. **Manipulate keyframes** across entire scene with SDXL\n5. **Interpolate** between keyframes for smooth video\n6. **Composite**: Keep product from original, use manipulated background\n7. **Export** both video and sample frame\n\n### Key Improvements\n- **Full scene manipulation** - Not just a static image inserted\n- **PySceneDetect** - Uses real scene boundaries, not estimates\n- **DRAMATIC changes** - Solid gray or psychedelic, not subtle\n- **Smooth video** - Keyframe interpolation, no jarring transitions\n- **Dual output** - Both video path AND frame path returned\n\n### Performance\n- ~2-5 min per scene (depends on scene length)\n- ~12-15GB GPU memory peak\n- No training required!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
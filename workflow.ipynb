{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Y3QkAAkaVG8iJZRYWjuy4hD4k4qeU4Qj","authorship_tag":"ABX9TyOVTKTNfrSLzI4oB//KFeVb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%cd drive/MyDrive/research/tiktok_cuts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MF0ycyR8hCop","executionInfo":{"status":"ok","timestamp":1721200043666,"user_tz":-480,"elapsed":352,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"a985e8ca-9973-4018-9e31-e42a0ad574c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/research/tiktok_model_finetune\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taXWeS9Ra1k9","executionInfo":{"status":"ok","timestamp":1721200508241,"user_tz":-480,"elapsed":216666,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"bd0d3917-884a-4aa7-b8dc-843fcffd60ec"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [03:36<00:00, 72.11s/it]\n"]}],"source":["# INPUT: a folder that contains scenes splitted from an advertising video\n","# the name of the folder is the video id\n","\n","### DESCRIPTION OF THE FOLLOWING CODES ###\n","# to estimate human attention within the scene\n","# Human attention is guided by meaning maps (semantic richness) stated by T.R. Henderson (2017)\n","# we aim to simulates their lab experiment procedure by\n","# 1) first separating the scene image into several patches\n","\n","# output dir: for patch, cuts_patch/[video_id]\n","#             for metadata, metadata/[video_id]\n","\n","import os\n","import numpy as np\n","from PIL import Image, ImageDraw\n","from tqdm import tqdm\n","\n","def create_circular_mask(h, w, center=None, radius=None):\n","    if center is None:  # use the middle of the image\n","        center = (int(w/2), int(h/2))\n","    if radius is None:  # use the smallest distance between the center and image walls\n","        radius = min(center[0], center[1], w-center[0], h-center[1])\n","\n","    Y, X = np.ogrid[:h, :w]\n","    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n","\n","    mask = dist_from_center <= radius\n","    return mask\n","\n","def extract_circular_patches(scene_name, image, degrees, overlap=0.1, save_dir_patch=\"raw/test\", save_dir_meta=\"raw/test\"):\n","    if not os.path.exists(save_dir_patch):\n","        os.makedirs(save_dir_patch)\n","\n","    h, w, _ = image.shape\n","    patch_count = 0\n","    metadata = []\n","    for degree in degrees:\n","        radius = degree_to_pixel(degree, h, w)\n","        step = int(radius * (1 - overlap))\n","        for y in range(0, h, step):\n","            for x in range(0, w, step):\n","                mask = create_circular_mask(h, w, center=(x, y), radius=radius)\n","                patch = np.zeros_like(image)\n","                patch[mask] = image[mask]\n","\n","                # Find the bounding box of the circular patch\n","                coords = np.argwhere(mask)\n","                y_min, x_min = coords.min(axis=0)\n","                y_max, x_max = coords.max(axis=0)\n","\n","                # Crop the patch to the bounding box\n","                cropped_patch = patch[y_min:y_max+1, x_min:x_max+1]\n","                patch_image = Image.fromarray(cropped_patch)\n","                patch_filename = f'patch_{patch_count}_deg_{degree}.png'\n","                patch_image.save(os.path.join(save_dir_patch, patch_filename))\n","\n","                # Save metadata for reconstruction\n","                metadata.append({\n","                    'filename': patch_filename,\n","                    'center': (x, y),\n","                    'radius': radius,\n","                    'bbox': (x_min, y_min, x_max, y_max)\n","                })\n","\n","                patch_count += 1\n","\n","    # Save metadata to a file\n","    np.save(os.path.join(save_dir_meta, f'{scene_name}_metadata.npy'), metadata)\n","\n","def degree_to_pixel(degree, h, w):\n","    # Assuming the image represents a certain field of view, convert degrees to pixels\n","    # This is a placeholder function and should be adjusted based on actual FOV and image dimensions\n","    fov = 90  # Example field of view in degrees\n","    return int((degree / fov) * min(h, w))\n","\n","def reconstruct_image(metadata_file, original_shape):\n","    metadata = np.load(metadata_file, allow_pickle=True)\n","    reconstructed_image = np.zeros(original_shape, dtype=np.uint8)\n","\n","    for data in metadata:\n","        patch_image = Image.open(os.path.join(os.path.dirname(metadata_file), data['filename']))\n","        patch_array = np.array(patch_image)\n","\n","        x_min, y_min, x_max, y_max = data['bbox']\n","        mask = create_circular_mask(original_shape[0], original_shape[1], center=data['center'], radius=data['radius'])\n","        mask_cropped = mask[y_min:y_max+1, x_min:x_max+1]\n","\n","        reconstructed_image[y_min:y_max+1, x_min:x_max+1][mask_cropped] = patch_array[mask_cropped]\n","\n","    return Image.fromarray(reconstructed_image)\n","\n","# Process all scenes in the folder \"raw/scenes\"\n","scenes_path = 'scene_cuts/7164043618378006530'\n","output_patch_dir = 'cuts_patch/7164043618378006530'\n","output_metadata_dir = 'cuts_metadata/7164043618378006530'\n","\n","if not os.path.exists(output_patch_dir):\n","    os.makedirs(output_patch_dir)\n","if not os.path.exists(output_metadata_dir):\n","    os.makedirs(output_metadata_dir)\n","\n","for scene_filename in tqdm(os.listdir(scenes_path)):\n","    scene_path = os.path.join(scenes_path, scene_filename)\n","    scene_name, _ = os.path.splitext(scene_filename)\n","\n","    # Load the scene image\n","    scene_image = Image.open(scene_path)\n","    scene_image_np = np.array(scene_image)\n","\n","    # Define the output directories for patches and metadata\n","    scene_patch_dir = os.path.join(output_patch_dir, scene_name)\n","    metadata_dir = output_metadata_dir\n","\n","    # Extract patches and save metadata\n","    extract_circular_patches(scene_name, scene_image_np, degrees=[3, 7],\n","                             save_dir_patch=scene_patch_dir,\n","                             save_dir_meta=metadata_dir)"]},{"cell_type":"code","source":["# 2) then, store the patch information to .csv file\n","# output dir: current path\n","\n","import csv\n","\n","def calculate_patch_scores_for_scenes(patch_data_folder, metadata_folder, output_csv):\n","    # Prepare CSV file\n","    with open(output_csv, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['scene', 'filename', 'center_x', 'center_y', 'radius', 'bbox_x_min', 'bbox_y_min', 'bbox_x_max', 'bbox_y_max', 'mean_score'])\n","\n","        # Iterate through each scene\n","        for scene_name in tqdm(os.listdir(patch_data_folder)):\n","            scene_folder = os.path.join(patch_data_folder, scene_name)\n","            metadata_file = os.path.join(metadata_folder, f\"{scene_name}_metadata.npy\")\n","\n","            if not os.path.exists(metadata_file):\n","                print(f\"Metadata file for scene {scene_name} not found, skipping.\")\n","                continue\n","\n","            # Load metadata\n","            metadata = np.load(metadata_file, allow_pickle=True)\n","\n","            # Iterate through each patch\n","            for data in metadata:\n","                patch_image = Image.open(os.path.join(scene_folder, data['filename'])).convert('L')\n","                patch_array = np.array(patch_image)\n","\n","                # Write data to CSV\n","                writer.writerow([\n","                    scene_name,\n","                    data['filename'],\n","                    data['center'][0], data['center'][1],\n","                    data['radius'],\n","                    data['bbox'][0], data['bbox'][1], data['bbox'][2], data['bbox'][3],\n","                ])\n","\n","# Example usage\n","patch_data_folder = 'cuts_patch/7164043618378006530'\n","metadata_folder = 'cuts_metadata/7164043618378006530'\n","output_csv = 'patch_info.csv'\n","calculate_patch_scores_for_scenes(patch_data_folder, metadata_folder, output_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8H-nxHTkTCG","executionInfo":{"status":"ok","timestamp":1721201380217,"user_tz":-480,"elapsed":0,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"b294c80a-ec80-498b-ce78-354d5f9e93e3"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [00:33<00:00, 11.30s/it]\n"]}]},{"cell_type":"code","source":["# we need to transfer the patch info along with the data into cluster for inference\n","# fine-tuned LLaVA model at the cluster will return a patch_info.csv with nonempty column of \"mean_score\"\n","# 3) given the results (.csv file), construct the meaning map\n","# output dir: scene_meaning/[video_id]\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy.ndimage import gaussian_filter\n","import cv2\n","\n","def create_circular_mask(h, w, center=None, radius=None):\n","    if center is None:  # use the middle of the image\n","        center = (int(w/2), int(h/2))\n","    if radius is None:  # use the smallest distance between the center and image walls\n","        radius = min(center[0], center[1], w-center[0], h-center[1])\n","\n","    Y, X = np.ogrid[:h, :w]\n","    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n","\n","    mask = dist_from_center <= radius\n","    return mask\n","\n","def likert_to_numeric(likert_label):\n","    likert_scale = {\n","        'very low': 1,\n","        'Very low': 1,    # outputs of LLaVA w.o. finetune\n","        'low': 2,\n","        'somewhat low': 3,\n","        'Somewhat low': 3,\n","        'somewhat high': 4,\n","        'Somewhat high': 4,\n","        'high': 5,\n","        'very high': 6,\n","        'Very high': 6\n","    }\n","    return likert_scale[likert_label]\n","\n","def plot_smoothed_meaning_map_from_csv(csv_file, original_shape, sigma=5, gamma=3.0,\n","                                       scene_filter=None, save_path=None):\n","    # Load CSV data\n","    data = pd.read_csv(csv_file)\n","\n","    # Filter data by scene if a scene_filter is provided\n","    if scene_filter:\n","        data = data[data['scene'] == scene_filter]\n","\n","    meaning_map = np.zeros(original_shape[:2], dtype=np.float32)\n","    count_map = np.zeros(original_shape[:2], dtype=np.float32)\n","\n","    # Iterate through each row in the CSV\n","    for _, row in data.iterrows():\n","        likert_label = row['likert_label_predicted']\n","        numeric_score = likert_to_numeric(likert_label)\n","        x_min, y_min, x_max, y_max = int(row['bbox_x_min']), int(row['bbox_y_min']), int(row['bbox_x_max']), int(row['bbox_y_max'])\n","        center = (int(row['center_x']), int(row['center_y']))\n","        radius = int(row['radius'])\n","\n","        # Get the mask\n","        mask = create_circular_mask(original_shape[0], original_shape[1], center=center, radius=radius)\n","        mask_cropped = mask[y_min:y_max+1, x_min:x_max+1]\n","\n","        # Place the numeric score in the corresponding location on the meaning map\n","        meaning_map[y_min:y_max+1, x_min:x_max+1][mask_cropped] += numeric_score\n","        count_map[y_min:y_max+1, x_min:x_max+1][mask_cropped] += 1\n","\n","    # Avoid division by zero\n","    count_map[count_map == 0] = 1\n","    smoothed_meaning_map = meaning_map / count_map\n","\n","    # Apply Gaussian filter for smoothing\n","    smoothed_meaning_map = gaussian_filter(smoothed_meaning_map, sigma=sigma)\n","    smoothed_meaning_map = np.power(smoothed_meaning_map, gamma)\n","\n","    # Plot the smoothed meaning map\n","    if save_path:\n","        plt.imsave(save_path, smoothed_meaning_map, cmap='gray')\n","    else:\n","        plt.imshow(smoothed_meaning_map, cmap='hot', interpolation='nearest')\n","        plt.title('Smoothed Meaning Map')\n","        plt.show()\n","\n","# Example usage\n","original_shape = (1024, 576)\n","scene_filter = '7164043618378006530-Scene-003-01'\n","plot_smoothed_meaning_map_from_csv('preds.csv', original_shape, scene_filter=scene_filter,\n","                                  save_path='scene_meaning/7164043618378006530/meaning-Scene-003-01.png')"],"metadata":{"id":"OFgD4cibnRhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then, 4) use prompt segmentation model (CLIPSeg) to find the range of product\n","# In this case, the product is \"jewelry\"\n","# Input: scene under the folder \"scene_cuts/[video_id]\"\n","# Output: product segmentation images under the folder \"cuts_segmentation/[video_id]\"\n","\n","!pip install -q git+https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bt9m5xWR_eBh","executionInfo":{"status":"ok","timestamp":1721208563033,"user_tz":-480,"elapsed":50898,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"ea76d32f-89bd-4025-fce6-ba5a5624db9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import requests\n","from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n","from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","\n","scene_path = \"scene_cuts/7164043618378006530\"\n","scene_files = os.listdir(scene_path)\n","\n","processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n","model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n","\n","processor.image_processor.image_mean = IMAGENET_DEFAULT_MEAN\n","processor.image_processor.image_std = IMAGENET_DEFAULT_STD\n","\n","def prompt_seg(scene_path, base_save_path):\n","    image = Image.open(scene_path)\n","\n","    prompts = [\"jewelry within a box\"]\n","    inputs = processor(text=prompts, images=[image], padding=\"max_length\", return_tensors=\"pt\")\n","\n","    # predict\n","    with torch.no_grad():\n","      outputs = model(**inputs)\n","\n","    preds = outputs.logits.unsqueeze(1)\n","\n","    # resize\n","    preds = nn.functional.interpolate(\n","        outputs.logits.unsqueeze(1),\n","        size=(image.size[1], image.size[0]),\n","        mode=\"bilinear\"\n","    )\n","\n","    # binary mask\n","    pred_image = preds[0][0].cpu().numpy()\n","    _, binary_mask = cv2.threshold(pred_image, 0.5, 1, cv2.THRESH_BINARY)\n","\n","    # save files\n","    scene_part = os.path.splitext(os.path.basename(scene_path))[0]  # e.g., 7164043618378006530-Scene-001-01\n","    new_filename = os.path.join(scene_part + '-seg')  # e.g., 7164043618378006530-Scene-001-01-seg\n","    save_path = os.path.join(base_save_path, new_filename + '.png')\n","    plt.imsave(save_path, binary_mask, cmap='gray')\n","\n","# experiment\n","base_save_path='cuts_segmentation/7164043618378006530'\n","for scene in scene_files:\n","    current_scene_path = os.path.join(scene_path, scene)\n","    prompt_seg(current_scene_path, base_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7VfmJRLAjgA","executionInfo":{"status":"ok","timestamp":1721220410512,"user_tz":-480,"elapsed":8086,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"cc395492-583f-48cd-e062-da5893a22f52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Unused or unrecognized kwargs: padding.\n","Unused or unrecognized kwargs: padding.\n","Unused or unrecognized kwargs: padding.\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","\n","def calculate_attention_proportion(product_map_path, attention_map_path):\n","    # Load the product map (binary mask)\n","    product_map = cv2.imread(product_map_path, cv2.IMREAD_GRAYSCALE)\n","    if product_map is None:\n","        raise FileNotFoundError(f\"Product map not found: {product_map_path}\")\n","\n","    # Load the attention map\n","    attention_map = cv2.imread(attention_map_path, cv2.IMREAD_GRAYSCALE)\n","    if attention_map is None:\n","        raise FileNotFoundError(f\"Attention map not found: {attention_map_path}\")\n","\n","    # Ensure both maps have the same dimensions\n","    if product_map.shape != attention_map.shape:\n","        raise ValueError(\"Product map and attention map must have the same dimensions\")\n","\n","    # Calculate the total attention\n","    total_attention = np.sum(attention_map)\n","\n","    # Apply the product map as a mask to the attention map\n","    product_attention = np.sum(attention_map[product_map > 0])\n","\n","    # Calculate the proportion of attention within the product region\n","    proportion = product_attention / total_attention if total_attention > 0 else 0\n","\n","    return proportion\n","\n","def main(product_map_dir, attention_map_dir):\n","    scene_ids = [\"001\", \"002\", \"003\"]\n","    for scene_id in scene_ids:\n","        product_map_filename = f\"7164043618378006530-Scene-{scene_id}-01-seg.png\"\n","        attention_map_filename = f\"meaning-Scene-{scene_id}-01.png\"\n","\n","        product_map_path = os.path.join(product_map_dir, product_map_filename)\n","        attention_map_path = os.path.join(attention_map_dir, attention_map_filename)\n","\n","        try:\n","            proportion = calculate_attention_proportion(product_map_path, attention_map_path)\n","            print(f\"Scene-{scene_id}-01: {proportion:.4f}\")\n","        except (FileNotFoundError, ValueError) as e:\n","            print(e)\n","\n","\n","if __name__ == \"__main__\":\n","    product_map_dir = \"cuts_segmentation/7164043618378006530\"\n","    attention_map_dir = \"scene_meaning/7164043618378006530\"\n","    main(product_map_dir, attention_map_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pi2hlxidnMpc","executionInfo":{"status":"ok","timestamp":1721231448429,"user_tz":-480,"elapsed":466,"user":{"displayName":"Zhan Shi","userId":"04359299908287882141"}},"outputId":"41549a16-4a34-4d08-ea4b-8d711d144e3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scene-001-01: 0.0000\n","Scene-002-01: 0.1396\n","Scene-003-01: 0.6090\n"]}]}]}
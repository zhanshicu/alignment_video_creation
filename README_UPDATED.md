# Video Advertisement Manipulation Framework (Updated)

This framework uses ControlNet and Stable Diffusion to generate experimental variants of video advertisements by manipulating the alignment between viewer attention and product placement.

## Overview

**Purpose:** Create 7 experimental video variants to test how attention-keyword alignment affects viewer engagement in TikTok-style advertisements.

**Key Innovation:** Uses AI-powered video editing to create counterfactual versions of ads where attention-product alignment is systematically manipulated while keeping everything else constant. This enables causal inference about what drives viewer engagement.

## Updated Data Structure

The framework now works with pre-computed alignment scores instead of raw attention heatmaps:

```
data/
├── alignment_score.csv          # Pre-computed alignment scores per scene
│   Columns: video id, Scene Number, attention_proportion, metrics...
│
├── keywords.csv                 # Video keywords/products
│   Columns: _id, keyword_list[0]
│
├── screenshots_tiktok/          # RGB images per scene (not in repo)
│   └── {video_id}/scene_{N}.png
│
├── videos_tiktok/               # Original videos (not in repo)
│   └── {video_id}.mp4
│
└── keyword_masks/               # Generated by CLIPSeg (preprocessing)
    └── {video_id}/scene_{N}.png
```

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- `torch`, `torchvision`
- `transformers` (for CLIPSeg)
- `diffusers` (for Stable Diffusion)
- `pandas`, `numpy`, `pillow`
- `opencv-python`, `matplotlib`

### 2. Prepare Dataset

**IMPORTANT:** Use `alignment_score.csv` as the source of truth for video IDs, not `keywords.csv`. Some videos in `keywords.csv` may not be valid or may not have alignment scores.

```python
from src.training import get_valid_video_ids, split_train_val_videos

# Get valid video IDs (intersection of alignment_score.csv and keywords.csv)
valid_video_ids = get_valid_video_ids(
    alignment_score_file='data/alignment_score.csv',
    keywords_file='data/keywords.csv'
)

# Split into train/val (80/20 split with shuffling)
train_videos, val_videos = split_train_val_videos(
    video_ids=valid_video_ids,
    val_ratio=0.2,
    random_seed=42
)
```

Or run the example script:
```bash
python examples/prepare_dataset.py
```

### 3. Preprocess Data: Generate Keyword Masks

Since we don't have raw keyword heatmaps, we use CLIPSeg to generate spatial masks:

```bash
python scripts/generate_keyword_masks.py \
    --screenshots_dir data/screenshots_tiktok \
    --keywords_file data/keywords.csv \
    --output_dir data/keyword_masks \
    --device cuda \
    --threshold 0.4
```

This creates binary masks showing where the product appears in each scene.

### 4. Train ControlNet Model

Open and run the updated notebook:

```bash
jupyter notebook finetune_and_inference_v2.ipynb
```

**Important Notes:**
- The dataset is automatically split into train (80%) and validation (20%) sets
- Video IDs are shuffled before splitting for randomness
- The split is based on **videos**, not scenes (all scenes from a video go to the same split)
- Use `random_seed=42` for reproducibility

Or use the Python API:

```python
from src.training.dataset_v2 import VideoSceneDataModule
from src.models import StableDiffusionControlNetWrapper
from src.training import ControlNetTrainer

# Create data module
data_module = VideoSceneDataModule(
    alignment_score_file='data/alignment_score.csv',
    keywords_file='data/keywords.csv',
    train_videos=train_video_ids,
    val_videos=val_video_ids,
    screenshots_dir='data/screenshots_tiktok',
    keyword_masks_dir='data/keyword_masks',
    batch_size=4,
)

# Initialize model
model = StableDiffusionControlNetWrapper(
    sd_model_name='runwayml/stable-diffusion-v1-5',
    controlnet_config={'control_channels': 2},
    device='cuda',
)

# Train
trainer = ControlNetTrainer(
    model=model,
    train_dataloader=data_module.train_dataloader(),
    val_dataloader=data_module.val_dataloader(),
    num_epochs=10,
    output_dir='outputs/training',
)
trainer.train()
```

### 5. Generate Experimental Variants

Generate 7 variants for each video:

```python
from src.video_editing.experimental_variants_v2 import VideoVariantGenerator

# Initialize generator
variant_gen = VideoVariantGenerator(
    alignment_score_file='data/alignment_score.csv',
    keywords_file='data/keywords.csv',
    boost_alpha=1.5,
    reduction_alpha=0.5,
)

# Generate variants for all videos
all_variants = variant_gen.generate_variants_for_all_videos(
    output_dir='outputs/variants'
)

# Save manifest
variant_gen.save_variant_manifest(
    all_variants,
    output_path='outputs/variants/manifest.json'
)
```

## The 7 Experimental Variants

For each video, we generate:

| Variant | Description | Manipulation |
|---------|-------------|--------------|
| `baseline` | Original video | No change (control) |
| `early_boost` | Boost early attention | Multiply alignment_proportion by 1.5 for first 33% of scenes |
| `middle_boost` | Boost middle attention | Multiply alignment_proportion by 1.5 for middle 33% |
| `late_boost` | Boost late attention | Multiply alignment_proportion by 1.5 for last 33% |
| `full_boost` | Boost throughout | Multiply alignment_proportion by 1.5 for all scenes |
| `reduction` | Reduce alignment | Multiply alignment_proportion by 0.5 for middle 33% |
| `placebo` | Control manipulation | Modify non-keyword regions only |

## Control Tensor Construction

The framework uses a 2-channel control tensor:

**C_t = [M_t, S_t]**

Where:
- **M_t** (Channel 0): Keyword mask from CLIPSeg
  - Binary mask indicating product locations
  - Values: {0, 1}

- **S_t** (Channel 1): Alignment map
  - S_t = M_t × alignment_score
  - Combines spatial mask with scalar alignment
  - Values: [0, 1]

This differs from the original design which used full spatial attention heatmaps.

## Architecture

### Key Components

1. **Preprocessing** (`scripts/generate_keyword_masks.py`)
   - Uses CLIPSeg to segment products in scenes
   - Generates spatial keyword masks

2. **Data Loading** (`src/training/dataset_v2.py`)
   - Loads alignment scores from CSV
   - Combines with keyword masks to create control tensors
   - Returns scenes ready for training

3. **Model** (`src/models/`)
   - Frozen Stable Diffusion backbone
   - Trainable ControlNet adapter (~50-100M params)
   - Conditioned on keyword masks and alignment scores

4. **Training** (`src/training/trainer.py`)
   - Fine-tunes ControlNet only
   - Loss = L_diff + L_recon + L_bg
   - Preserves backgrounds, manipulates products

5. **Variant Generation** (`src/video_editing/experimental_variants_v2.py`)
   - Modulates alignment scores per variant type
   - Generates 7 variants per video
   - Saves specifications for inference

## Project Structure

```
alignment_video_creation/
├── data/
│   ├── alignment_score.csv          # Pre-computed alignment data
│   ├── keywords.csv                 # Video keywords
│   ├── screenshots_tiktok/          # Scene screenshots (not pushed)
│   ├── videos_tiktok/               # Original videos (not pushed)
│   └── keyword_masks/               # Generated keyword masks
│
├── scripts/
│   └── generate_keyword_masks.py    # CLIPSeg preprocessing
│
├── src/
│   ├── data_preparation/
│   │   └── control_tensor.py        # Control tensor construction
│   ├── models/
│   │   ├── controlnet_adapter.py    # ControlNet architecture
│   │   └── stable_diffusion_wrapper.py
│   ├── training/
│   │   ├── dataset_v2.py            # Updated dataset (uses CSV)
│   │   ├── trainer.py               # Training loop
│   │   └── losses.py                # Loss functions
│   └── video_editing/
│       └── experimental_variants_v2.py  # Variant generation
│
├── finetune_and_inference_v2.ipynb  # Updated main notebook
├── requirements.txt
└── README_UPDATED.md
```

## Differences from Original Design

| Aspect | Original | Updated |
|--------|----------|---------|
| **Attention data** | Raw attention heatmaps (spatial) | Pre-computed alignment_proportion (scalar) |
| **Keyword data** | Raw keyword heatmaps (from CLIPSeg) | Generated on-the-fly from screenshots |
| **Data organization** | Frame-based | Scene-based |
| **Control signal** | [M_t, S_t, A_t, K_t] (4 channels) | [M_t, S_t] (2 channels) |
| **Alignment map** | Spatial (A_t ⊙ K_t) | Scalar × Mask (M_t × score) |

## Research Application

### Experimental Design

1. **Generate variants** for each ad
2. **Deploy** all 7 variants in production
3. **Measure** engagement metrics:
   - CTR (Click-Through Rate)
   - CVR (Conversion Rate)
   - Watch time
   - Engagement
4. **Analyze** causal effects:
   - Does alignment boost → higher engagement?
   - When is alignment most important (early/middle/late)?
   - What's the magnitude of effect?

### Key Questions

- Does attention-product alignment causally affect engagement?
- Is there a dose-response relationship (more alignment → more clicks)?
- When in the video is alignment most critical?
- Are effects heterogeneous across product categories?

## Citation

If you use this framework, please cite:

```bibtex
@software{video_alignment_manipulation,
  title={Video Advertisement Manipulation Framework},
  author={Your Name},
  year={2025},
  description={ControlNet-based framework for experimental video ad manipulation}
}
```

## License

MIT License

## Contact

For questions or issues, please open a GitHub issue.
